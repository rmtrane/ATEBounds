---
title: "Bounds in Two-Sample Mendelian Randomization With Summary Statistics"
output: 
  bookdown::pdf_document2:
    keep_tex: true
    citation_package: natbib
    toc: true
    number_sections: true
    includes:
      in_header: preamble.tex
bibliography: "../references.bib"
---

```{r setup, include = FALSE}
library(tidyverse)
```

\newpage

# Introduction

The gold standard to estimate the causal effect of a treatment or an exposure on an outcome is a randomized trial where the treatment assignment is randomized. However, in many epidemiological studies, randomized experiments are not feasible. For example, a study estimating the negative effects of smoking on depression [@wootton_evidence_2019] would not be feasible with a randomized trial due to ethical concerns. In such settings, epidemiologists rely on different types of observational studies, which introduces potential biases from unmeasured confounders. In recent years, there has been an increase in using instrumental variable (IV) in the form of Mendelian randomization (MR) [@davey_smith_mendelian_2003; @lawlor_mendelian_2008]. Briefly, IV is a variable that is (A1) associated with the exposure, (A2) is independent from unmeasured confounders affecting the exposure and the outcome, and (A3) affects the outcome only through its effect on the exposure; see \ref{sec:iv-assumptions-and-two-sample-mr} for details. MR uses genetic variants, usually single nucleotide polymorphisms, as instruments to estimate the causal effect of an exposure on an outcome. This is based on the idea being that genotypes are randomly assigned when passed on from parents to offspring at meiosis [@lawlor_mendelian_2008] and thus, they make excellent candidates for satisfying (A2), especially when the exposure of interest is environmental.

MR analyses often use published summary statistics from two independent genome wide association studies [@burgess_mendelian_2013; @burgess_using_2015; @davies_reading_2018] \textcolor{red}{Not sure about these citations...we can discuss(Erlich and Narayanan 2014; Fuller et al. 1999; Wang et al. 2017)}. Typically, the first study provides information about the exposure and instrument and the second study provides information about the outcome and instrument. Once investigator have summary statistics from two studies, they use methods based on parametric modeling assumptions [@burgess_mendelian_2015; @burgess_review_2017] to arrive at point estimates and tests for the exposure effect.

An alternative approach to study the casual effect of the exposure without parametric assumptions is through non-parametric IV bounds [@balke_bounds_1997; @cheng_bounds_2006; @manski_nonparametric_1990; @richardson_ace_2014; @robins_analysis_1989]. Briefly, nonparametric IV bounds only use the bare minimum amount of assumptions, usually (A1)-(A3), to provide a range of plausible values for the exposure effect. They are typically used when the outcome, the exposure, and the instrument are all binary and are simultaneously observed. The most well-known are the Balke-Pearl bounds for the average treatment effect under slight variants of assumptions (A1)-(A3), and a set of instrumental inequalities to falsify the IV assumptions. Since then, @cheng_bounds_2006 and @richardson_ace_2014 extended the Balke-Pearl bounds to allow for a non-binary instrument. @ramsahai_causal_2012 derived bounds for the two-sample setting where the exposure and instrument are observed from one study and the outcome and instrument are observed from another study. @bpbounds-package provides software to compute IV bounds for two-sample MR studies using only summary statistics. For a recent overview, see Swanson et al. (2018). 

Due to their nonparametric nature, the IV bounds are attractive approaches to analyze exposure effects especially if some modeling assumptions are suspect or difficult to justify. In MR, the minimal set of assumptions needed for non-parametric bounds is a stark contrast to many other MR approaches, such as the IVW estimator [@burgess_mendelian_2013], MR-Egger regression [@bowden_assessing_2016], weighted median [@bowden_consistent_2016] and modes [@hartwig_robust_2017], MRRAPs [@zhao_statistical_2020], and others, that make parametric assumptions about the exposure effect. More generally, if IV bounds using fewer assumptions arrive at similar conclusions about the exposure effect as those based on parametric approaches, the case for the causal effect on the exposure becomes stronger. 

Despite their attractive properties, there is a poor understanding about the behavior of IV bounds in two-sample MR studies using only summary statistics. The specific questions we will tackle in our work are

\begin{enumerate}
\item What kind of genetic instruments are needed to provide useful conclusions about the exposure effect, say the bound does not contain the null effect?
\item Can combining multiple instruments lead to shorter and tighter bounds on the exposure effect? 
\item How do the bounds change if many instruments are weak, which is typically the case in MR studies based on genetic instruments? 
\end{enumerate}

In traditional setups for IV bounds where individual-level data consisting jointly of the outcome, the exposure, and a single instrument are available, the Balke-Pearl bounds are usually conservative and contain the null effect. However, it is not clear if the same principle holds for two-sample MR studies, especially if multiple candidate IVs are available. The goal of the paper is to address these questions and provide a more in-depth exploration of these bounds, specifically addressing what we can expect to learn and what information can be gained by utilizing multiple IVs under assumptions similar to Balke-Pearl bounds

The paper is divided as follows. Section \ref{} \textcolor{red}{Fill this after done}.

# Setup

## Review: Notation and Definitions 
\label{notation-and-definitions}

In the following, let \(X\) and \(Y\) be binary exposure and outcome, respectively, \(Z\) be a categorical instrumental variable taking values in \{0, 1, and 2\}, and \(U\) an unmeasured confounder for the effect of \(X\) on \(Y\). No assumptions about the structure of \(U\) are made. Let \(Y^{z,x}\) be the potential outcome [@rubin_estimating_1974; @splawa-neyman_application_1990] had the subject received exposure value \(X = x\) and instrument value \(Z = z\). Throughout the paper, we assume the stable unit treatment value assumption (SUTVA) [@cox_planning_1958; @rubin_randomization_1980], formalized as $Y = \sum_{x,z} I[Z = z, X = x] Y^{x,z}$ and $I[\cdot]$ is the indicator function.

We make the following set of assumptions about the instrument, the exposure, the outcome, and the unmeasured confounder that are typical in MR studies; see @didelez_mendelian_2007 and @wang_bounded_2018 for details

\begin{itemize}
\tightlist
\item[(A1)] \emph{(Relevance)}: $Z \not\perp X$ 
\item[(A2)] \emph{(Independent instrument)}: $Z \perp U$
\item[(A3)] \emph{(Exclusion restriction)}: $Y^{z,x} = Y^{z',x} = Y^{x}$ for all $x,z,z'$
\item[(A4)] \emph{(Conditional ignorability of $X,Z$ given $U$)}: $Y^{z,x} \perp Z, X | U$
\end{itemize}

Briefly, assumption (A1) can be assessed by finding SNPs that have been consistently associated with the exposure through multiple GWAS (Marigorta et al. 2018). Assumption (A2) is usually checked based on scientific theory surrounding how the genetic instrument was inherited from the parents to the offspring. Assumption (A3) states that there is no direct effect of the instrument \(Z\) on the outcome \(Y\) other than that through the exposure \(X\) and like assumption (A2), is assessed by scientific theory. Both assumptions (A2) and (A3) can be violated if the SNP is (i) in linkage disequilibrium with an unmeasured SNP that affects the exposure and outcome, (ii) pleiotropic and has multiple functions beyond affecting the exposure, or (iii) under population stratification, to name a few. For a more in-depth discussion of (A1)-(A3) in MR studies, see @lawlor_mendelian_2008. Finally (A4) states that if $U$ is observed, then it is sufficient to unconfound the relationship between $X$ and $Y$. 

We make a few additional remarks about assumptions (A1)-(A4). First, most MR studies only make assumptions (A1)-(A3) along with some modeling assumptions \textcolor{red}{Cite MR book}\textcolor{blue}{ RMT:  The burgess and Thompson book?}. Second, the role of assumption (A4) is to mainly show the role that an unmeasured confounder $U$ plays in potentially allowing identification of the causal effect of the treatment if it were measured; @richardson_ace_2014 showed that one can remove (A4) and strengthen (A2) with $Z \perp U, Y^{z,x}$ and arrive at the same IV bounds of Balke and Pearl. Third, under SUTVA and assumptions (A3)-(A4), we have $Y \perp Z | X, U$, which is another common way to express the exclusion restriction in MR studies [@didelez_mendelian_2007]. Fourth, for simplicity, we do not assume the the existence of a potential treatment $X^{z}$; the existence of $X^z$ does not change the IV bounds [@swanson_partial_2018; @richardson_ace_2014], and its primarily purpose is to define a "causal" instrument [@hernn_instruments_2006].

We conclude by introducing two assumptions and defining instrument strength; the assumptions are not necessarily to construct bounds, but will help us explain the behavior of the IV bounds. First, we state the assumptions restricting the direction of the instrument's effect on the exposure and the outcome.

\begin{itemize}
\item[(A5)] \emph{(Monotonicity between $Z$ and $X$)} $P(X = 1 | Z = z, U) \le P(X = 1 | Z = z+1, U)$ for $z=0,1$
\item[(A6)] \emph{(Monotonicity between $Z$ and $Y$)} $P(Y = 1 | Z = z, U) \le P(Y = 1 | Z = z+1, U)$ for $z=0,1$
\end{itemize}

A variant of assumption (A5) is common in the IV literature to study noncompliance [@angrist_identification_1996; @baiocchi_instrumental_2014]. Assumption (A6) is an extension of assumption (A5) to the outcome variable. Assumptions (A5) or (A6) is plausible in MR if the direction of the effects of the genetic instrument on the exposure or the outcome are well-established from scientific theory and replication of findings from many observational studies. 

Second, we define instrument strength as the maximum possible contrast between the exposure when instruments take on different values 

\begin{equation}
\text{ST} = \max_{z_1 \neq z_2} | P(X = 1 | Z = z_1) - P(X = 1 | Z = z_2) | \label{eq:strength}
\end{equation}

ST matches the definition of instrument strength used in @balke_bounds_1997 when the instrument is binary; in that work, it was used to justify how the width of the IV bounds changed as a function of $\text{ST}$. However, \eqref{eq:strength} differs from other definitions of strength based on a parametric model between the exposure and the outcome, say the concentration parameter; @stock_survey_2002 for an overview.


## Review: Study Designs and Target Estimand 

There are roughly two designs of IV studies, the two-sample design and the one-sample design. The two-sample design has two separate data sources, one providing information of \((X,Z)\) and one providing information of \((Y,Z)\), and is the most popular design in MR studies. The one-sample design has a single data source providing information on all observed variables \((X,Y,Z)\) and is more common in traditional IV studies involving non-genetic instruments. Also, the behavior of bounds under a one-sample design has been well-studied more extensively than in two-sample design [@swanson_partial_2018]. 

In a MR study under a two-sample design, investigators often rely on summary statistics from GWAS to study the exposure effect. When both the outcome and the exposure are binary as is the case for case-control study, these summary statistics are computed by running a logistic regression between the exposure $X$ and the outcome $Y$ for each genetic instrument $Z$ and extracting the estimated slope coefficients associated with $Z$; it's also common for the logistic regression to adjust for age, sex, and principal components. To focus our paper on studying behavior of bounds not due to sampling errors, we will assume that we have population-level quantities $P(Y = 1 | Z = z)$ from one data source and $P(X = 1 | Z = z)$ from another data source for different values of $z$.

The focus of the paper is on the average treatment effect (ATE)

\[
\text{ATE} = E[Y^1 - Y^0] = \int P(Y=1 \mid X = 1, U=u) P(U=u) du - \int P(Y=1 \mid X = 0, U=u) P(U=u) du, 
\]

where the second equality follows from SUTVA and assumptions (A3)-(A4). Since $U$ is not observed, additional assumptions are needed to point-identify the ATE. In particular, even with the remaining assumptions (A1), (A2), and (A5), the ATE cannot be point-identified; see @robins_analysis_1989, @manski_nonparametric_1990, and @balke_counterfactuals_1995. 

In one-sample designs, sharp bounds on the ATE are well-established under assumptions (A1)-(A4) [@balke_bounds_1997; @richardson_ace_2014; @swanson_partial_2018]; these bounds can also be used when individual-level data are not available, but population summary statistics in the form of $P(Y = y, X = x | Z = z)$ for $y,x,z$ are known. In two-sample designs, @ramsahai_causal_2012 showed that under assumptions (A1)-(A4), the bounds for the ATE are

\[
\begin{aligned}
\max &\left \{
\begin{array}{ll}
  \max_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) - 2\cdot P(Y = 1 | Z = z_2) - 2\cdot P(X = 1 | Z = z_2) \\
  \max_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) + P(X = 1 | Z = z_1) - P(Y = 1 | Z = z_2) - P(X = 1 | Z = z_2) - 1 \\
  \max_{z_1 \neq z_2} & 2\cdot P(Y = 1 | Z = z_1) + 2\cdot P(X = 1 | Z = z_1) - P(Y = 1 | Z = z_2) - 3 \\
  \max_z & -P(Y = 1 | Z = z) - P(X = 1 | Z = z) \\
  \max_z & P(Y = 1 | Z = z) +  P(X = 1 | Z = z) - 2
\end{array}
\right \} \\ \\
& \qquad \qquad \qquad \qquad \le ATE \le \\ \\
& \qquad \quad \min \left \{
\begin{array}{ll}
  \min_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) - 2\cdot P(Y = 1 | Z = z_2) +  2\cdot P(X = 1 | Z = z_2) + 1 \\
  \min_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) + 2\cdot P(Y = 1 | Z = z_2) -  2\cdot P(X = 1 | Z = z_2) + 1 \\
  \min_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) - P(X = 1 | Z = z_1) + P(X = 1 | Z = z_2) - P(Y = 1 | Z = z_2) + 1 \\
  \min_z & P(X = 1 | Z = z) - P(Y = 1 | Z = z) + 1 \\
  \min_z & P(Y = 1 | Z = z) - P(X = 1 | Z = z) + 1 
\end{array} 
\right \} \label{eq:ate_bound}
\end{aligned}
\]

Additionally, the data from two-sample designs can be used to check the validity of the assumptions

\begin{equation}
\min \left\{
  \begin{array}{ll}
    \min_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) - P(X = 1 | Z = z_1) - P(Y = 1 | Z = z_2) - P(X = 1 | Z = z_2) + 2 \\
    \min_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) + P(X = 1 | Z = z_1) - P(Y = 1 | Z = z_2) + P(X = 1 | Z = z_2) \\
    \min_{z} & P(X = 1 | Z = z) \\
    \min_{z} & P(Y = 1 | Z = z) \\
    \min_{z} & 1 - P(X = 1 | Z = z) \\
    \min_{z} & 1 - P(Y = 1 | Z = z) 
  \end{array} 
\right \} \ge 0 \label{eq:constraints}
\end{equation}

The inequalities in equation \eqref{eq:constraints} are extensions of the "IV inequalities" of @balke_bounds_1997 used to check the validity of the IV assumptions. Versions of these inequalities have been used in MR studies [@diemer_application_2020] to check whether the genetic variants satisfy the IV assumptions. In the Appendix, we provide some details on deriving equations \eqref{eq:ate_bound} and \eqref{eq:constraints} as well as implementing the procedure using Polymake [@assarf_computing_2017], a linear program solver in R. We also discuss a minor, but important numerical issue concerning ordering of the bounds; we believe this issue is pertinent among investigators who are using a linear-program based software to compute these bounds [@palmer_nonparametric_2011]. \textcolor{blue}{ RMT: Bounds from Richardson and Robins 2014 agree with these, and therefore also result in LB > UB every now and then. Not sure how they derive their expression, though...}

# Properties of Bounds from Summary-Level Data

```{r include = FALSE}
violation_summaries <- read_csv(here::here("data/many_tri_bounds_violations.csv")) %>%
  rename(upper_smallest = `upper < lower`)
```


<!-- Non-parametric bounds obtained from trivariate data sources have been thoroughly explored in the past for binary instruments. These bounds come with a few very desirable properties. For one, under the assumptions presented in Section \ref{iv-assumptions-and-two-sample-mr}, the width is guaranteed to be less than $1 - \text{ST} = P(X = 0 | Z = 1) - P(X = 1 | Z = 0)$ [@balke_nonparametric_1993]. The bounds are computationally easy to calculate, and rely only on summary level data. In particular, with values of $P(X = x, Y = y | Z = z)$, non-parametric bounds can be obtained. As mentioned, this allows us to avoid many of the privacy concerns that often arise when handling genetic data. -->

<!-- Scenarios where $k > 2$ has, to our knowledge, not been thoroughly explored. @richardson_ace_2014 provide bounds for a general categorical instrumental variable, but the behavior of these is not described in detail. Since the main scope of this paper is to explore bounds from bivariate data sources, we will not spend much time on the trivariate case, but will note that, from simulations, it seems like the width of trivariate bounds are indeed still bounded by $1 - \text{ST}$ for $k=3$ and $k=4$ (Figure \ref{fig:trivariate-bound-on-width}).  -->

<!--  ```{r include = FALSE} -->
<!--  violation_summaries <- read_csv(here::here("data/many_tri_bounds_violations.csv")) %>% -->
<!--    rename(upper_smallest = `upper < lower`) -->
<!--  ``` -->

<!-- \begin{figure}[H] -->
<!--   \center -->
<!--   \includegraphics[width = 0.99\linewidth]{`r here::here("figures/trivariate_widths_vs_strengths.png")`} -->
<!--   \caption{The results of calculating widths of bounds. `r sum(subset(violation_summaries, k == 3)[['n']])` distributions of $(X,Y|Z)$ were randomly generated for both $k = 3$ and $k = 4$. For $k = 3$, `r format(sum(subset(violation_summaries, k == 3 & violations)[['n']]), scientific = F, big.mark = ",")` of these violated one or more of the constraints. For $k = 4$, `r format(sum(filter(violation_summaries, k == 4, violations)[['n']]), scientific = F, big.mark = ",")` violated one or more of the constraints, and `r format(filter(violation_summaries, k == 4, !violations, upper_smallest)[['n']], scientific = F, big.mark = ",")` resulted in an upper bound that is smaller than the lower bound. These are not included here. Black line indicates $\text{Width} = 1-\text{ST}$.} -->
<!--   \label{fig:trivariate-bound-on-width} -->
<!-- \end{figure} -->

<!-- In Mendelian randomization, trivariate data sources are scarce. Bivariate data sources, on the other hand, are plentiful. The rest of this section is dedicated to explore the behavior of non-parametric bounds derived from bivariate data sources. These were first introduced by @ramsahai_causal_2007.  -->



## Bounds from Bivariate Data

We begin our investigation of bounds in equation \eqref{eq:ate_bound} under two-sample MR studies with summary data when there is a single instrument. We are interested in whether we can gain any insights into the direction and magnitude of the ATE by examining the length of the bounds; wide bounds provide less information about the magnitude of the ATE, and are much less likely to provide any information regarding direction as compared to narrower bounds.

Theorem \ref{thm:uppderBoundWidth} shows the width of the ATE bound in equation \eqref{eq:ate_bound} under a near-ideal MR study where all the assumptions (A1)-(A6) hold; in addition to having some evidence in support of assumptions (A1)-(A4) that are needed to obtain the bound in equation \eqref{eq:ate_bound}, the investigator knows that the genetic instrument has a monotonic effect on the exposure and the outcome for every value of the unmeasured value. Theoretically, the extra assumptions (A5)-(A6) simplify the bound formula in equation \eqref{eq:ate_bound}, allowing us to precisely characterize the width of the min/max inequalities.

\begin{theorem}\label{thm:upperBoundWidth}
Under assumptions (A1)-(A6), the bounds for the ATE in \eqref{eq:ate_bound} become

\[
  \begin{aligned}
    &\max
      \begin{Bmatrix}
        -P(Y = 0 | Z = 2) - P(Y = 1 | Z = 0) + P(X = 0 | Z = 0) - P(X = 0 | Z = 2) \\
        P(Y = 0 | Z = 0) - 2\cdot P(Y = 0 | Z = 2) - P(X = 0 | Z = 2) \\
        -P(Y = 0 | Z = 2) - 2\cdot P(Y = 1 | Z = 0) + P(X = 0 | Z = 0)
      \end{Bmatrix} \\
    &\qquad \qquad \qquad \qquad \qquad\le ATE \le \\
    &\qquad \qquad \qquad \min
      \begin{Bmatrix}
        1 + P(Y = 0 | Z = 0) - P(X = 0 | Z = 0) \\
        1 + P(Y = 0 | Z = 0) - P(Y = 0 | Z = 2) - P(X = 0 | Z = 0) + P(X = 0 | Z = 2) \\
        1 - P(Y = 0 | Z = 2) +  P(X = 0 | Z = 2)
      \end{Bmatrix}
  \end{aligned}
\]

and the width of the above bounds is bounded from above by $2 - 2\cdot \text{ST}$. \textcolor{red}{is this upper bound sharp when (A1)-(A6) hold? That is, there exists a DGP that satisfy (A1)-(A6) and the width of the bound from it is equal to the upper bound?}\textcolor{blue}{ RMT: Yes. I did construct a DGP artificially that gave width 2, but we also have lots of examples from the simulation of DGPs that hit the 2 - 2 ST line. I just checked, and 31 of those also satisfy the monotonicity assumptions.}
\end{theorem}

The proof is presented in Appendix \ref{proof-of-theorem}. The bounds under the near-ideal MR setting is \textcolor{blue}{ RMT: up to (?)} twice as large as the Balke-Pearl bounds with a binary IV in single-sample designs where the width is $1-\text{ST}$. An instrument with strength $\text{ST} = 0.6$ would lead to a smaller bound with width $0.4$ under a binary IV, single-sample design setting compared to a length of \textcolor{blue}{ RMT: up to (?)} $0.8$ in the near-ideal MR study. The \textcolor{blue}{ RMT: potential(?)} doubling of the bound length under two-sample MR with summary data is a "cost" of using both non-binary instruments and two-sample designs. In particular, two-sample designs do not provide any information about the joint distribution of $P(Y,X | Z)$, which can tighten the bounds; see Section \ref{quasi-bayesian} where we exploit this phenomena to obtain more informative bounds in MR studies. Also, the width of the bounds in Theorem \ref{thm:upperBoundWidth} is only guaranteed to be less than 1 when the instrument strength ST is greater than 0.5; a bound with length greater than 1 provides no information about the existence of the exposure effect since it will always cover zero. However, this does not imply that instruments with strength less than $0.5$ have length less than $1$ (see Figure \ref{fig:biv_bounds_vs_strength} for examples).

\textcolor{red}{We've been mostly focusing on width, but is it possible to derive sufficient condition about when $0 <$ lower bound OR when $0> UB$? This would help us justify the centering plot in Fig 1a.?} \textcolor{blue}{ RMT: I'll have to think more about this... I think it would require a more rigorous simulation study, since we would need the ability to control the ATE.}

\textcolor{red}{Contrary to what we discussed before, the more I think about this, the more I feel like we should avoid discussing this result since we don't exactly know what's going on with the bounds with LB $>$ UB and simply simulate until we have bounds that not only satisfy the IV inequalities above but pass basic sanity checks? It also distracts from the main message of the paper, I think. I did mention the LB $>$ UB issue above, just in case.} \textcolor{blue}{ RMT: Maybe move discussion entirely to appendix?}

```{r echo = FALSE, message = FALSE, warning = FALSE}
upper_less_than_lower <- read.csv(here::here("tables", "upper_less_than_lower.csv"), check.names = FALSE) %>% 
  mutate(across(everything(), ~format(.x, scientific = FALSE, big.mark = ",")))

proportion_with_width <- read.csv(here::here("tables", "proportion_of_biv_widths_greater_than.csv"), check.names = FALSE)

proportion_with_width_latex <- proportion_with_width %>% 
  mutate(
    Strength = paste0("$", Strength, "$"),
    across(
      where(is.numeric),
      ~paste0("$", sprintf(.x, fmt = "%.7f"), "$")
    )
  ) %>%
  rename_with(
    ~str_split(.x, "= ", simplify = TRUE)[,2],
    -1
  ) %>% 
  kableExtra::kable(format = "latex", row.names = FALSE, booktabs = TRUE, escape = FALSE, align = "lccc") %>% 
  kableExtra::add_header_above(
    header = c(" " = 1, "Proportion of bounds with\n width greater than..." = 3)
  )
```

To illustrate our theorem, we randomly generate `r upper_less_than_lower[["n"]]` sets of values of \(P(X = 1 | Z = z)\) and \(P(Y = 1 | Z = z)\) that satisfy the IV inequalities, and calculate the corresponding bounds from equation \eqref{eq:ate_bound}. This simulation mimics a scenario where there is a uniform/flat prior over the possible summary statistics that can arise from two-sample MR studies satisfying assumptions (A1)-(A4) and\textcolor{red}{fix this later?: provides a benchmark to compare with real data} \textcolor{blue}{ RMT: not sure what you mean?}. Figure \ref{fig:biv_bounds_vs_strength} shows the bounds for `r upper_less_than_lower[["good"]]`; the remaining `r upper_less_than_lower[["bad"]]` did not satisfy the constraints \textcolor{blue}{ RMT: Just to be clear, the 123 did in fact satisfy the constraints, but resulted in LB > UB.}

\begin{figure*}
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{`r here::here("figures/all_bivariate_bounds.png")`}
    \caption{Bounds ordered by the center of the bounds.}
    \label{fig:all_biv_bounds}
  \end{subfigure}%
  ~
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\textwidth]{`r here::here("figures/bivariate_width_vs_strength.png")`}
    \caption{Black line has intercept 2 and slope -2.}
    \label{fig:biv_width_vs_strength}
  \end{subfigure}
  \caption{`r upper_less_than_lower[["n"]]` values for bivariate distributions were randomly generated such that no constraints were violated. Of these, `r upper_less_than_lower[['bad']]` resulted in bounds where the lower bound was greater than the upper bounds. These have been removed from these plots.}
  \label{fig:biv_bounds_vs_strength}
\end{figure*}

\textcolor{red}{I think Fig1a may not be as useful as I originally thought...I think Fig a could be perhaps replaced by a version of Fig 1b, but where we actually plot the smoking data example here? Or even find a GWAS in MR-Base where $X$ and $Y$ have strong causal link and there's a very strong genetic determinant (e.g. cholesterol/obesity to heart attack?)} \textcolor{blue}{ RMT: I will try to find such an example.}

<!-- Figure \ref{fig:all_biv_bounds} shows the bounds sorted by their midpoints. What is interesting about this figure is that the entire interval -1 to 1 is covered, and the widths of the bounds vary quite a bit. In many cases, we do indeed see widths greater than 1. It is important to note that the center of the bounds carry no real significance in this context, but is simply used to sort these bounds for illustrative purposes. -->

Figure \ref{fig:biv_width_vs_strength} shows the widths of the same `r upper_less_than_lower[["good"]]` bounds plotted against the strength of the instruments. The black line is the upper bound for the width of the bounds in Theorem \ref{thm:upperBoundWidth}. We see that the width of the bounds frequently exceed $1$. In particular, Table \ref{tab:prop_of_biv_widths_large} shows that the proportion of the intervals presented on Figure \ref{fig:biv_bounds_vs_strength} with width greater than 1, 0.75, and 0.5, stratified by strength. The table reveals that while is possible to observe bounds with width less than 1, for IVs with strength less than 0.05, $`r round(proportion_with_width[["w = 1"]][1], digits = 3)*100`\%$ of the bounds lead to widths greater than 1 and about $`r round(proportion_with_width[["w = 1"]][2], digits = 3)*100`\%$ of bounds from IVs with strength between \(0.05\) and \(0.1\) have width greater than 1. Also, only $`r (1-round(proportion_with_width[["w = 0.5"]][5], digits = 3))*100`\%$ of bounds with strength greater than \(0.5\) have widths less than \(0.5\).


\begin{table}[H]
  \begin{center}
  \caption{Proportion of bounds from distributions where width is greater than $1$, $0.75$, and $0.5$ stratified by strength of the instrument $Z$ on the exposure $X$.}
  \label{tab:prop_of_biv_widths_large}
  `r proportion_with_width_latex`
  \end{center}
\end{table}

Overall, in the context of two-sample MR studies with summary statistics, most genetic instruments are weak, which means that the chances that bivariate bounds from MR analyses are informative with width less than 1 are very slim. In particular, a strength requirement of $\text{ST} > 0.5$ to guarantee a bound with length less than $1$ is a tall order in many MR studies. \textcolor{red}{Perhaps, convert the difference in probability to roughly what it would correspond to in a logistic model, either as an odds ratio or beta coef? Even tying it to the smoking example by saying that one must find a genetic variant that can change the odds of smoking by a factor of BLANK would be helpful to understand for MR/GWAS practitioners looking at odds ratios/z-stats/p-vals all day.} \textcolor{blue}{ RMT: Can try to do this, but since these are randomly generated, they don't necessarily fit the logistic model setting, i.e. monotonicity might be violated. Thoughts on how to express ST as an OR? Could maybe do a $max OR_{z_1,z_2}$.} Additionally, as our numerical results revealed, a set of bounds with width just below 1 does not provide much more information; a bound of \([-0.1, 0.8]\) for the ATE does not indicate that the average treatment effect is more likely to be positive than a bound of \([-0.7, 0.2]\) for the ATE.

\textcolor{red}{Another good thing that we can perhaps include is a power curve of sorts? That is, given that the effect size to detect is equal to $0.2 = E[P(Y | X=1, U) - P(Y | X = 0, U)]$, what kind of instrument do we need to detect this?}

\textcolor{blue}{Would have to restructure a bit to have more of a simulation rather than a "randomly draw values" setup.}


## Bounds With Multiple IVs 

<!-- In Section \ref{properties-of-bounds-from-summary-level-data}, we found that non-parametric bounds derived from bivariate data require rather strong instrumental variables to guarantee useful results. It seems that there simply is not enough information in bivariate data. One natural question is to ask is whether we can aggregate the information from multiple instrumental variables to obtain a set of improved bounds. In this section, we will consider one approach to do just that, and try to characterize what gains can be expected. -->

<!-- To maintain the close connection to MR analyses, we will consider a logistic model that is often used in MR analyses. Specifically, let -->

<!-- $$\begin{aligned} -->
<!-- \text{logit}(P(X = 1 | Z_1 = z_1, ..., Z_n = z_n)) &= \beta_0 + \sum_i \beta_i z_i \\ -->
<!-- \text{logit}(P(Y = 1 | X = x)) &= \gamma_0 + \gamma_1 x, -->
<!-- \end{aligned}$$ -->

<!-- where $\text{logit}(a) = \frac{1}{1 + \exp(-a)}$, $y \in \{0,1\}, x \in \{0,1\}$, $z_i \in \{0, 1, 2\}$, and $\beta_i, \gamma_j \in \mathbb{R}$. This particular model is often used in GWAS studies that focus on associations between genetic variants and binary phenotypes (for example, a quick search of the GWAS Central [@beck_gwas_2020] for "logistic regression" returns 177 hits **NOTE: MANY OF THE ARTICLES ASSOCIATED WITH THESE DO NOT MENTION LOG REG IN METHODS.**). Furthermore, $P(Z_j = 0) = P(Z_j = 2) = 0.25$ and $P(Z_j = 1) = 0.5$, which is in line with what would be expected in a Mendelian randomization scenario where pairs of binary alleles are randomly chosen. Here, we will use $\gamma_0 = -2, \gamma_1 = 0.2$.  -->

<!-- To avoid any unpleasant surprises due to randomness from simulating actual data, we decided to integrate (either exactly or numerically depending on the value of $n$) to find the probabilities $P(Y = 1 | Z_j = z_j)$ and $P(X = 1 | Z_j = z_j)$. We draw the coefficients $\beta_i \sim \text{Uniform}(0, 1/n)$. This is a scenario similar to what we encounter in the MR setting. Figure \ref{fig:bounds_vs_strength_many_IVs_varying_betas} shows the resulting bounds for the three different values of $n$ plotted against the strengths of the IVs. -->

<!-- \begin{figure}[H] -->
<!--   \center -->
<!--   \includegraphics[width = .99\linewidth]{`r here::here("figures", "varying_betas_bounds_vs_strength_no_mono.png")`} -->
<!--   \caption{Bounds based on probabilities derived from the logistic model. Here, the coefficients are randomly chosen as $\text{Uniform}(0, 1/n)$ for different values of $n$.}   -->
<!--   \label{fig:bounds_vs_strength_many_IVs_varying_betas} -->
<!-- \end{figure} -->

<!-- Figure \ref{fig:bounds_vs_strength_many_IVs_varying_betas} shows that when many valid IVs are present, the individual bivariate upper and lower bounds seems to be monotonically decreasing and increasing, respectively, as the strength of the IV increases. This means intersections of intervals from many IVs will result in an interval very similar, if not identical, to the most narrow of the individual bounds, which in turn will be the interval derived using the strongest of the IVs.  -->

<!-- Another interesting, although not surprising, observation is the shrinking of the strengths of the indiviudal instruments when the total number of instruments increases. With many instruments, the linear combination $\beta_0 + \sum_i \beta_i z_i$ will generally be relatively large, simply by chance. This means that the effect on $P(X = 1 | Z_1, ..., Z_n)$ of a single instrument being 1 instead of 0 is quite small. This is important. In Section \ref{bounds-from-bivariate-data}, we saw that chances of obtaining informative bounds with weak instruments are slim. If one believes the true model includes many instruments, chances are that these are relatively weak. -->

<!-- In MR analyses based on bivariate data from GWAS results, this indicates that aggregating information from multiple instruments in a simple manor, such as taking interceptions, will not result in more information than simply using the best of the individual bounds. This strategy is only appropriate in a situation where all the proposed instruments are valid. If we on the other hand want to protect ourselves against using bounds from a potentially invalid instrument by taking the union of the bounds from many proposed instruments, we would end up with very conservative bounds. This would in turn provide even less information about the ATE. This is partly due to the lack of guarantees on the width of the bounds obtained from bivariate data (Section \ref{bounds-from-bivariate-data}), which means it is very unlikely that a collection of instruments all provide informative bounds. When combining bounds through unions, only attributes shared among all bounds will be preserved, so if just one weak instrument is included, the resulting union bounds will provide very little information. -->

Prior section revealed that bounds from two-sample MR studies with summary data require a strong instrument to guarantee meaningful widths. However, it did not address whether the bound can be used improved upon by using multiple instruments. In this section, we consider one approach to aggregate bounds across multiple instruments.

Formally, consider the following logistic model for the outcome when there are multiple instruments

\[
\begin{aligned}
\text{logit}(P(X = 1 | Z_1 = z_1, ..., Z_n = z_n)) &= \beta_0 + \sum_i \beta_i z_i \\
\text{logit}(P(Y = 1 | X = x)) &= \gamma_0 + \gamma_1 x,
\end{aligned}
\]

where \(\text{logit}(a) = \frac{1}{1 + \exp(-a)}\), \(y \in \{0,1\}, x \in \{0,1\}\), \(z_i \in \{0, 1, 2\}\), and \(\beta_i, \gamma_j \in \mathbb{R}\). This particular model is popular in MR \textcolor{red}{I would actually use MR methods/studies to justify this model; I think some of the parametric approaches that I mentioned above use this model}. \textcolor{red}{I should have caught this earlier, but are we simulating unmeasured $U$? Otherwise, the effect of $X$ on $Y$ is not confounded at all and you would be able to estimate without needing IVs}\textcolor{blue}{RMT: I seem to recall we previously discussed this way back when... Right now we do not have any unmeasured confounders, but I'm not sure how to include these when we don't really simulate data, but rather numerically integrate.}
<!-- This particular model is often used in GWAS studies that focus on associations between genetic variants and binary phenotypes (for example, a quick search of the GWAS Central (Beck, Shorter, and Brookes 2020) for ``logistic regression'' returns 177 hits \textbf{NOTE: MANY OF THE ARTICLES ASSOCIATED WITH THESE DO NOT MENTION LOG REG IN METHODS.}). -->

We set \(P(Z_j = 0) = P(Z_j = 2) = 0.25\) and \(P(Z_j = 1) = 0.5\) and \(\gamma_0 = -2, \gamma_1 = 0.2\). We also generate $\beta_i$ from an i.i.d. uniform distribution with support $0$ to $1/n$\textcolor{red}{Ralph, can we actually draw this from our smoking data's distribution? This way, it doesn't seem like the simulation is highly contrived?}. \textcolor{blue}{RMT: done. One problem with this is that these coefficients are much smaller, which leads to weaker instruments, which makes it nearly impossible to distinguish the intervals from one another.} \textcolor{red}{I just realized that perhaps we should use $p$ instead of $n$ to denote number of IVs since $n$ is typically used for sample size?} \textcolor{blue}{Agree. Will fix}. We then obtain summary-level statistics \(P(Y = 1 | Z_j = z_j)\) and \(P(X = 1 | Z_j = z_j)\) by numerically integration the above model.

<!-- which is in line with what would be expected in a Mendelian randomization scenario where pairs of binary alleles are randomly chosen. Here, we will use \(\gamma_0 = -2, \gamma_1 = 0.2\). -->

Figure \ref{fig:bounds_vs_strength_many_IVs_varying_betas} shows the resulting bounds for the three different values of \(n\) plotted against the strengths of the IVs.

\textcolor{red}{Thinking about this figure more closely and what this section represents, I actually think we need a bit more in-depth discussion about the behavior of multi-IV bounds when IV may be invalid, specifically discussing (1) union bounds and (2) finding a region of $[-1,1]$, say $\mathcal{S}$ where more than 50\% of bounds contain $\mathcal{S}$; the latter idea is inspired by the fact that if majority of the IVs are valid, then all these valid IV bounds will contain the ATE and thus, we only need to find the region of $[-1,1]$ where majority of the IV bounds agree on.}

\begin{figure}[H]
  \center
  \includegraphics[width = .99\linewidth]{`r here::here("figures", "varying_betas_bounds_vs_strength_no_mono.png")`}
  \caption{Bounds based on probabilities derived from the logistic model. Here, the coefficients are randomly chosen as $\text{Uniform}(0, 1/n)$ for different values of $n$.}
  \label{fig:bounds_vs_strength_many_IVs_varying_betas}
\end{figure}

<!-- Figure \ref{fig:bounds_vs_strength_many_IVs_varying_betas} shows that when many IVs are present, the individual bivariate upper and lower bounds seems to be monotonically decreasing and increasing, respectively, as the strength of the IV increases. This means intersections of intervals from many IVs will result in an interval very similar, if not identical, to the most narrow of the individual bounds, which in turn will be the interval derived using the strongest of the IVs. -->

<!-- Another interesting, although not surprising, observation is the shrinking of the strengths of the individual instruments when the total number of instruments increases. With many instruments, the linear combination \(\beta_0 + \sum_i \beta_i z_i\) will generally be relatively large, simply by chance. This means that the effect on \(P(X = 1 | Z_1, ..., Z_n)\) of a single instrument being 1 instead of 0 is quite small. This is important. In Section \ref{bounds-from-bivariate-data}, we saw that chances of obtaining informative bounds with weak instruments are slim. If one believes the true model includes many instruments, chances are that these are relatively weak. -->

Our results suggest that if all the instruments are valid, aggregating information from multiple instruments through intersections  will not result in more information than simply using a single, strongest individual bound. If, on the other hand, some instruments do not satisfy the IV assumptions and one takes a conservative approach by taking unions of bounds, the resulting bound would be valid in the sense that it will cover the true ATE. But, the union bound would be extremely conservative and ultimately provide even less information about the ATE. While other approaches to aggregation are possible, \textcolor{red}{work on later}

Combining our investigation into the behavior of bounds from Section \ref{bounds-from-bivariate-data}, our general conclusion is that constructing bounds for two-sample MR studies with summary data rely on few assumptions, but are rarely informative. The two primarily reasons for this is that (1) genetic instruments in MR are generally weak, leading to wide bounds and (2) only two-sample data is available to construct bounds. Without having strong instruments in the study, reason (1) is generally difficult, if not impossible, to address with any statistical methodology. Also, unless we change how MR data is collected, going from one-sample to two-sample studies, reason (2) is challenging, but not necessarily impossible, to address. In particular, in the next section, we discuss how to obtain a plausible range of the joint distribution of the outcome and the exposure given the instrument $P(Y, X | Z)$ given two sample MR data $P(Y|Z)$ and $P(X | Z)$ in order to create more informative bounds from two-sample MR studies.


# What can you do with summary-level data for bounds? A Quasi-Bayesian Path to More Information
\label{quasi-bayesian}

Our approach to creating more informative bounds from two-sample MR rests on creating a plausible range of the joint distribution of the outcome and the exposure given the instrument $Z$, \(P(X = x, Y = y | Z = z)\). The plausible range of the joint distribution is informed by quantities available from two-sample MR studies, specifically \(P(X = x | Z = z)\) and \(P(Y = y | Z = z)\), as well as the constraints imposed by the IV assumptions.

Formally, the joint conditional distribution \(P(X = x, Y = y | Z = z)\) is a function of the marginal conditional distributions \(P(X = x | Z = z)\) and \(P(Y = y | Z = z)\) and the conditional covariance of the exposure $X$ and $Y$ given $Z=z$  \(\text{Cov}(X, Y | Z = z)\) for each \(z\)

\begin{equation}
P(X = x, Y = y | Z = z) = P(X = x | Z = z)P(Y = y | Z = z) + (2\cdot I[x = y] - 1)\text{Cov}(X, Y | Z = z). \label{eq:cov-expression}
\end{equation}

Since $\text{Cov}(X, Y | Z = z)$ is impossible to estimate \textcolor{blue}{from two-sample data}, we instead propose to put a uniform flat prior on this quantity that also results in the joint conditional distribution of \((X,Y|Z)\) being an actual probability distribution and satisfying the verifiable constraints \eqref{eq:constraints} from the IV assumptions. Specifically, by the definition of a proper probability distribution, $\text{Cov}(X, Y | Z = z)$ must satisfy \textcolor{red}{If you want, I think we can make a lemma out of this?} \textcolor{blue}{RMT: do you think that would be beneficial? I can definitely reword this part as a lemma.}

\[
\begin{aligned}
  \max_z\left\{
      \begin{array}{c}
        -P(X = 1 | Z = z)P(Y = 1 | Z = z) \\
        -P(X = 0 | Z = z)P(Y = 0 | Z = z) \\
        P(X = 1 | Z = z)P(Y = 0 | Z = z) - 1\\
        P(X = 0 | Z = z)P(Y = 1 | Z = z) - 1
      \end{array}
    \right\} & \\
    \le \text{Cov}(X, &Y | Z = z) \le \\
    &\min_z\left\{
      \begin{array}{c}
        1 - P(X = 1 | Z = z)P(Y = 1 | Z = z) \\
        1 - P(X = 0 | Z = z)P(Y = 0 | Z = z) \\
        P(X = 1 | Z = z)P(Y = 0 | Z = z) \\
        P(X = 0 | Z = z)P(Y = 1 | Z = z)
      \end{array}
    \right\}
\end{aligned}
\]

Additionally, by the IV inequality constraints, for any pair of \((z_1, z_2) \in \{0,1,2\} \times \{0,1,2\}\), the values of \(\text{Cov}(X, Y | Z = z_1)\) and \(\text{Cov}(X, Y | Z = z_2)\) must satisfy

\[
\begin{aligned}
  \max\left\{
      \begin{array}{c}
        -P(X = 0 | Z = z_1)P(Y = 0 | Z = z_1) - P(X = 0 | Z = z_2)P(Y = 1 | Z = z_2) \\
        P(X = 1 | Z = z_1)P(Y = 0 | Z = z_1) + P(X = 1 | Z = z_2)P(Y = 1 | Z = z_2) -1 \\
        P(X = 0 | Z = z_2)P(Y = 0 | Z = z_2) + P(X = 0 | Z = z_1)P(Y = 1 | Z = z_1) - 1 \\
        -P(X = 1 | Z = z_2)P(Y = 0 | Z = z_2) - P(X = 1 | Z = z_1)P(Y = 1 | Z = z_1)
      \end{array}
    \right\} \qquad \qquad & \\ \\
    \le \text{Cov}(X,Y | Z = z_1) - \text{Cov}(X,Y | Z = z_2) \le \qquad \qquad \qquad \qquad  \qquad& \\ \\
    \min\left\{
      \begin{array}{c}
        1 -P(X = 0 | Z = z_1)P(Y = 0 | Z = z_1) - P(X = 0 | Z = z_2)P(Y = 1 | Z = z_2) \\
        P(X = 1 | Z = z_1)P(Y = 0 | Z = z_1) + P(X = 1 | Z = z_2)P(Y = 1 | Z = z_2) \\
        P(X = 0 | Z = z_2)P(Y = 0 | Z = z_2) + P(X = 0 | Z = z_1)P(Y = 1 | Z = z_1) \\
        1 - P(X = 1 | Z = z_2)P(Y = 0 | Z = z_2) - P(X = 1 | Z = z_1)P(Y = 1 | Z = z_1)
      \end{array}
    \right\} &
\end{aligned}
\]

Then, we sequentially sample values of \(\text{Cov}(X, Y | Z = 0), \text{Cov}(X, Y | Z = 1), \text{Cov}(X, Y | Z = 2)\), such that the above inequalities plus the existing constraints in \eqref{eq:constraints} are satisfied. Then, among samples of \(\text{Cov}(X, Y | Z = 0), \text{Cov}(X, Y | Z = 1), \text{Cov}(X, Y | Z = 2)\) that satisfy the constraints, we calculate the joint distribution of \(P(X = x, Y = y | Z = z)\) using \eqref{eq:cov-expression}. Ultimately, we have a plausible set of the joint distribution \(P(X = x, Y = y | Z = z)\).

For each plausible set of the joint distribution of \(P(X = x, Y = y | Z = z)\), we use the usual IV bounds by @balke_bounds_1997 and @richardson_ace_2014 from one-sample IV studies to obtain a bound for the ATE. If a large number of these bounds do not cover zero, then there is some evidence to suggest of a causal effect of the exposure and only reason we are not able to detect this effect is due to the limitations of the two-sample design. However, if the majority of these IV bounds do cover zero, there is less evidence to suggest a causal exposure effect and/or that utilizing bounds to obtain some information about the ATE may be a hopeless exercise. In short, we are trying answer "had we observed one-sample data that satisfies the constraints of the two-sample data we currently have, could we have detected the presence of an exposure effect?"

The approach above can be thought of as using a quasi-empirical bayesian framework for partially identified sets. Specifically, our procedure generates a posterior distribution of IV bounds given the marginalized probabilities from two-sample data (i.e. the likelihood) and a uniform, flat prior on the unknown quantities \(\text{Cov}(X, Y | Z = z)\). The constraints that we impose on \(\text{Cov}(X, Y | Z = z)\) are almost empirically Bayesian in nature as they are informed by data from two-sample MR.

<!-- ## Sampling Procedure -->

<!-- The joint conditional distribution $P(X = x, Y = y | Z = z)$ can be constructed from the marginal conditional distributions $P(X = x | Z = z)$ and $P(Y = y | Z = z)$ if we know the values of $\text{Cov}(X, Y | Z = z)$ for each $z$, since  -->

<!-- \begin{equation} -->
<!-- P(X = x, Y = y | Z = z) = P(X = x | Z = z)P(Y = y | Z = z) + (2\cdot I[x = y] - 1)\text{Cov}(X, Y | Z = z). \label{eq:cov-expression} -->
<!-- \end{equation} -->

<!-- Since these covariances are essentially completely unknown to us, we draw them uniformly from the set of values that result in the joint conditional distribution of $(X,Y|Z)$ being an actual probability distribution satisfying the verifiable constraints from \eqref{eq:constraints}.  -->

<!-- This set of values is not trivial, but fortunately we can find a superset of values that is much smaller than $[-1,1]^k$. When implementing this approach, we propose a set of covariances by sampling from this superset, construct the trivariate distribution, and check if any constraints are violated. If there are any violations, the proposed set of covariances is discarded, and a new set proposed.  -->

<!-- Combining $0 \le P(X = x, Y = y | Z = z) \le 1$ for all values of $z = 0, 1, ..., k-1$ with \eqref{eq:cov-expression}, we see that  -->

<!-- \begin{equation*} -->
<!-- -P(X = x | Z = z) P(Y = y | Z = z) \le \text{Cov}(X, Y | Z = z) \le 1 - P(X = x | Z = z)P(Y = y | Z = z) -->
<!-- \end{equation*} -->

<!-- when $x = y$, and  -->

<!-- \begin{equation*} -->
<!-- P(X = x | Z = z) P(Y = y | Z = z) - 1 \le \text{Cov}(X, Y | Z = z) \le P(X = x | Z = z)P(Y = y | Z = z) -->
<!-- \end{equation*} -->

<!-- when $x \neq y$. Since this holds for all $z = 0,1,...,k-1$, we find that $\text{Cov}(X, Y | Z = z)$ must be such that  -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!--   \max_z\left\{  -->
<!--       \begin{array}{c} -->
<!--         -P(X = 1 | Z = z)P(Y = 1 | Z = z) \\ -->
<!--         -P(X = 0 | Z = z)P(Y = 0 | Z = z) \\  -->
<!--         P(X = 1 | Z = z)P(Y = 0 | Z = z) - 1\\ -->
<!--         P(X = 0 | Z = z)P(Y = 1 | Z = z) - 1 -->
<!--       \end{array}  -->
<!--     \right\} & \\  -->
<!--     \le \text{Cov}(X, &Y | Z = z) \le \\ -->
<!--     &\min_z\left\{  -->
<!--       \begin{array}{c} -->
<!--         1 - P(X = 1 | Z = z)P(Y = 1 | Z = z) \\ -->
<!--         1 - P(X = 0 | Z = z)P(Y = 0 | Z = z) \\  -->
<!--         P(X = 1 | Z = z)P(Y = 0 | Z = z) \\ -->
<!--         P(X = 0 | Z = z)P(Y = 1 | Z = z) -->
<!--       \end{array}  -->
<!--     \right\} -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- Furthermore, enforcing the IV inequalities $\max_x \sum_y \max_z P(X = x, Y = y | Z = z) \le 1$, we get constraints on the differences between $\text{Cov}(X, Y | Z = z_1)$ and $\text{Cov}(X, Y | Z = z_2)$. For any $x=0,1$, and any pair $(z_1, z_2) \in \{0,1,...,k-1\} \times \{0,1,...,k-1\}$, we see that $0 \le P(X = x, Y = 0 | Z = z_1) + P(X = x, Y = 1 | Z = z_2) \le 1$ (where the first inequality is a result of summing two positive quantities). From \eqref{eq:cov-expression}, we also see that for $x=0$, -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- 0 \le & \\  -->
<!-- & P(X = 0 | Z = z_1)P(Y = 0 | Z = z_1) + \text{Cov}(X, Y | Z = z_1) + P(X = 0 | Z = z)P(Y = 1 | Z = z_2) - \text{Cov}(X, Y | Z = z_2) \\  -->
<!-- & \le 1, -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- which is equivalent to  -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- -P(X = 0 | Z = z_1)P(Y = 0 | &Z = z_1) - P(X = 0 | Z = z)P(Y = 1 | Z = z_2) \\ -->
<!-- \le \text{Cov}(X, Y | Z = z_1) &- \text{Cov}(X, Y | Z = z_2) \le \\ -->
<!-- 1 - P(X = 0 | &Z = z_1)P(Y = 0 | Z = z_1) - P(X = 0 | Z = z)P(Y = 1 | Z = z_2). -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- A similar exercise can be done for $x = 1$. The result is that, for any pair of $(z_1, z_2) \in \{0,1,...,k-1\} \times \{0,1,...,k-1\}$, the values of $\text{Cov}(X, Y | Z = z_1)$ and $\text{Cov}(X, Y | Z = z_1)$ must satisfy   -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!--   \max\left\{  -->
<!--       \begin{array}{c} -->
<!--         -P(X = 0 | Z = z_1)P(Y = 0 | Z = z_1) - P(X = 0 | Z = z_2)P(Y = 1 | Z = z_2) \\  -->
<!--         P(X = 1 | Z = z_1)P(Y = 0 | Z = z_1) + P(X = 1 | Z = z_2)P(Y = 1 | Z = z_2) -1 \\ -->
<!--         P(X = 0 | Z = z_2)P(Y = 0 | Z = z_2) + P(X = 0 | Z = z_1)P(Y = 1 | Z = z_1) - 1 \\ -->
<!--         -P(X = 1 | Z = z_2)P(Y = 0 | Z = z_2) - P(X = 1 | Z = z_1)P(Y = 1 | Z = z_1) -->
<!--       \end{array}  -->
<!--     \right\} \qquad \qquad & \\ \\ -->
<!--     \le \text{Cov}(X,Y | Z = z_1) - \text{Cov}(X,Y | Z = z_2) \le \qquad \qquad \qquad \qquad  \qquad& \\ \\ -->
<!--     \min\left\{  -->
<!--       \begin{array}{c} -->
<!--         1 -P(X = 0 | Z = z_1)P(Y = 0 | Z = z_1) - P(X = 0 | Z = z_2)P(Y = 1 | Z = z_2) \\  -->
<!--         P(X = 1 | Z = z_1)P(Y = 0 | Z = z_1) + P(X = 1 | Z = z_2)P(Y = 1 | Z = z_2) \\ -->
<!--         P(X = 0 | Z = z_2)P(Y = 0 | Z = z_2) + P(X = 0 | Z = z_1)P(Y = 1 | Z = z_1) \\ -->
<!--         1 - P(X = 1 | Z = z_2)P(Y = 0 | Z = z_2) - P(X = 1 | Z = z_1)P(Y = 1 | Z = z_1) -->
<!--       \end{array}  -->
<!--     \right\} &  -->
<!-- \end{aligned} -->
<!-- $$ -->


<!-- To create a possible set of values of $P(X = x, Y = y | Z = z)$, we sequentially draw values for $\text{Cov}(X, Y | Z = 0), \text{Cov}(X, Y | Z = 1), ..., \text{Cov}(X, Y | Z = k-1)$, such that the above inequalities hold, calculate the values of $P(X = x, Y = y | Z = z)$ using \eqref{eq:cov-expression}, and check that the constraints in \eqref{eq:constraints} are satisfied. If any of the constraints are violated, the values are rejected, and the procedure repeated until we have a set of values for $\text{Cov}(X, Y | Z = 0), \text{Cov}(X, Y | Z = 1), ..., \text{Cov}(X, Y | Z = k-1)$ that result in a trivariate probability distribution that satisfies the constraints in \eqref{eq:constraints}.  -->

<!-- The ultimate goal of this exercise is to try to assess whether knowledge about the full trivariate probabilities would allow us to determine direction of the ATE. In most cases there is not a firm answer to this question, as some trivariate probabilities result in bounds that would, while other trivariate probability distributions result in bounds that would not. So, we are really trying to asses questions such as "given the bivariate probabilities, what is the chance that the trivariate data would allow us to determine direction?"  -->

<!-- The approach presented here can, under the right set of assumptions, be interpreted as generating a sample from the posterior distribution over all possible trivariate distributions given the marginalized probabilities, and a uniform prior on the unknown quantities $\text{Cov}(X, Y | Z = z)$. This means that we can obtain posterior probabilities of certain events. In particular, we will be interested in the posterior probability that the trivariate bounds contain $0$. We will use this as a heuristic measure of the loss of information in going from trivariate to bivariate data.  -->

## Single Instrument

We illustrate the potential utility of the proposed method from the previous section by considering nine hypothetical MR studies, each using one instrument; see Section \ref{multiple-iv-case} for the case with multiple instruments. Table \ref{tab:subset_plot_summaries_b} presents nine different sets of values of the marginal distributions $P(Y|Z)$ and $P(X | Z)$ from nine hypothetical two-sample MR studies \textcolor{red}{Do you think we can find an empirical example to support some of these nine from MR-Base? I'm thinking about using popular exposures in the MR field (like CRP, cholesterol, or vitamin D?)} \textcolor{blue}{ RMT: I will take a look.}.

```{r include = FALSE}
subset_plot_summaries <- readr::read_rds(here::here("vignettes_data", "subset_plot_summaries.Rds"))

subset_plot_summaries_for_print <- subset_plot_summaries %>% 
  mutate(p_no_zero = 1 - p_no_zero) %>% 
  rename(Row = row_i, Column = col_j,
         Lower = bivariate_lower,
         Upper = bivariate_upper) %>% 
  #        `Proportion overlapping 0` = p_no_zero) %>% 
  select(Row, Column, Lower, Upper, everything())

subset_plot_summaries_for_print_A <- subset_plot_summaries_for_print %>% 
  select(-starts_with("P(")) %>% 
  mutate(p_no_zero = sprintf(fmt = "%.2f", p_no_zero * 100),
         across(c(Lower, Upper), round, digits = 3),
         across(c(Row, Column), ~paste(cur_column(), .x)),
         cells = glue::glue("[{Lower}, {Upper}]\n {p_no_zero}\\%")) %>% 
  select(" " = Row, Column, cells) %>% 
  mutate(across(everything(), kableExtra::linebreak)) %>% 
  pivot_wider(names_from = Column, values_from = cells) %>% 
  knitr::kable(format = "latex", escape = FALSE, booktabs = TRUE) 


subset_plot_summaries_for_print_B <- subset_plot_summaries_for_print %>% 
  select(Row, Column, starts_with("P(")) %>% 
  mutate(across(starts_with("P("), ~sprintf("%.3f", .x))) %>% 
  unite(col = "pxz", c(3:5), sep = ", ") %>% 
  unite(col = "pyz", 4:6, sep =", ") %>% 
  mutate(
    across(
      c(pxz, pyz),
      ~paste0("\\{", .x, "\\}")
    )
  ) %>% 
  unite(col = "p", 3:4, sep = "\n") %>% 
  mutate(across(p, kableExtra::linebreak),
         across(c(Row, Column), ~paste(cur_column(), .x))) %>% 
  pivot_wider(names_from = Column, values_from = p) %>% 
  rename(" " = Row) %>% 
  knitr::kable(format = "latex", escape = FALSE, booktabs = TRUE) 

# subset_plot_summaries_for_print_B <- subset_plot_summaries_for_print %>% 
#   select(Row, Column, starts_with("P(")) %>% 
#   knitr::kable(format = "latex", digits = 3,
#                col.names = c("Row", "Column", rep(paste("z =", 0:2), 2))) %>% 
#   kableExtra::add_header_above(header = c(" " = 2, "P(X = 1 | Z = z)" = 3, "P(Y = 1 | Z = z)" = 3))
```


\begin{table}[H]
  \center
  \caption{Values of $P(X = 1 | Z = z)$ and $P(Y = 1 | Z = z)$ used to illustrate our quasi-bayesian approach. These are presented with $\{P(X = 1 | Z = 0), P(X = 1 | Z = 1), P(X = 1 | Z = 2)\}$ on the first row, and $\{P(Y = 1 | Z = 0), P(Y = 1 | Z = 1), P(Y = 1 | Z = 2)\}$ on the second row.}
  \label{tab:subset_plot_summaries_b}
  `r subset_plot_summaries_for_print_B`
\end{table}


<!-- \begin{table}[H] -->
<!--   \center -->
<!--   \caption{For each of the nine panels displayed in figure \ref{fig:trivariate_bounds}, this table includes lower and upper bounds based on the bivariate data, and proportion of trivariate distributions overlapping 0.} -->
<!--   \label{tab:subset_plot_summaries_a} -->
<!--   `r subset_plot_summaries_for_print_A` -->
<!-- \end{table} -->

\begin{figure}[H]
  \center
  \includegraphics[width=\linewidth]{`r here::here("figures", "trivariate_bounds_subset_plot.png")`}
  \caption{One-sample and two-sample bounds. Even similar bivariate distributions can result in very different insights.}
  \label{fig:trivariate_bounds}
\end{figure}

Figure \ref{fig:trivariate_bounds} shows the resulting bounds only using two-sample data and the set of plausible one-sample IV bounds from our procedure. Row a shows three scenarios where the two-sample bounds are all more or less centered around zero with similar widths. However, the conclusions are rather different. Column 1 shows no one-sample bounds would allow us to determine the presence of a non-zero causal effect. Column 2 indicates that about `r filter(subset_plot_summaries, row_i == "a", col_j == 2)[["p_no_zero"]]*100`\% of the one-sample IV bounds does not contain $0$ while for column 3 that number is approximately `r filter(subset_plot_summaries, row_i == "a", col_j == 3)[["p_no_zero"]]*100`\%. However, while the direction of the effect is always the same for column 2 (positive), it varies for column 3.

Row b illustrates three scenarios where the two-sample bounds are centered well above zero and have large widths. Here, we see one case where we have no hope of determining direction from the one-sample bounds (column 1), one case where we are most likely to be able to determine the direction of the ATE to be positive from the one-sample bounds (column 2), and one case where we are rather unlikely to be able to determine the direction of the ATE from the one-sample bounds (column 3).

Row c is similar to row a in that all the two-sample bounds are centered around 0, but the width of the two-sample bounds are narrow. The three columns indicate similar conclusions as seen in row a. This shows that even with rather narrow two-sample bounds centered around 0, the one-sample bounds may still be have to reveal some information about presence as well as the direction of the exposure effect.

Some caution should be exercised when interpreting the proportion of one-sample bounds not containing $0$. A scenario like the one resulting in the bounds presented in row b, column 2 only provides information about the one-sample bounds, but only if our flat prior on $\text{Cov}(X,Y|Z)$ is correct. Under this prior, it tells us that it is much more likely that the ATE is positive. It does not, however, rule out a negative value of the ATE, but it does rule out the possibility of one-sample bounds being able to determine direction *if the ATE is in fact negative*. More generally, the conclusion we can reach from our approach hinges on the instrument satisfying the IV assumptions presented in Section \ref{iv-assumptions-and-two-sample-mr}.


## Multiple Instruments

Although the bivariate bounds often do not provide much information themselves, as we saw in the previous section, the little information available can sometimes provide some insights. The approach presented draws on the fact that trivariate bounds are guaranteed to be much narrower than bivariate bounds. It remains to be seen if utilizing such an approach while aggregating information from multiple IVs through intersections of bounds can be useful.

The simplest extension to the multiple IV scenario, is to simply repeat the sampling procedure presented in Section \ref{sampling-procedure} for each proposed instrument before creating the combined bounds by taking the intersection of the pairwise bounds. This builds on one main assumption in that the two sampling procedures are done independently, and so implicitly assume that the covariances of $X$ and $Y$ given $Z_1$ are independent of the covariances of $X$ and $Y$ given $Z_2$. 

Specifically, say we get bounds $(LB_{1i},UB_{1i}),i = 1,2,...,m$ by sampling m trivariate distributions based on the information we have on $(X,Z_1)$ and $(Y,Z_1)$, and bounds $(LB_{2i}, UB_{2i}),i = 1,2,...,m$ by sampling $m$ trivariate distributions based on the information we have on $(X,Z_2)$ and $(Y,Z_2)$. We then create the intersection bounds as $\left(\max_{z \in {1,2}} LB_{zi}, \min_{z \in {1,2}} UB_{zi}\right), i = 1, 2, ..., m$. This, under the assumption that $\text{Cov}(X, Y | Z_1 = z)$ and $\text{Cov}(X, Y | Z_2 = z)$ are independent of each other, gives us a sample from the posterior distribution of intersection bounds. We can use this to assess the potential usefulness of aggregating information from two sets of trivariate data, $(X, Y, Z_1)$ and $(X, Y, Z_2)$, using intersection bounds.

We will illustrate this approach in the next section using data obtained from MRBase. 


# Data Analysis

We consider two example analyses to demonstrate our findings above. Specifically, we aim to study the effect of smoking on lung cancer and depression through an MR analysis. The effect of smoking on lung cancer is well-known to be strong and causal and serves as a positive control to demonstrate the usefulness of the MR analysis via bounds. The effect of smoking on depression has also been studied in other works [@wootton_evidence_2019]\textcolor{red}{cite from other MR paper as well as citations within that MR paper}. In both cases, we will explore the non-parametric bounds obtained from two-sample designs and what conclusions are attainable based on our approach.

```{r include = FALSE}
experiments <- read_csv(here::here("vignettes_data/example_analyses/experiment_info.csv")) %>% 
  filter(id %in% c("ukb-d-20116_0", "ukb-d-20544_11", "ukb-d-40001_C349"))
```

The data to study both effects was obtained from the UK Biobank data curated at the IEU GWAS database, which is available in R through the \texttt{TwoSampleMR} package [@mrbase].  Specifically, data on smoking was obtained from data entry with ID `r filter(experiments, str_detect(tolower(trait), "smoking"))[['id']]`, data on depression from entry with ID `r filter(experiments, str_detect(tolower(trait), "depression"))[['id']]`, and data on lung cancer from entry with ID `r filter(experiments, str_detect(tolower(trait), "lung"))[['id']]`. We followed the defaults of the package where linkage disequilibrium based clumping (\(r^2 \ge 0.001\) within a \(10,000\) kb window using \(p < 5 \times 10^{-8}\) as the level of significance) were performed such that only independent instruments with significant associations are returned. The data was harmonized to make sure that the effects of the SNPs on exposure and outcome were measured with the same allele as reference. Afterwards, we obtain the estimated coefficients from previous GWAS experiments corresponding to the effects of the SNPs on the exposure, and the outcome from a logistic model. Since estimates of the intercept are included in these reported results, but marginal proportions of the outcome, exposure, and allele frequencies are known, we find the intercepts by solving \(P(X = 1) = \sum_{z = 0}^2\text{logit}(\beta_0 + \hat{\beta_1}\cdot z)\cdot P(Z = z)\) and \(P(Y = 1) = \sum_{z = 0}^2\text{logit}(\gamma_0 + \hat{\gamma_1}\cdot z)\cdot P(Z_j = z)\) for \(\beta_0\) and \(\gamma_0\), respectively. Overall, the `TwoSampleMR` package along with our estimates of the intercept allowed us to calculate  \(P(Y = 1 | Z_j = z)\) and \(P(X = 1 | Z_j = z)\) for every \(j\) and \(z=0,1,2\); see [link to vignette showing analysis on pkgdown page] for the code. 

\textcolor{red}{We should stress test this intercept finding (and ultimately, recovering $P(Y|Z)$ and $P(X|Z)$) procedure? For example, I know the FTO-genetic marker is very strongly associated with obesity and hopefully,  $P(X|Z=z) - P(X | Z= z')$ is large? Or, we can also simulate to verify this procedure...} 

\textcolor{blue}{RMT: Found a study in mrbase with obesity as outcome. Strongest IVs $\approx 0.065$. Is that large? As for other stress tests, simulations is probably the way to go. Will implement in the coming days.}

## Smoking effect on depression

<!-- In our first example, we explore the effect smoking has as an exposure on the outcome depression. It has previously been suggested that smoking increases the risk of depression. @wootton_evidence_2019 estimated the odds ratio using an inverse-variance weighted method to be 1.99 with a 95\% confidence interval of [1.71, 2.32] suggesting a relatively strong causal effect.  -->

Previous reports suggested that smoking increases the risk of depression, with the most recent estimate suggesting the odds ratio to be 1.99 with a 95\% confidence interval of [1.71, 2.32] [@wootton_evidence_2019] based on a non-IV approach. Our MR analysis uses 84 genetic variants as instruments and Figure \ref{fig:strength_histogram} shows a histogram of their strength. We see here that the strength of the strongest instrument is less than 0.01, which is much smaller than the 0.5 needed to guarantee narrow bounds. More information on the instruments and summary statistics can be found in Appendix \ref{more-details-data-application-appendix}.

\begin{figure}[H]
  \center
  \includegraphics[width = 0.99\linewidth]{`r here::here("figures/example_analyses/strength_histogram.png")`}
  \caption{Histogram of strengths of IVs on the exposure. Here, SNPs are IVs, and smoking status (ever/never) is exposure. We see that all IVs are very weak, with the largest value just below 0.01.}
  \label{fig:strength_histogram}
\end{figure}

From the 84 instruments, we obtain 84 sets of two-sample bounds and they are shown in Figure \ref{fig:smoking_on_depression_ind_bounds} \textcolor{red}{Can we transpose this graph so that as we go down the page, we have snps and the x-axis is the bounds?}. From this figure, it is immediately clear that all the intervals are practically identical and none of them provide any information about the presence of an effect. Also, aggregating across 84 bounds, either by intersection or union, will not reveal any useful insights about the exposure effect. This is not surprising given that all instruments are very weak and our analysis in Section 3.1 showed that weak instruments in two-sample settings tend to lead to large bounds.

\begin{figure}[H]
  \includegraphics[width = 0.99\linewidth]{`r here::here("figures/example_analyses/smoking_depression_bivaraite_bounds_ukb-d-20116_0_ukb-d-20544_11.png")`}
  \caption{Non-parametric bounds for the 84 genetic variants identified as potential instruments for the effect of smoking on depression created based on the bivariate probabilities found from GWAS summary statistics.}
  \label{fig:smoking_on_depression_ind_bounds}
\end{figure}

Next, we use our approach to generate plausible one-sample IV bounds and Figure \ref{fig:smoking_on_depression_tri_bounds} shows the resulting bounds \textcolor{red}{can we only present a few of these snps, and put rest in appendix? We can say other bounds are similar to what we have in the main.}. While the one-sample bounds are much narrower than the corresponding two-sample bounds, in line with our expectations, all the one-sample bounds founds contain $0$. This means that we will not be able to use non-parametric bounds to determine the direction of the ATE of smoking on the chances of developing depression, even if we were to obtain one-sample data. In other words, the bound-based analysis of the MR study may not yield any useful conclusion about the exposure effect.

\clearpage

\begin{sidewaysfigure}
  \includegraphics[width = 0.99\textheight]{`r here::here("figures/example_analyses/smoking_depression_individual_SNPs_plot_ukb-d-20116_0_ukb-d-20544_11.png")`}
    \caption{500 sets of bounds of the average treatment effect of smoking on depression for each of the 84 SNPs. Each bound is based on a set of values for the trivariate distribution randomly sampled. Bounds are color coded to show if they overlap 0 (grey) or do not (red). All bounds overlap 0.}
    \label{fig:smoking_on_depression_tri_bounds}
\end{sidewaysfigure}

\clearpage

\textcolor{red}{fix after discussing what to do with multiple IV section} Aggregating the information from multiple IVs through intersections is a simple idea, but everything we have seen so far points to this not being useful in practice. Figure \ref{fig:smoking_on_depression_intersections} shows the results from doing exactlt this for 9 pairs of SNPs, both when simply creating intersection of the bivariate bounds, and when using our quasi-bayesian approach to estimate the distribution of intersections of bounds from trivariate distributions as described in Section \ref{multiple-iv-case}. Comparing Figure \ref{fig:smoking_on_depression_intersections} to Figure \ref{fig:smoking_on_depression_tri_bounds}, we notice that the intersection bounds are essentially the same width. As for intersections of trivariate bounds, these are narrower than the corresponding intersections of bivariate bounds, but we do not see any scenario where intersections of trivariate bounds would help us determine direction of the ATE. Again, the conclusion is that no sets of trivariate distributions allows us to determine direction of the average treatment effect through the use of non-parametric bounds.

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.99\linewidth]{`r here::here("figures/example_analyses/smoking_depression_intersection_bounds_plot_ukb-d-20116_0_ukb-d-20544_11.png")`}
  \caption{Intersection bounds of the average treatment effect of smoking on depression based on randomly sampled trivariate distributions from pairs of SNPs. These 9 pairs were randomly chosen from all possible pairs.}
  \label{fig:smoking_on_depression_intersections}
\end{figure}

<!-- Data on $(X|Z)$ is obtained from the experiment with id `r filter(experiments, stringr::str_detect(trait, "Smoking"))[['id']]` in the MRBase database, and data on $(Y|Z)$ is obtained from the experiment with id `r filter(experiments, stringr::str_detect(trait, "Depression"))[['id']]`. We find 84 genetic variants that can be used as instruments for the effect of smoking on depression. Using these 84 genetic variantes, we want to first explore if non-parametric bounds can be used to obtain any information about the average treatment effect of smoking on the chances of developing depression. We will follow up on this with an exploration of the conclusions our quasi-bayesian approach can provide in this specific example. Finally, we will take a look at intersections of these bounds, and comment on the usefulness of this approach in this practical setting. -->

<!-- The genetic markers that were identified here as potential instruments all look very similar in terms of the estimated conditional distributions of exposure and outcome given instruments. Figure \ref{fig:smoking_on_depression_marginals} shows histograms of the values of $P(X = 1|Z = z)$ and $P(Y = 1|Z = z), z = 0,1,2$, for all 84 instruments. As you can see, there is very little variation in the actual values. Especially the small change when moving from $z = 0$ through $z = 1$ to $z = 2$ on the three left panels is of interest here, as this is our first indication that these instruments might be rather weak.  -->

<!-- \begin{figure}[H] -->
<!--   \includegraphics[width = 0.99\linewidth]{`r here::here("figures", "example_analyses", "smoking_depression_marginal_conditionals.png")`} -->
<!--   \caption{Histograms of the marginal conditional probabilities $P(X = 1 | Z = z), z = 0,1,2$ and $P(Y = 1 | Z = z), z=0,1,2$.} -->
<!--   \label{fig:smoking_on_depression_marginals} -->
<!-- \end{figure} -->


<!-- Figure \ref{fig:strength_histogram} shows a histogram of the strength of all the potential instruments. We see here that the strength of the strongest instrument is less than 0.01, which is much smaller than the 0.5 needed to guarantee narrow bounds, and well in line with the observation on Figure \ref{fig:smoking_on_depression_marginals}. More information on the instruments and summary statistics can be found in Appendix \ref{more-details-data-application-appendix}. -->

<!-- \begin{figure}[H] -->
<!--   \center -->
<!--   \includegraphics[width = 0.99\linewidth]{`r here::here("figures/example_analyses/strength_histogram.png")`} -->
<!--   \caption{Histogram of strengths of IVs on the exposure. Here, SNPs are IVs, and smoking status (ever/never) is exposure. We see that all IVs are very weak, with the largest value just below 0.01.} -->
<!--   \label{fig:strength_histogram} -->
<!-- \end{figure} -->

<!-- From the 84 instrumental variables found, we obtain 84 sets of bounds using the values of $P(X = 1 | Z_j = z), P(Y = 1 | Z_j = z), z = 0,1,2,\ j=1,2,...,84$. These are shown on Figure \ref{fig:smoking_on_depression_ind_bounds}. From this figure, it is immediately clear that all the intervals are practically identical. This means that none of these instruments result in bounds that can help us determine direction of the ATE, and, in general, the amount of information they provide on the magnitude of the ATE is limited. This also tells us that trying to aggregate information from these instruments by simply creating intersection bounds will lead to essential no gains. -->

<!-- The very minimal amount of information we get from these bounds is no surprise. As already mentioned, all instruments are very weak, which as we saw in Section 3.1 tend to lead to wide bounds. -->

<!-- \begin{figure}[H] -->
<!--   \includegraphics[width = 0.99\linewidth]{`r here::here("figures", "example_analyses", "smoking_depression_bivaraite_bounds_ukb-d-20116_0_ukb-d-20544_11.png")`} -->
<!--   \caption{Non-parametric bounds for the 84 genetic variants identified as potential instruments for the effect of smoking on depression created based on the bivariate probabilities found from GWAS summary statistics.} -->
<!--   \label{fig:smoking_on_depression_ind_bounds} -->
<!-- \end{figure} -->

<!-- With this in mind, we proceed to explore our quasi-bayesian approach. For each of the 84 genetic variants, we sample 500 potential trivariate distributions as described in Section \ref{quasi-bayesian}. From these 500 trivariate distributions, non-parametric bounds are created. Figure \ref{fig:smoking_on_depression_tri_bounds} shows the resulting bounds.  -->

<!-- It is clear that while the trivariate bounds are much narrower than the corresponding bivariate bounds. This is very much so in line with our expectations. Unfortunately, all the bounds founds based on potential trivariate distributions overlap 0. This means that we will not be able to use non-parametric bounds to determine the direction of the ATE of smoking on the chances of developing depression, even if we were to obtain trivariate data.  -->

<!-- \clearpage -->

<!-- \begin{sidewaysfigure} -->
<!--   \includegraphics[width = 0.99\textheight]{`r here::here("figures", "example_analyses", "smoking_depression_individual_SNPs_plot_ukb-d-20116_0_ukb-d-20544_11.png")`} -->
<!--     \caption{500 sets of bounds of the average treatment effect of smoking on depression for each of the 84 SNPs. Each bound is based on a set of values for the trivariate distribution randomly sampled. Bounds are color coded to show if they overlap 0 (grey) or do not (red). All bounds overlap 0.} -->
<!--     \label{fig:smoking_on_depression_tri_bounds} -->
<!-- \end{sidewaysfigure} -->

<!-- \clearpage -->

<!-- Aggregating the information from multiple IVs through intersections is a simple idea, but everything we have seen so far points to this not being useful in practice. Figure \ref{fig:smoking_on_depression_intersections} shows the results from doing exactlt this for 9 pairs of SNPs, both when simply creating intersection of the bivariate bounds, and when using our quasi-bayesian approach to estimate the distribution of intersections of bounds from trivariate distributions as described in Section \ref{multiple-iv-case}. Comparing Figure \ref{fig:smoking_on_depression_intersections} to Figure \ref{fig:smoking_on_depression_tri_bounds}, we notice that the intersection bounds are essentially the same width. As for intersections of trivariate bounds, these are narrower than the corresponding intersections of bivariate bounds, but we do not see any scenario where intersections of trivariate bounds would help us determine direction of the ATE.  -->

<!-- Again, the conclusion is that no sets of trivariate distributions allows us to determine direction of the average treatment effect through the use of non-parametric bounds. -->

<!-- \begin{figure}[H] -->
<!--   \centering -->
<!--   \includegraphics[width = 0.99\linewidth]{`r here::here("figures", "example_analyses", "smoking_depression_intersection_bounds_plot_ukb-d-20116_0_ukb-d-20544_11.png")`} -->
<!--   \caption{Intersection bounds of the average treatment effect of smoking on depression based on randomly sampled trivariate distributions from pairs of SNPs. These 9 pairs were randomly chosen from all possible pairs.} -->
<!--   \label{fig:smoking_on_depression_intersections} -->
<!-- \end{figure} -->


<!-- It is very important to keep in mind that this conclusion is only valid as long as the probabilities obtained are the true population probabilities. If this is the case, then non-parametric bounds simply will not allow us to determine direction of the ATE. The accuracy of the probabilities can be questioned, as these are derived from logistic regression results.  -->

## Smoking effect on lung cancer 

As a positive control, we consider the effect of smoking on lung cancer. We use the same 84 instruments as in Section \ref{smoking-effect-on-depression} and as expected, the two-sample bounds (Figure \ref{fig:smoking_on_lung_cancer_ind_bounds}) are rather wide; all of them have width greater than 1 and they convey no truly useful information about the ATE. Additionally, even if we were to obtain one-sample data, we will not be able to determine the direction of the ATE (Figure \ref{fig:smoking_on_lung_cancer_tri_bounds}). Aggregating through intersections (Figure \ref{fig:smoking_on_lung_cancer_intersections}) does not lead to real gain in information, even if this is done using bounds based on trivariate distributions.

\begin{figure}[H]
  \includegraphics[width = 0.99\linewidth]{`r here::here("figures", "example_analyses", "smoking_lung_cancer_3_bivaraite_bounds_ukb-d-20116_0_ukb-d-40001_C349.png")`}
  \caption{Non-parametric bounds on the average treatment effect of smoking on lung cancer.}
  \label{fig:smoking_on_lung_cancer_ind_bounds}
\end{figure}

\clearpage

\begin{sidewaysfigure}
  \includegraphics[width = 0.99\textheight]{`r here::here("figures", "example_analyses", "smoking_lung_cancer_3_individual_SNPs_plot_ukb-d-20116_0_ukb-d-40001_C349.png")`}
    \caption{500 sets of bounds of the average treatment effect of smoking on lung cancer for each of the 84 SNPs. Each bound is based on a set of values for the trivariate distribution randomyl sampled. Bounds are color coded to show if they overlap 0 (grey) or do not (red). All bounds overlap 0.}
    \label{fig:smoking_on_lung_cancer_tri_bounds}
\end{sidewaysfigure}

\clearpage

\begin{figure}[H]
  \includegraphics[width = 0.99\linewidth]{`r here::here("figures", "example_analyses", "smoking_lung_cancer_3_intersection_bounds_plot_ukb-d-20116_0_ukb-d-40001_C349.png")`}
  \caption{Intersection bounds of the average treatment effect of smoking on lung cancer based on randomly sampled trivariate distributions from pairs of SNPs. These 9 pairs were randomly chosen from all possible pairs.}
  \label{fig:smoking_on_lung_cancer_intersections}
\end{figure}

The result from our positive control is a cause for concern. In particular, it is well established that smoking has a strong causal effect on the chances of developing lung cancer [@cornfield_smoking_1959]. The fact that we are unable to say anything about the ATE in this case does not leave much hope in terms of future discoveries based on non-parametric bounds from two-sample MR studies. Even more concerning is the fact that had we obtained one-sample MR data, we would still be unsuccessful in determining the direction of the effect based on a bound-based analysis of the ATE. In short, while non-parametric bounds allow us to make little assumptions about the data and as such, is robust to some common modeling assumptions in MR, they are often too conservative and are not suited for MR studies with many weak instruments.

\newpage

# Conclusion and Practical Considerations

Non-parametric bounds are without a doubt an attractive concept. With a minimal set of assumptions they let us obtain bounds on the average treatment effect. However, as we have seen here, in typical MR studies with two-sample summary data and many weak instruments, bounds may be too uninformative to make meaningful conclusions about the ATE. Specifically, non-parametric bounds in usual one-sample settings data come with very nice guarantees, such as the width always being less than 1. But, in Mendelian randomization analyses with two-sample data, we lose the strong guarantees on the maximum width of the bounds and strong assumptions about the strength of the IV are often required to make sure that the width is less than $1$. Even aggregating information from many instruments through simple intersections will only be as good as using a single strong instrument.

To address the limitations that the two-sample design has in terms of producing informative bounds, we outline an approach to generate a plausible range of one-sample bounds that are in agreement with the two-sample data at hand. This gives us the opportunity to assess the range of conclusions that can be drawn from bound-based approaches had we had one-sample data. We applied our method to a few different settings of two-sample data and showed the range conclusions about the ATE  that can be drawn from it. This exercise also highlighted a significant loss of information in two-sample designs compared to one-sample designs.


\textcolor{red}{need a better way to tie these two paragraphs to rest}
To demonstrate the use of non-parametric bounds in Mendelian randomization analyses, we considered two examples. In the first example, we aimed at finding bounds on the effect of smoking on the chances of developing depression. Unfortunately, all instruments available were very weak with the strongest instrument having a strength of less than \(0.01\). This results in bounds that provide very little information. Our approach suggests that even one-sample bounds would not be able to provide much extra information.

In our second example, we explored the effect of smoking on the chances of developing lung cancer. It has been well established that there is a rather strong causal effect of smoking on the chances of developing lung cancer. Unfortunately, our non-parametric bounds were not able to determine the direction of this effect, and the one-sample bounds once again brought marginal improvement.

%In this context, it is important to note that the conclusions made about the trivariate distributions only hold if the bivariate probabilities are correct. Whether that is the case here is questionable, as these probabilities are estimated based on logistic regression models.

Using non-parametric bounds in two-sample MR studies seem a promising idea since many MR analysis rely on a host of potentially unjustifiable modeling assumptions. But, as we seen above, the non-parametric nature of these bounds as well as the two-sample design can make these bounds often meaningless in practice. Nevertheless, one potential use case of non-parametric bounds in two-sample MR studies could be when one has prior knowledge about the direction of the effect, but wish to get a better sense of the magnitude. By knowing the sign of the effect a priori, non-parametric bounds can provide an upper limit on this magnitude. This is especially useful in cases where the exposure is known to cause harm or benefit, for example in our smoking lung cancer example where the direction of the effect of smoking on lung cancer is well known and an upper bound on this effect would tell investigators about the maximum possible effect that smoking could have on increasing the propensity of lung cancer.

\newpage

# (APPENDIX) Appendix {-}

# Proof of Theorem \ref{thm:upperBoundWidth}

First of all, we note that the bounds found using the approach previously described when we impose both of the mentioned monotonicity assumptions are as follows:

\[
  \begin{aligned}
    &\max
      \begin{Bmatrix}
        -P(Y = 0 | Z = 2) - P(Y = 1 | Z = 0) + P(X = 0 | Z = 0) - P(X = 0 | Z = 2) \\
        P(Y = 0 | Z = 0) - 2\cdot P(Y = 0 | Z = 2) - P(X = 0 | Z = 2) \\
        -P(Y = 0 | Z = 2) - 2\cdot P(Y = 1 | Z = 0) + P(X = 0 | Z = 0)
      \end{Bmatrix} 
      \begin{matrix} (L1) \\ (L2) \\ (L3) \end{matrix}  \\
    &\qquad \qquad \qquad \qquad \qquad\le ATE \le \\
    &\min
      \begin{Bmatrix}
        1 + P(Y = 0 | Z = 0) - P(X = 0 | Z = 0) \\
        1 + P(Y = 0 | Z = 0) - P(Y = 0 | Z = 2) - P(X = 0 | Z = 0) + P(X = 0 | Z = 2) \\
        1 - P(Y = 0 | Z = 2) +  P(X = 0 | Z = 2)
      \end{Bmatrix}
      \begin{matrix} (U1) \\ (U2) \\ (U3) \end{matrix}
  \end{aligned}
\]

This gives us a total of nine different expressions for the width of the bounds. Since we assume monotonicity of the effect of $Z$ on $X$, the strength simplifies to $\text{ST} = P(X = 1 | Z = 2) - P(X = 1 | Z = 0)$. 

\textbf{Width = U1 - L1}

If the upper bound is $U1$, $U1 \le U2$, which implies $P(Y = 0 | Z = 2) - P(X = 0 | Z = 2) \le 0$. Therefore,

$$\begin{aligned}
U1 - L1 &= 1 + P(Y = 0 | Z = 0) - P(X = 0 | Z = 0) + P(Y = 0 | Z = 2) + \\
        & \qquad \qquad P(Y = 1 | Z = 0) - P(X = 0 | Z = 0) + P(X = 0 | Z = 2) \\
        &= 2 - ST + P(Y = 0 | Z = 2) - P(X = 0 | Z = 0) \\
        &= 2 - 2\cdot ST + P(Y = 0 | Z = 2) - P(X = 0 | Z = 2) \le 2 - 2\cdot ST.
\end{aligned}$$

\textbf{Width = U2 - L1}

$$\begin{aligned}
U2 - L1 &= 1 + P(Y = 0 | Z = 0) - P(Y = 0 | Z = 2) - P(X = 0 | Z = 0) + P(X = 0 | Z = 2) \\
        &\qquad + P(Y = 0 | Z = 2) + P(Y = 1 | Z = 0) - P(X = 0 | Z = 0) + P(X = 0 | Z = 2) \\
        &= 2 - 2\cdot ST
\end{aligned}$$


\textbf{Width = U3 - L1}

Since the upper bound is $U3$, $U3 \le U2$, which implies $P(X = 0 | Z = 0) - P(Y = 0 | Z = 0) \le 0$. Therefore,

$$\begin{aligned}
U3 - L1 &= 1 - P(Y = 0 | Z = 2) +  P(X = 0 | Z = 2) + P(Y = 0 | Z = 2) + \\
        & \qquad \qquad P(Y = 1 | Z = 0) - P(X = 0 | Z = 0) + P(X = 0 | Z = 2) \\
        &= 1 + P(Y = 1 | Z = 0) - ST + P(X = 0 | Z = 2) \\
        &= 2 - 2\cdot ST + P(X = 0 | Z = 0) - P(Y = 0 | Z = 0) \le 2 - 2 \cdot ST.
\end{aligned}$$

\textbf{Width = U1 - L2}

Since the upper bound is $U1$, $P(Y = 0 | Z = 2) \le P(X = 0 | Z = 2)$. Since the lower bound is $L2$, $L2 \ge L1$, which gives us $1 - P(X = 0 | Z = 0) \ge P(Y = 0 | Z = 2)$. Therefore,

$$\begin{aligned}
U1 - L2 &= 1 + P(Y = 0 | Z = 0) - P(X = 0 | Z = 0) - P(Y = 0 | Z = 0) + 2\cdot P(Y = 0 | Z = 2) + P(X = 0 | Z = 2) \\
        &= 1 - ST + 2P(Y = 0 | Z = 2) \\
        &\le 2 - ST - P(X = 0 | Z = 0) + P(X = 0 | Z = 2) = 2 - 2\cdot ST.
\end{aligned}$$

\textbf{Width = U2 - L2}

Since the lower bound is $L2$, $1 - P(X = 0 | Z = 0) \ge P(Y = 0 | Z = 2)$. So,

$$\begin{aligned}
U2 - L2 &= 1 - P(X = 0 | Z = 0) + P(X = 0 | Z = 2) + P(Y = 0 | Z = 2) + P(X = 0 | Z = 2) \\
        &= 1 - ST + P(Y = 0 | Z = 2) + P(X = 0 | Z = 2) \\
        &\le 2 - 2\cdot ST.
\end{aligned}$$

\textbf{Width = U3 - L2}

Since the lower bound is $L2$, $1 - P(X = 0 | Z = 0) \ge P(Y = 0 | Z = 2)$. Since the upper bound is $U3$, $P(X = 0 | Z = 0) \le P(Y = 0 | Z = 0)$. Therefore,

$$\begin{aligned}
U3 - L2 &= 1 - P(Y = 0 | Z = 2) +  P(X = 0 | Z = 2) - P(Y = 0 | Z = 0) + 2\cdot P(Y = 0 | Z = 2) + P(X = 0 | Z = 2) \\
        &= 1 + 2\cdot P(X = 0 | Z = 2) + P(Y = 0 | Z = 2) - P(Y = 0 | Z = 0) \\
        &= 1 - 2\cdot ST + 2 P(X = 0 | Z = 0) + P(Y = 0 | Z = 2) - P(Y = 0 | Z = 0) \\
        &\le 2 - 2\cdot ST
\end{aligned}$$

\textbf{Width = U1 - L3}

Since the upper bound is $U1$, $P(Y = 0 | Z = 2) \le P(X = 0 | Z = 2)$. Since the lower bound is $L3$, $L3 \ge L1$, which implies $P(Y = 1 | Z = 0) \le P(X = 0 | Z = 2)$. So,

$$\begin{aligned}
U1 - L3 &= 2 - P(X = 0 | Z = 0) + P(Y = 0 | Z = 2) + P(Y = 1 | Z = 0) - P(X = 0 | Z = 0) \\
        &= 2 - 2\cdot ST - 2\cdot P(X = 0 | Z = 2) + P(Y = 0 | Z = 2) + P(Y = 1 | Z = 0) \\
        &\le 2 - 2\cdot ST
\end{aligned}$$

\textbf{Width = U2 - L3}

Since the lower bound is $L3$, $P(Y = 1 | Z = 0) \le P(X = 0 | Z = 2)$

$$\begin{aligned}
U2 - L3 &= 2 - 2\cdot P(X = 0 | Z = 0) + P(X = 0 | Z = 2) + P(Y = 1 | Z = 0) \\
        &= 2 - ST + P(Y = 1 | Z = 0) - P(X = 0 | Z = 0) \\
        &= 2 - 2\cdot ST + P(Y = 1 | Z = 0) - P(X = 0 | Z = 2) \le 2 - 2\cdot ST
\end{aligned}$$

\textbf{Width = U3 - L3}

Since the lower bound is $L3$, $P(Y = 1 | Z = 0) \le P(X = 0 | Z = 2)$. Since the upper bound is $U3$, $1 - P(X = 0 | Z = 0) \ge P(Y = 1 | Z = 0)$. Therefore,

$$\begin{aligned}
U3 - L3 &= 1 + P(X = 0 | Z = 2) + 2\cdot P(Y = 1 | Z = 0) - P(X = 0 | Z = 0) \\
        &\le 1 - ST + P(X = 0 | Z = 2) + 1 - P(X = 0 | Z = 0) \\
        &= 2 - 2\cdot ST.
\end{aligned}$$

\newpage

# Bounds on Average Treatment Effect

We briefly review the method presented by @ramsahai_causal_2012 to bound the average treatment effect using two-sample summary data. Let $\vec{\tau}^* = \Big(P(Y = 1 | X = 0, U), P(Y = 1 | X = 1, U), P(X = 1 | Z = 0, U), ..., P(X = 1 | Z = k-1, U)\Big) \in [0,1]^{2+k}$ and $\vec{v}^* = \Big(P(Y = 0 | Z = 0, U), ..., P(Y = 1 | Z = k-1, U), P(X = 0 | Z = 0, U), ..., P(X = 1 | Z = k-1, U), \alpha^*\Big)$ where

$$
\begin{aligned}
\alpha^* &= P(Y = 1 | X = 1, U) - P(Y = 1 | X = 0, U).
\end{aligned}
$$

Since $U \perp Z$, $E_U[P(X = x | Z = z, U)] = P(X = x | Z = z)$ and $E_U[P(Y = y | Z = z, U)] = P(Y = y | Z = z)$. Let $\vec{v} = E_U[\vec{v}^*] = \Big(P(Y = 0 | Z = 0), ..., P(Y = 1 | Z = k-1), P(X = 0 | Z = 0), ..., P(X = 1 | Z = k-1), \alpha \Big)$, where

$$
\begin{aligned}
\alpha &= E_U[P(Y = 1 | X = 1, U) - P(Y = 1 | X = 0, U)] \\
       &= E[Y^1] - E[Y^0] = \text{ATE}.
\end{aligned}
$$

Note that while $\vec{\tau}^*$ and $\vec{v}^*$ are both entirely unobervable, $\vec{v}$ consists of $k$ observable values, and one unobservable value, the ATE.

By the exclusion restriction, we have

\[
P(X = x, Y = y | Z = z, U) = P(Y = 1 | X = x, U) P(X = x | Z = z, U),
\]

which means we can define a mapping $f:[0,1]^{2+k} \mapsto \mathcal{V}$ such that $f(\vec{\tau}^*) = \vec{v^*}$ as

\[
f(y_0, y_1, x_0, x_1, ..., x_{k-1}) =
  \begin{pmatrix}
    (1-y_0)\cdot(1-x_0) + (1 - y_1)\cdot x_0 \\
    y_0\cdot (1-x_0) + y_1\cdot x_0 \\
    \vdots \\
    (1-y_0)\cdot(1-x_{k-1}) + (1 - y_1)\cdot x_{k-1} \\
    y_0\cdot (1-x_{k-1}) + y_1\cdot x_{k-1} \\
  \end{pmatrix} \label{eq:f}
\]

We define $\mathcal{V} = f([0,1]^{2+k})$.

Since $\vec{v} = E_U[\vec{v}^*]$, $\vec{v}$ must be a convex combination of $\vec{v}^*$. Let $\mathcal{H}$ be the convex hull of $\mathcal{V}$. Then $\vec{v}$ will be in $\mathcal{H}$.

Now, let $\hat{\mathcal{T}}$ be the set of extreme vertices of $[0,1]^{2+k}$, $\hat{\mathcal{V}} = f(\hat{\mathcal{T}})$, and $\hat{\mathcal{H}}$ be the convex hull of $\hat{\mathcal{V}}$. By Theorem 1 in Appendix B of @ramsahai_causal_2012, $\mathcal{H} = \mathcal{\hat{H}}$. This means that $\vec{v} \in \mathcal{\hat{H}}$. Utilizing a program such as Polymake, we can describe $\mathcal{H}$ with a set of inequalities, which give us constraints that $\vec{v}$ must satisfy.

This means that we can obtain inequalities that the components of $\vec{v}$ must satisfy by describing the extreme vertices of $[0,1]^{2+k}$, map them to $\mathcal{V}$ using the relatively simple function $f$, and then use polymake to find inequalities that characterize the convex hull of $f([0,1])^{2+k}$. This gives us a set of inequalities involving the components of $\vec{v}$. Some of these will be verifiable, as they will not include the only unobservable quantity $\alpha$. Others will not be verifiable, but will allow us to obtain bounds on the unobservable quantity $\alpha$ using the observable entries of $\vec{v}$.

<!-- This exact same approach can be used in the trivariate setting, and when imposing different extra assumptions, such as monotonicity of the effect of $Z$ on $X$ \eqref{eq:x_monotone} or $Z$ on $Y$ \eqref{eq:y_monotone}. The latter is done by imposing these assumption on $\hat{\mathcal{V}}$ by removing vectors that violate these extra assumptions. -->

<!-- One important thing to note here is that these expressions do not always result in a valid set of bounds. In our simulations we found that a small fraction of the simulated probability distributions result in bounds where the lower limit is larger than the upper limit. Whether this is a problem in practice remains to be seen, but we are not aware of any real life data sets giving rise to bounds with this behavior. The cause for this is not clear, but among possible explanations is the existence of an assumption that is not captured in the checkable constraints. It is only natural to conclude that one or more assumptions are violated when one encounters a scenario where the bounds are flipped. For more, see Appendix \ref{exploration-of-scenarios-where-bounds-are-flipped}. -->

<!-- We implemented this approach using `R` version 4.0.2 [@R] for the high level calculations, and Polymake [@assarf_computing_2017] to obtain the inequalities. -->

Following the approach from Ramsahai (2012) as outlined in Section \ref{bounds-on-average-treatment-effect}, we obtain bounds on the average treatment effect from the quantities \(P(X = 1 | Z = z)\) and \(P(Y = 1 | Z = z)\), \(z = 0,1,2\). To do so, we first write down the most extreme values of each of \(P(Y = 1 | X = x, U)\) and \(P(X = x | Z = z, U)\) for all \(x=0,1\), \(z=0,1,2\). Since these are probabilities, the extreme values are \(0\) and \(1\).

```{r echo = FALSE}
expand_grid(PY1X0U = c(0, 1),
            PY1X1U = c(0, 1),
            PY1Z0U = c(0, 1),
            PX1Z1U = c(0, 1),
            PX1Z2U = c(0, 1)) %>%
  pander::pander(split.table = Inf,
                 caption = "Most extreme values of $P(Y = 1 | X = x, U)$ and $P(X = 1 | Z = z, U)$. Here, PY1XxU = $P(Y = 1 | X = x, U)$ and PX1ZzU = $P(X = 1 | Z = z, U)$.")
```

By applying the function \(f\), as presented in \eqref{eq:f}, to each row, we get the most extreme vertices of \(P(X = x | Z = z, U)\) and \(P(Y = y | Z = z, U)\) for all \(x=0,1,\ y=0,1\) and \(z=0,1,2\).

```{r echo = FALSE}
extreme_vertices <- ACEBounds::create_vertices(n_z_levels = 3, data_format = "bivariate")

colnames(extreme_vertices) <- c(paste0("PY", rep(0:1, each = 3), "Z", rep(0:2, times = 2)),
                                paste0("PX", rep(0:1, each = 3), "Z", rep(0:2, times = 2)),
                                "$\\alpha$")

pander::pander(extreme_vertices,
               split.table = Inf,
               caption = "Most extreme values of $P(Y = y | Z = z)$ and $P(X = x | Z = z)$. Here, PYyZz = $P(Y = y | Z = z)$, PXxZz = $P(X = x | Z = z)$, and $\\alpha = P(Y = 1 | X = 1,U) - P(Y = 1 | X = 0,U)$.")
```

Theorem 1 of Ramsahai (2012) tells us that the values of \(P(X = 1 | Z = z), P(Y = 1 | Z = z),\ z = 0,1,2\) must lie in the convex hull. This means that the vector of these values must be a convex combination of the rows in the matrix above. Using this with the fact that they must sum to 1 is what enables us to use polymake to find inequalities that the values of \(P(X = 1 | Z = z)\), \(P(Y = 1 | Z = z)\), and \(\alpha\) must satisfy. In this particular case, these are as presented below. This table should be read as rows of coefficients \(PYyZz, PXxZz\) such that \(\sum_{z = 0}^2 PX1Zz \cdot P(X = 1 | Z = z) + \sum_{z = 0}^2 PY0Zz\cdot P(Y = 0 | Z = z) + PY1Z0\cdot P(Y = 1 | Z = 0) + c_\alpha \alpha \ge 0\).

```{r echo = FALSE}
bivariate_bounds_example <- ACEBounds:::matrices_from_polymake %>% 
  filter(data_format == "bivariate", !x_monotone, !y_monotone, n_z_levels == 3) %>% 
  pull(matrix) %>% .[[1]] %>% 
  select(where(~sum(abs(.x)) > 0))

colnames(bivariate_bounds_example) <- c(
  paste0("PY", c(0,0,0,1), "Z", c(0:2, 0)), 
  paste0("PX1Z", c(0:2)), 
  "$c_{\\alpha}$"
)
  

pander::pander(bivariate_bounds_example, 
               caption = "Results from polymake. Columns with all zeroes have been removed.")
```

The matrix presented in the table above simplifies to the following set of bounds on the average treatment effect. These are obtained by considering the rows above where \(c_\alpha \neq 0\).

\[
\begin{aligned}
\max &\left \{
\begin{array}{ll}
  \max_{i\neq j} & P(Y = 1 | Z = i) - 2\cdot P(Y = 1 | Z = j) - 2\cdot P(X = 1 | Z = j) \\
  \max_{i\neq j} & P(Y = 1 | Z = i) + P(X = 1 | Z = i) - P(Y = 1 | Z = j) - P(X = 1 | Z = j) - 1 \\
  \max_{i\neq j} & 2\cdot P(Y = 1 | Z = i) + 2\cdot P(X = 1 | Z = i) - P(Y = 1 | Z = j) - 3 \\
  \max_i & -P(Y = 1 | Z = i) - P(X = 1 | Z = i) \\
  \max_i & P(Y = 1 | Z = i) +  P(X = 1 | Z = i) - 2
\end{array}
\right \} \\ \\
& \qquad \qquad \qquad \qquad \le \alpha \le \\ \\
& \qquad \quad \min \left \{
\begin{array}{ll}
  \min_{i \neq j} & P(Y = 1 | Z = i) - 2\cdot P(Y = 1 | Z = j) +  2\cdot P(X = 1 | Z = j) + 1 \\
  \min_{i \neq j} & P(Y = 1 | Z = i) + 2\cdot P(Y = 1 | Z = j) -  2\cdot P(X = 1 | Z = j) + 1 \\
  \min_{i \neq j} & P(Y = 1 | Z = i) - P(X = 1 | Z = i) + P(X = 1 | Z = j) - P(Y = 1 | Z = j) + 1 \\
  \min_i & P(X = 1 | Z = i) - P(Y = 1 | Z = i) + 1 \\
  \min_i & P(Y = 1 | Z = i) - P(X = 1 | Z = i) + 1
\end{array}
\right \}
\end{aligned}
\]

Furthermore, we obtain the following checkable constraints from the rows where \(\alpha = 0\):

\begin{equation}
\min \left\{
  \begin{array}{ll}
    \min_{i\neq j} & P(Y = 1 | Z = i) - P(X = 1 | Z = i) - P(Y = 1 | Z = j) - P(X = 1 | Z = j) + 2 \\
    \min_{i\neq j} & P(Y = 1 | Z = i) + P(X = 1 | Z = i) - P(Y = 1 | Z = j) + P(X = 1 | Z = j) \\
    \min_{i} & P(X = 1 | Z = i) \\
    \min_{i} & P(Y = 1 | Z = i) \\
    \min_{i} & 1 - P(X = 1 | Z = i) \\
    \min_{i} & 1 - P(Y = 1 | Z = i)
  \end{array}
\right \} \ge 0 \label{eq:constraints}
\end{equation}

We notice that the constraints from the law of probability are recovered (the last four expressions above) along with 12 non-trivial constraints.

These bounds involve 24 different expressions on both the lower and upper end, making an algebraic exploration of the width very challenging. However, by imposing the two monotonicity assumptions \eqref{eq:x_monotone} and \eqref{eq:y_monotone}, the bounds reduce to just three on the lower end and three on the upper end:

\[
\begin{aligned}
    &\max
      \begin{Bmatrix}
        -P(Y = 0 | Z = 2) - P(Y = 1 | Z = 0) + P(X = 0 | Z = 0) - P(X = 0 | Z = 2) \\
        P(Y = 0 | Z = 0) - 2\cdot P(Y = 0 | Z = 2) - P(X = 0 | Z = 2) \\
        -P(Y = 0 | Z = 2) - 2\cdot P(Y = 1 | Z = 0) + P(X = 0 | Z = 0)
      \end{Bmatrix} \\
    &\qquad \qquad \qquad \qquad \qquad\le ATE \le \\
    &\qquad \qquad \qquad \min
      \begin{Bmatrix}
        1 + P(Y = 0 | Z = 0) - P(X = 0 | Z = 0) \\
        1 + P(Y = 0 | Z = 0) - P(Y = 0 | Z = 2) - P(X = 0 | Z = 0) + P(X = 0 | Z = 2) \\
        1 - P(Y = 0 | Z = 2) +  P(X = 0 | Z = 2)
      \end{Bmatrix}
\end{aligned}
\]

# Exploration of Scenarios Where Bounds are Flipped 

```{r include = FALSE}
bivariate_bounds <- read_rds(here::here("vignettes_data/bivariate_bounds.Rds"))

flipped_bivariate_bounds <- bivariate_bounds %>% 
  filter(upper < lower, !violations) %>% 
  arrange(width) %>% 
  select(Strength = strength_x, lower, upper, width, thetas, gammas) %>% 
  unnest_wider(thetas) %>% 
  rename(`P(X=1|Z=0)` = ...1, 
         `P(X=1|Z=1)` = ...2, 
         `P(X=1|Z=2)` = ...3) %>% 
  unnest_wider(gammas) %>% 
  rename(`P(Y=1|Z=0)` = ...1, 
         `P(Y=1|Z=1)` = ...2, 
         `P(Y=1|Z=2)` = ...3,
         `Lower Bound` = lower,
         `Upper Bound` = upper,
         Width = width) %>% 
  relocate(starts_with("P(X=1"), starts_with("P(Y=1"))

flipped_bivariate_bounds_table <- flipped_bivariate_bounds %>% 
  kableExtra::kable(format = "latex", booktabs = TRUE, longtable = TRUE, caption = "Marginal conditional probabilities resulting in bounds where the upper bound is smaller than the lower bound.", label = "upper-less-than-lower") %>% 
  kableExtra::landscape() %>% 
  kableExtra::kable_styling(
    font_size = 9, 
    latex_options = "repeat_header"
  )
```

Of `r format(nrow(bivariate_bounds), scientific = FALSE, big.mark = ",")` randomly generated sets of values for $P(X = 1 | Z = z), P(Y = 1 | Z = z),\ z = 0,1,2$, `r nrow(flipped_bivariate_bounds)` resulted in bounds where the upper limit is smaller than the lower limit without violating any of the verifiable constraints presented in \eqref{eq:constraints}. Table \ref{tab:upper-less-than-lower} gives the values of the marginal conditional distributions with the strength of the IV, the corresponding bounds, and the width. It is notable that the IVs are rather strong in all cases where we see the bounds flip, but the bounds themselves and the widths vary quite a bit. 

We first attributed this to the transition from trivariate to bivariate bounds, but later realized similar scenarios arise when dealing with trivariate bounds from four category IVs. Of `r format(sum(pull(filter(violation_summaries, k == 4), n)), scientific = FALSE, big.mark = ",")` randomly generated sets of values for $P(X = x, Y = y | Z = z),\ x=0,1,\ y=0,1,\ z=0,1,2,3$, `r filter(violation_summaries, k == 4, upper_smallest, !violations)[['n']]` result in bounds where the upper limit is smaller than the lower limit without any violation of the verifiable constraints. It is also worth noting that in a similar number of trivariate distributions randomly generated with a trichotomous instrument, we did not see any cases of flipped bounds without a violation of one or more of the verifiable constraints. Table \ref{tab:flipped-trivariates} show the bounds from these trivariate distributions with the strengths of the IVs, and the width. Again, it is interesting to see the large span of widths and strengths present. 

We have been unable to unearth a reason for why we see this phenomenon. One possible explanation is that the distributions that result in flipped bounds violate some uncheckable assumption. 

```{r include = FALSE}
tri_bounds_flipped <- read_rds(here::here("data/tri_bounds_flipped.Rds"))
tri_bounds_flipped_table <- tri_bounds_flipped %>% 
  select(Lower = lower, Upper = upper, Strength = strength, Width = width) %>% 
  arrange(Width) %>% 
  kableExtra::kable(format = "latex", 
                    booktabs = TRUE, longtable = TRUE, 
                    caption = "Lower and Upper limits of bounds where the upper limit is less than the lower limit for trivariate distributions with four category instruments.", 
                    label = "flipped-trivariates") %>% 
  kableExtra::kable_styling(latex_options = "repeat_header")
```

<!-- Print tables from R -->
`r flipped_bivariate_bounds_table`

`r tri_bounds_flipped_table`


```{r include = FALSE}
ggplot(flipped_bivariate_bounds,
       aes(x = Strength, y = Width)) + 
  geom_point() + 
  lims(x = c(0, 1), y = c(-1, 0)) +
  theme_bw()
```


# Additional Summary Statistics for Analyses Presented in Section \ref{data-analysis} \label{more-details-data-application-appendix}

```{r include = FALSE}
summaries_depression <- read_csv(here::here("vignettes_data/summary_stats_smoking_on_depression.csv"))
summaries_lung_cancer <- read_csv(here::here("vignettes_data/summary_stats_smoking_on_lung_cancer_3.csv"))

snp_marginals_depression <- summaries_depression %>% 
  select(SNP, starts_with("P(Z =")) %>% 
  unique()

snp_coefs_depression <- summaries_depression %>% 
  select(SNP, regression, starts_with("beta")) %>% 
  unique() %>% 
  pivot_longer(cols = c(beta, beta0)) %>% 
  mutate(name = case_when(regression == "exposure" & name == "beta"  ~ "$\\beta_1$",
                          regression == "exposure" & name == "beta0" ~ "$\\beta_0$",
                          regression == "outcome"  & name == "beta"  ~ "$\\gamma_1$",
                          regression == "outcome"  & name == "beta0" ~ "$\\gamma_0$",
                          TRUE ~ NA_character_)) %>% 
  select(-regression) %>% 
  pivot_wider(names_from = name, values_from = value)


snp_marginals_lung_cancer <- summaries_lung_cancer %>% 
  select(SNP, starts_with("P(Z =")) %>%
  unique()

snp_coefs_lung_cancer <- summaries_lung_cancer %>% 
  select(SNP, regression, starts_with("beta")) %>% 
  unique() %>% 
  pivot_longer(cols = c(beta, beta0)) %>% 
  mutate(name = case_when(regression == "exposure" & name == "beta"  ~ "$\\beta_1$",
                          regression == "exposure" & name == "beta0" ~ "$\\beta_0$",
                          regression == "outcome"  & name == "beta"  ~ "$\\gamma_1$",
                          regression == "outcome"  & name == "beta0" ~ "$\\gamma_0$",
                          TRUE ~ NA_character_)) %>% 
  select(-regression) %>% 
  pivot_wider(names_from = name, values_from = value)
```


\begin{figure}[H]
  \center
  \includegraphics[width = \textwidth]{`r here::here("figures/example_analyses/smoking_depression_marginal_Z.png")`}
  \caption{Histograms of the marginal distribution of instruments, $P(Z = z), z=0,1,2$, estimated after preprocessing for analysis in Section \ref{smoking-effect-on-depression}}
  \label{fig:marginal-distribution-of-instruments-depression}
\end{figure}

\begin{table}[H]
  \caption{Table of the marginal distribution of instruments, $P(Z = z), z=0,1,2$, estimated after preprocessing for analysis in Section    \ref{smoking-effect-on-depression}}
  \label{tab:marginal-distribution-of-instruments-depression}
  \begin{minipage}{0.5\linewidth}
    \center
    `r knitr::kable(filter(snp_marginals_depression, row_number() < 43), format = "latex", booktabs = TRUE) %>% kableExtra::kable_styling(latex_options = "repeat_header")`
  \end{minipage}
  \qquad
  \begin{minipage}{0.5\linewidth}
    \center
    `r knitr::kable(filter(snp_marginals_depression, row_number() > 42), format = "latex", booktabs = TRUE) %>% kableExtra::kable_styling(latex_options = "repeat_header")`
  \end{minipage}
\end{table}

\begin{figure}[H]
  \includegraphics[width = 0.99\linewidth]{`r here::here("figures", "example_analyses", "smoking_depression_marginal_conditionals.png")`}
  \caption{Histograms of the marginal conditional probabilities $P(X = 1 | Z = z), z = 0,1,2$ and $P(Y = 1 | Z = z), z=0,1,2$.}
  \label{fig:smoking_on_depression_marginals}
\end{figure}

\begin{figure}[H]
  \center
  \includegraphics[width = \textwidth]{`r here::here("figures/example_analyses/smoking_depression_coefficients.png")`}
  \caption{Histograms of the coefficients from GWAS results of logistic regression of the SNPs on smoking status and depression status, respectively. Intercepts ($\beta_0$ and $\gamma_0$) are inferred, while slopes ($\beta_1$ and $\gamma_1$) are as reported.}
  \label{fig:marginal-distribution-of-coefficients-depression}
\end{figure}


<!-- Print table from R -->
`r kableExtra::kable(snp_coefs_depression, "latex", longtable = T, booktabs = TRUE, caption = "Coefficients from GWAS results of logistic regression of the SNPs on smoking status and depression status. Intercepts ($\\beta_0$ and $\\gamma_0$) are inferred, while slopes ($\\beta_1$ and $\\gamma_1$) are as reported.", label = "coefficients-depression", escape = FALSE) %>% kableExtra::kable_styling(latex_options = "repeat_header")`

\begin{figure}[H]
  \center
  \includegraphics[width = \textwidth]{`r here::here("figures/example_analyses/smoking_lung_cancer_3_marginal_Z.png")`}
  \caption{Histograms of the marginal distribution of instruments, $P(Z = z), z=0,1,2$, estimated after preprocessing for analysis in Section \ref{smoking-effect-on-lung-cancer}.}
  \label{fig:marginal-distribution-of-instruments-lung-cancer}
\end{figure}

\begin{table}[H]
  \caption{Table of the marginal distribution of instruments, $P(Z = z), z=0,1,2$, estimated after preprocessing for analysis in Section \ref{smoking-effect-on-lung-cancer}}
  \label{tab:marginal-distribution-of-instruments-lung-cancer}
  \begin{minipage}{0.5\linewidth}
    \center
    `r knitr::kable(filter(snp_marginals_lung_cancer, row_number() < 43), format = "latex", booktabs = TRUE) %>% kableExtra::kable_styling(latex_options = "repeat_header")`
  \end{minipage}
  \qquad
  \begin{minipage}{0.5\linewidth}
    \center
    `r knitr::kable(filter(snp_marginals_lung_cancer, row_number() > 42), format = "latex", booktabs = TRUE) %>% kableExtra::kable_styling(latex_options = "repeat_header")`
  \end{minipage}
\end{table}

\begin{figure}[H]
  \center
  \includegraphics[width = \textwidth]{`r here::here("figures/example_analyses/smoking_lung_cancer_3_coefficients.png")`}
  \caption{Histograms of the coefficients from GWAS results of logistic regression of the SNPs on smoking status and lung cancer status. Intercepts ($\beta_0$ and $\gamma_0$) are inferred, while slopes ($\beta_1$ and $\gamma_1$) are as reported.}
  \label{fig:marginal-distribution-of-coefficients-lung-cancer}
\end{figure}

<!-- Print table from R -->
`r kableExtra::kable(snp_coefs_lung_cancer, "latex", longtable = T, booktabs = TRUE, caption = "Coefficients from GWAS results of logistic regression of the SNPs on smoking status and lung cancer status. Intercepts ($\\beta_0$ and $\\gamma_0$) are inferred, while slopes ($\\beta_1$ and $\\gamma_1$) are as reported.", label = "coefficients-lung-cancer", escape = FALSE) %>% kableExtra::kable_styling(latex_options = "repeat_header")`

# References
