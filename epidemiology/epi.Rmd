---
title: "Non-parametric Bounds in Two-Sample Summary-Data Mendelian Randomization: Some Cautionary Tales for Practice"
output: 
  bookdown::pdf_document2:
    keep_tex: true
    citation_package: biblatex
    toc: false
    number_sections: true
    includes:
      in_header: epi_preamble.tex
abstract: |
  Recently, in genetic epidemiology, Mendelian randomization (MR) has become a popular approach to estimate causal exposure effects by using single nucleotide polymorphisms from genome-wide association studies (GWAS) as instruments. The most popular type of MR study, a two-sample summary-data MR study, relies on having summary statistics from two independent GWAS and using parametric methods for estimation. However, little is understood about using a nonparametric bound-based analysis, a popular approach in traditional instrumental variables frameworks, to study causal effects in two-sample MR. In this work, we explore using a bound-based analysis in two-sample MR studies. We also propose a framework to assess how likely one can obtain more informative bounds on the causal effect if we used a different MR design, notably a one-sample MR design. We conclude by demonstrating our findings through two real data analyses concerning the causal effect of smoking on lung cancer and the causal effect of high cholesterol on heart attacks. Overall, our results suggest that while a bound-based analysis may be appealing due to its nonparametric nature, it is rarely suitable as a method of analysis to bound the causal exposure effect in two-sample MR studies unless strong assumptions are met.
bibliography: "../bibliography/references.bib"
linkcolor: blue
link-citations: true
geometry: "margin=1in"
csl: "../bibliography/american-medical-association-10th-edition.csl"
# classoption: "draft"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
## Also need bookdown to be installed
library(tidyverse)
library(kableExtra)
library(here)
library(glue)
library(pander)
```

\newpage

# Introduction

In recent years, genetic variants have been used as instrumental variables (IV) to estimate causal effects in epidemiological studies, often referred to as Mendelian randomization (MR) studies [@davey_smith_mendelian_2003; @lawlor_mendelian_2008; @burgess_mendelian_2015]. Typically, MR studies are based on a two-sample study design where published summary statistics from two independent genome wide association studies (GWAS), with one providing information about the exposure and instrument and the other about the outcome and instrument, are used [@burgess_mendelian_2013; @burgess_using_2015; @davies_reading_2018]. Under a two-sample study design, investigators frequently use parametric methods to study exposure effects. Notable examples include the IVW estimator [@burgess_mendelian_2013], MR-Egger regression [@bowden_assessing_2016], weighted median [@bowden_consistent_2016], MR-PRESSO [@verbanck_detection_2018] and MR-RAPS [@zhao_statistical_2020], to name a few; see @burgess_mendelian_2015, @burgess_review_2017 and @slob_comparison_2020 for recent reviews.

An alternative approach to study exposure effects in instrumental variables without parametric assumptions is through nonparametric IV bounds [@balke_bounds_1997; @cheng_bounds_2006; @manski_nonparametric_1990; @richardson_ace_2014; @robins_analysis_1989]. Briefly, nonparametric IV bounds use a minimum set of assumptions to provide a range of plausible values for the exposure effect. They are typically used when the outcome, the exposure, and the instrument are all binary and are simultaneously observed; we refer to this setting as the one-sample setting to contrast it from the two-sample setting. Arguably, the most well-known IV bounds are the Balke-Pearl bounds [@balke_bounds_1997] for the average treatment effect. @cheng_bounds_2006 and @richardson_ace_2014 extended the Balke-Pearl bounds to allow for a non-binary instrument. @ramsahai_causal_2012 derived bounds under two-sample study designs; see @swanson_partial_2018 and references therein for a recent summary of IV bounds.

Using IV bounds can be an attractive alternative to study exposure effects in non-MR, one-sample settings [@swanson_commentary_2013; @swanson_partial_2018] and some [@didelez_mendelian_2007; @swanson_commentary_2017] have suggested using IV bounds to study exposure effects in MR studies given the strong parametric assumptions accompanying most MR analyses. Despite these suggestions, to the best of our knowledge, there is little work on using bounds in typical MR settings, i.e. two-sample study designs with summary statistics. For example, what kind of genetic variants provide the most informative conclusions about the exposure effect in terms of the bounds not containing the null effect? Can combining multiple variants lead to shorter and tighter bounds? How do the bounds change if many instruments are weak, which is typical in MR studies from two-sample designs? The overall goal of the paper is to offer some practical guidance on using IV bounds in two-sample MR studies. We focus on two aspects: (1) the length of the bounds and (2) whether bounds cover the null effect of zero, both of which inform MR investigators about the direction and relative magnitude of the exposure effect.

Our overall takeaway message for investigators using two-sample MR studies is that unless the candidate instruments are very strong, a bound-based analysis will often be non-informative. Instead, investigators should either try to use one-sample MR studies or use well-informed parametric approaches to ascertain the exposure effect.

# Methods \label{setup}

## Review: Notation, Definitions, and Assumptions

\label{notation-and-definitions}

Let \(X\) and \(Y\) be binary exposure and outcome varaibles, respectively, \(Z\) be a categorical instrumental variable taking values in $\{0, 1, 2\}$, and \(U\) be an unmeasured confounder for the effect of \(X\) on \(Y\). Let \(Y^{z,x}\) be the potential outcome [@rubin_estimating_1974; @splawa-neyman_application_1990] had the subject received exposure value \(X = x\) and instrument value \(Z = z\). We assume the stable unit treatment value assumption (SUTVA) [@cox_planning_1958; @rubin_randomization_1980], formalized as $Y = \sum_{x,z} I[Z = z, X = x] Y^{x,z}$ where $I[\cdot]$ is the indicator function.

We make the following set of assumptions about $X, Y, Z$, and $U$ that are typical in MR studies; see @didelez_mendelian_2007 and @wang_bounded_2018 for details.

\begin{itemize}
\tightlist
\item[(A1)] \emph{(Relevance)}: $Z \not\perp X$ 
\item[(A2)] \emph{(Independent instrument)}: $Z \perp U$
\item[(A3)] \emph{(Exclusion restriction)}: $Y^{z,x} = Y^{z',x} = Y^{x}$ for all $x,z,z'$
\item[(A4)] \emph{(Conditional ignorability of $X,Z$ given $U$)}: $Y^{z,x} \perp Z, X | U$
\end{itemize}

Briefly, (A1) can be satisfied by finding SNPs that have been consistently associated with the exposure. (A2) and (A3) are justified by scientific theory and can be violated if the SNP is (i) in linkage disequilibrium with an unmeasured SNP that affects the exposure and the outcome or (ii) has multiple functions beyond affecting the exposure (i.e. pleiotropic), to name a few. Finally, (A4) states that if \(U\) is observed, then it is sufficient to unconfound the relationship between \(X\) and \(Y\).

Throughout the paper, we will assume (A1)-(A4) hold to focus the discussion on the bounds, even though they are important to assess in practice. We make some additional remarks about assumptions (A1)-(A4). First, in practice, most MR studies only explicitly state assumptions (A1)-(A3) along with some parametric modeling assumptions [@burgess_mendelian_2015]. Second, @richardson_ace_2014 showed that one can remove (A4) and strengthen (A2) with \(Z \perp U, Y^{z,x}\) without consequence on the IV bounds. Third, under SUTVA and assumptions (A3)-(A4), we have $Y \perp Z | X, U$, which is another common way to express the exclusion restriction in MR studies [@didelez_mendelian_2007; @swanson_partial_2018]. Fourth, for simplicity, we do not assume the existence of a potential treatment $X^{z}$.

Next, we introduce the following assumptions (A5) and (A6); these assumptions are not necessary to construct bounds, but they will help characterize IV bounds in two-sample studies.

\begin{itemize}
\item[(A5)] \emph{(Monotonicity between $Z$ and $X$)} $P(X = 1 | Z = z, U) \le P(X = 1 | Z = z+1, U)$ for $z=0,1$
\item[(A6)] \emph{(Monotonicity between $Z$ and $Y$)} $P(Y = 1 | Z = z, U) \le P(Y = 1 | Z = z+1, U)$ for $z=0,1$
\end{itemize}

A variant of (A5) is common in the IV literature to study noncompliance [@angrist_identification_1996; @baiocchi_instrumental_2014]. (A6) is an extension of (A5) to the outcome variable. (A5) or (A6) is plausible in MR if the direction of the genetic instrument's effect on the exposure or the outcome is well-established from scientific theory.

We also define instrument strength ST as

\begin{equation}
\text{ST} = \max_{z_1 \neq z_2} | P(X = 1 | Z = z_1) - P(X = 1 | Z = z_2) | \label{eq:strength}
\end{equation}

ST reduces to the definition of instrument strength in @balke_bounds_1997 when the instrument is binary; @balke_bounds_1997 used ST to characterize the width of their IV bounds. Also, \eqref{eq:strength} differs from other definitions of instrument strength based on a parametric model between the exposure and the outcome, say the concentration parameter; see @stock_survey_2002 for an overview.

<!-- Another popular summary statistic measuring instrument strength in MR studies is the coefficient from a logistic model [@lawlor_mendelian_2008; @burgess_sample_2014; @verma_simulation_2018; @millard_mr-phewas_2019; @king_mendelian_2020]. We will utilize a logistic model for the exposure, where the log-odds is given as a linear combination of an intercept, the instruments, and an unmeasured confounder $U$ from the standard normal, and a logistic model for the outcome, where the log-odds is given as a linear combination of an intercept, the exposure, and the unmeasured confounder $U$; see Appendix \ref{appendix-logistic-models} for details. -->

## IV Bounds Under Two-Sample Designs and Goals of Paper 

\label{review-study-designs-and-target-estimand}

The most popular design in MR studies is a two-sample design which has two separate data sources, one providing information about \((X,Z)\) in the form of \(P(X = 1 | Z = z)\), \(z \in \{0, 1, 2\}\), and another providing information about \((Y,Z)\) in the form of \(P(Y = 1 | Z = z)\),  \(z \in \{0, 1, 2\}\). A two-sample design differs from a more traditional one-sample design which has a single data source providing information on all observed variables \((X,Y,Z)\) in the form of  \(P(Y = 1, X=1 | Z = z)\). IV bounds have been well-studied in one-sample designs and there is a rich array of practical guidance for practitioners interested in using them in their studies [@balke_bounds_1997; @richardson_ace_2014; @swanson_partial_2018]. However, as noted in the introduction, not much is known about the behavior of IV bounds under a two-sample design, especially how (or when) MR investigators should use them in their own studies.

Formally, the goal of this paper is to offer concrete practical advice on using IV bounds to study the average treatment effect (ATE), defined as
\[
\text{ATE} = E[Y^1 - Y^0] = \int P(Y=1 \mid X = 1, U=u) P(U=u) du - \int P(Y=1 \mid X = 0, U=u) P(U=u) du
\]

based on using \(P(Y = 1 | Z = z)\) and \(P(X = 1 | Z = z)\) for each \(z=0,1,2\) obtained from a two-sample design. Specifically, under a two-sample design and assumptions (A1)-(A4), @ramsahai_causal_2012 derived a sharp bound for the ATE (see Appendix  \ref{bounds-on-average-treatment-effect} for a detailed review and a discussion of "IV Inequalities" [@balke_bounds_1997; @diemer_application_2020] in two-sample MR studies.)

\begin{gather}
\max \left \{
\begin{array}{ll}
  \max_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) - 2\cdot P(Y = 1 | Z = z_2) - 2\cdot P(X = 1 | Z = z_2) \\
  \max_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) + P(X = 1 | Z = z_1) - P(Y = 1 | Z = z_2) - P(X = 1 | Z = z_2) - 1 \\
  \max_{z_1 \neq z_2} & 2\cdot P(Y = 1 | Z = z_1) + 2\cdot P(X = 1 | Z = z_1) - P(Y = 1 | Z = z_2) - 3 \\
  \max_z & -P(Y = 1 | Z = z) - P(X = 1 | Z = z) \\
  \max_z & P(Y = 1 | Z = z) +  P(X = 1 | Z = z) - 2
\end{array}
\right \} \nonumber \\ \nonumber \\
\le ATE \le \label{eq:ate_bound} \\ \nonumber \\
\min \left \{
\begin{array}{ll}
  \min_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) - 2\cdot P(Y = 1 | Z = z_2) +  2\cdot P(X = 1 | Z = z_2) + 1 \\
  \min_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) + 2\cdot P(Y = 1 | Z = z_2) -  2\cdot P(X = 1 | Z = z_2) + 1 \\
  \min_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) - P(X = 1 | Z = z_1) + P(X = 1 | Z = z_2) - P(Y = 1 | Z = z_2) + 1 \\
  \min_z & P(X = 1 | Z = z) - P(Y = 1 | Z = z) + 1 \\
  \min_z & P(Y = 1 | Z = z) - P(X = 1 | Z = z) + 1 
\end{array} 
\right \} \nonumber
\end{gather}

<!-- Additionally, the IV assumptions imply a set of "IV inequalities" [@balke_bounds_1997] that can falsify IV assumptions; see Appendix \ref{bounds-on-average-treatment-effect} for details and @diemer_application_2020 for examples in MR. -->

<!-- \begin{equation} -->
<!-- \min \left\{ -->
<!--   \begin{array}{ll} -->
<!--     \min_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) - P(X = 1 | Z = z_1) - P(Y = 1 | Z = z_2) - P(X = 1 | Z = z_2) + 2 \\ -->
<!--     \min_{z_1 \neq z_2} & P(Y = 1 | Z = z_1) + P(X = 1 | Z = z_1) - P(Y = 1 | Z = z_2) + P(X = 1 | Z = z_2) \\ -->
<!--     \min_{z} & P(X = 1 | Z = z) \\ -->
<!--     \min_{z} & P(Y = 1 | Z = z) \\ -->
<!--     \min_{z} & 1 - P(X = 1 | Z = z) \\ -->
<!--     \min_{z} & 1 - P(Y = 1 | Z = z)  -->
<!--   \end{array}  -->
<!-- \right \} \ge 0 \label{eq:constraints} -->
<!-- \end{equation} -->

This paper studies arguably the two most relevant properties of the above bounds that can guide practice: (1) the length of the bounds and (2) the ability to obtain bounds not covering the null effect of zero. To better understand bound-specific characteristics not due to sampling errors, we will assume we have population-level quantities of \(P(Y = 1 | Z = z)\) and \(P(X = 1 | Z = z)\); in practice, they are estimated summary GWAS statistics from logistic models [@lawlor_mendelian_2008; @burgess_sample_2014; @verma_simulation_2018; @millard_mr-phewas_2019; @king_mendelian_2020] and Appendix \ref{appendix-logistic-models} contains additional details.

# Properties of IV Bounds

## Length of Bounds and Coverage of Null Effect

\label{bounds-from-bivariate-data}
\label{properties-of-bounds-from-summary-level-data}

```{r include = FALSE}
violation_summaries <- read_csv(here("data/many_tri_bounds_violations.csv")) %>%
  rename(upper_smallest = `upper < lower`)
```

Theorem \ref{thm:upperBoundWidth} characterizes the length of the IV bound in equation \eqref{eq:ate_bound} under two-sample designs and assumptions (A1)-(A6); the extra assumptions (A5)-(A6) simplify the formula for the length of the bound to be an interpretable, linear function of instrument strength ST.


<!-- Under assumptions (A1)-(A6), the bounds for the ATE in \eqref{eq:ate_bound} become -->

<!-- $$ -->
<!--   \begin{aligned} -->
<!--     &\max -->
<!--       \begin{Bmatrix} -->
<!--         -P(Y = 0 | Z = 2) - P(Y = 1 | Z = 0) + P(X = 0 | Z = 0) - P(X = 0 | Z = 2) \\ -->
<!--         P(Y = 0 | Z = 0) - 2\cdot P(Y = 0 | Z = 2) - P(X = 0 | Z = 2) \\ -->
<!--         -P(Y = 0 | Z = 2) - 2\cdot P(Y = 1 | Z = 0) + P(X = 0 | Z = 0) -->
<!--       \end{Bmatrix} \\ -->
<!--     &\qquad \qquad \qquad \qquad \qquad\le ATE \le \\ -->
<!--     &\qquad \qquad \qquad \min -->
<!--       \begin{Bmatrix} -->
<!--         1 + P(Y = 0 | Z = 0) - P(X = 0 | Z = 0) \\ -->
<!--         1 + P(Y = 0 | Z = 0) - P(Y = 0 | Z = 2) - P(X = 0 | Z = 0) + P(X = 0 | Z = 2) \\ -->
<!--         1 - P(Y = 0 | Z = 2) +  P(X = 0 | Z = 2) -->
<!--       \end{Bmatrix} -->
<!--   \end{aligned} -->
<!-- $$ -->

<!-- and a sharp upper bound on the width of the bounds is $2 - 2\cdot \text{ST}$, i.e. there exists a data generating process satisfying (A1)-(A6) and has width equal to $2 - 2\cdot \text{ST}$. -->

\begin{theorem}\label{thm:upperBoundWidth}
Under assumptions (A1)-(A6), a sharp upper bound on the length of the bound in equation \eqref{eq:ate_bound} is $2 - 2\cdot \text{ST}$, i.e. there exists a data generating process satisfying (A1)-(A6) and has width equal to $2 - 2\cdot \text{ST}$.
\end{theorem}

See Appendix \ref{proof-of-theorem} for the proof, which extends Theorem \ref{thm:upperBoundWidth} to instruments with 2, 3, or 4 categories. Compared to the Balke-Pearl IV bounds with a binary IV in single-sample designs whose width is \(1-\text{ST}\) [@balke_bounds_1997], the length of the two-sample bounds can be twice as long. Also, the length of two-sample IV bounds is only guaranteed to be less than 1 if instrument strength ST is greater than 0.5; note that this does not imply that instruments with ST less than 0.5 has length greater than 1. In contrast, one-sample IV bounds always have lenth less than 1 unless ST is zero. In short, there is a cost, in length, of using a two-sample design instead of a one-sample design when performing a bound-based analysis of the ATE in MR.

```{r echo = FALSE, message = FALSE, warning = FALSE}
upper_less_than_lower <- read.csv(here("tables", "upper_less_than_lower.csv"), check.names = FALSE) %>% 
  mutate(across(everything(), ~format(.x, scientific = FALSE, big.mark = ",")))

proportion_with_width <- read.csv(here("tables", "proportion_of_biv_widths_greater_than.csv"), check.names = FALSE)

proportion_with_width_latex <- proportion_with_width %>% 
  mutate(
    Strength = paste0("$", Strength, "$"),
    across(
      where(is.numeric),
      ~paste0("$", sprintf(.x, fmt = "%.3f"), "$")
    )
  ) %>%
  rename_with(
    ~str_split(.x, "= ", simplify = TRUE)[,2],
    -1
  ) %>% 
  kable(format = "latex", row.names = FALSE, booktabs = TRUE, escape = FALSE, align = "lccc") %>% 
  add_header_above(
    header = c(" " = 1, "Proportion of bounds with\n width greater than..." = 3)
  )
```


Figure \ref{fig:biv_width_vs_strength} numerically illustrates the consequences of Theorem \ref{thm:upperBoundWidth} by calculating the bounds in equation \eqref{eq:ate_bound} from `r upper_less_than_lower[["good"]]` randomly generated sets of values of \(P(X = 1 | Z = z)\) and \(P(Y = 1 | Z = z)\) that satisfy the IV inequalities and assumptions (A1)-(A4). We also use three real-world data examples where the causal effect is known to exist: the effect of high cholesterol on incidence of heart attacks [@cholesterol_treatment_trialists_ctt_collaborators_effects_2012], the effect of smoking on incidence of lung cancer [@cornfield_smoking_1959], and the effect of obesity on incidence of heart attacks [@yusuf_obesity_2005]. The first two studies are discussed in detail in Section \ref{data-analysis}. We see that the width of the bounds often exceed \(1\) as the instrument strength decreases. Also, the three real-world studies generally do not lead to bounds with length less than \(1\). Figure \ref{fig:coef_vs_strength} further illustrates this point by characterizing the relationship between instrument strength ST and the summary statistic coefficient $\gamma_1$ from a logistic exposure model $\text{logit}(P(X = 1 | Z = z, U = u)) = \gamma_0 + \gamma_1 z + \gamma_U u$ <!--$P(X = 1 \mid Z = z) = \int {\rm logit}(\gamma_0 + \gamma_1 z + \gamma_U U) P(U | Z = z)$--> commonly used in parametric approaches to analyzing two-sample MR studies; see Appendix \ref{appendix-sim-results} for details. We see that instrument strength ST of \(0.5\) corresponds to a regression coefficient \(\gamma_1\) of approximately \(1.1, 1.16, 1.4\) and \(1.8\) if \(\gamma_U\) is \(0.1, 0.5, 1\) and \(2\), respectively. Coefficients with such magnitudes are rare in GWAS where genetic variants often explain a small amount of variation in the exposure. To better contextualize this observation, we note that the values of $\gamma_1$ correspond to odds ratios between $3$ and $6$ whereas the strong causal effect of exposure to ultraviolet light on the incidence of skin cancer is estimated be in the range from $1.4$ to $2.22$ [@schmitt_occupational_2011]. <!-- Also, to better contextualize this observation, from [@united1964smoking], a back-of-the-envelope calculation of smoking's causal effect on lung cancer leads to a $\gamma_1$ of roughly 0.95 using a 9-fold relative risk estimate and low incidence of lung cancer among non-smokers. -->

Next, for bounds with length less than $1$, we examine what kind of $\gamma_1$ is needed in order for the two-sample IV bounds to exclude $0$ for an anticipated effect size of the ATE. This question is akin to computing the power of bounds but with population-level quantities. We reuse the same logistic model above for the exposure and the outcome; see Appendix \ref{appendix-sim-results} for details on this setup. Figure \ref{fig:power_curves} shows the smallest $\gamma_1$ needed to exclude $0$ for different values of the ATE. Even for moderate effect sizes of 0.4, the corresponding $\gamma_1$ must be around $2$, a tall order for most GWAS. Also, as the effect of unmeasured confounding increases via $\gamma_U$, a larger $\gamma_1$ is needed to exclude $0$.

Overall, using two-sample MR studies with a bound-based analysis is unlikely to be informative. The bounds will often have length greater than $1$ and rarely exclude $0$ unless very strong genetic variants are used.


\begin{figure*}
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \caption{}
    \label{fig:biv_bounds_vs_strength}
    \includegraphics[width=\textwidth]{`r here("figures/bivariate_width_vs_strength_pip.png")`}
  \end{subfigure}%
  \hspace{0.15in}
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \caption{}
    \label{fig:biv_width_vs_strength}
    \includegraphics[width=\textwidth]{`r here("figures/MR_coefs_vs_strength.png")`}
  \end{subfigure}
  \caption{Illustration of the relationship between instrument strength, and width of bounds obtained from two-sample design and coefficients from logistic regression model. A: Relationship between instrument strength (ST) and width of the IV bounds. Black line is the upper bound on the two-sample IV bounds from Theorem 1. Black dots indicate one of the 10,000 IV bounds. Colored dots indicate bounds from real data; see Section \ref{data-analysis} for details. B: Coefficients from logistic regression model and instrument strength (ST).}
  \label{fig:coef_vs_strength}
\end{figure*}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{`r here("figures/loess_power.png")`}
  \caption{The smallest $\gamma_1$ needed for a two-sample IV bound to exclude $0$.}
  \label{fig:power_curves}
\end{figure}


## Would Multiple Instruments Help?

Based on the results above with a single instrument, a natural question from investigators is whether using multiple instruments can lead to more informative bounds for the ATE; see @swanson_commentary_2017 for a recent discussion on this point. For example, suppose we aggregate two-sample IV bounds across multiple instruments by taking intersections of individual IV bounds. This approach may be inferior to another alternative where we expand the levels of \(Z\) from \(0,1,2\) to accommodate multiple instruments [@swanson_commentary_2017], but has the benefit of being applicable to most two-sample MR studies. However, as we show in Appendix \ref{appendix-sim-results-multiple-IVs}, the strongest instrument essentially determines the length of the intersection bound because the bounds from each instrument exhibit a nesting property. In short, using a bound based on the strong instrument provides the same amount of information about the ATE as the intersection of individual IV bounds from multiple instruments.


# Charactering the Loss of Information in Two-Sample MR Studies
\label{quasi-bayesian}

As hinted in Theorem \ref{thm:upperBoundWidth}, the increase in the bound's length is an inevitable "cost" of using two-sample designs instead of one-sample designs in MR studies. This section investigates this loss of information in a bit more detail by creating a plausible range of the joint distribution of the outcome and the exposure given the instrument \(Z\), \(P(X = x, Y = y | Z = z)\) based on the observed data from two-sample MR studies. Specifically, using \(P(X = x | Z = z)\) and \(P(Y = y | Z = z)\), and a uniform prior on unknown quantities  \(\text{Cov}(X = x, Y = y| Z = z)\) that make up \(P(X,Y | Z)\) and satisfy IV assumptions, we compute \(P(X = x, Y = y | Z = z)\) and its corresponding one-sample IV bounds of @balke_bounds_1997 and @richardson_ace_2014; see Appendix \ref{appendix-quasi-bayesian-details} for details and also the connection to empirical Bayesian frameworks. If a large number of the one-sample IV bounds obtained from this procedure do not cover zero, then there is some evidence for a non-zero exposure effect and a one-sample MR study may yield informative bounds on the ATE. However, if a large number of the one-sample IV bounds cover zero, there is little hope of obtaining information about the ATE from bound-based approaches even if we used a one-sample MR design; in other words, the one-sample IV bounds are likely as conservative as the two-sample IV bounds.


```{r include = FALSE}
subset_plot_summaries <- read_rds(here("vignettes_data", "subset_plot_summaries.Rds"))

subset_plot_summaries_for_print <- subset_plot_summaries %>% 
  mutate(p_no_zero = 1 - p_no_zero) %>% 
  rename(Row = row_i, Column = col_j,
         Lower = bivariate_lower,
         Upper = bivariate_upper) %>% 
  #        `Proportion overlapping 0` = p_no_zero) %>% 
  select(Row, Column, Lower, Upper, everything())

subset_plot_summaries_for_print_A <- subset_plot_summaries_for_print %>% 
  select(-starts_with("P(")) %>% 
  mutate(p_no_zero = sprintf(fmt = "%.2f", p_no_zero * 100),
         across(c(Lower, Upper), round, digits = 3),
         across(c(Row, Column), ~paste(cur_column(), .x)),
         cells = glue("[{Lower}, {Upper}]\n {p_no_zero}\\%")) %>% 
  select(" " = Row, Column, cells) %>% 
  mutate(across(everything(), linebreak)) %>% 
  pivot_wider(names_from = Column, values_from = cells) %>% 
  kable(format = "latex", escape = FALSE, booktabs = TRUE) 

subset_plot_summaries_for_print_B <- subset_plot_summaries_for_print %>% 
  select(Row, Column, starts_with("P(")) %>% 
  mutate(across(starts_with("P("), ~sprintf("%.3f", .x))) %>% 
  unite(col = "pxz", c(3:5), sep = ", ") %>% 
  unite(col = "pyz", 4:6, sep =", ") %>% 
  mutate(
    across(
      c(pxz, pyz),
      ~paste0("\\{", .x, "\\}")
    )
  ) %>% 
  unite(col = "p", 3:4, sep = "\n") %>% 
  mutate(across(p, linebreak),
         across(c(Row, Column), ~paste(cur_column(), .x))) %>% 
  pivot_wider(names_from = Column, values_from = p) %>% 
  rename(" " = Row) %>% 
  kable(format = "latex", escape = FALSE, booktabs = TRUE) 
```
Table \ref{tab:subset_plot_summaries} presents nine different sets of values of the marginal distributions \(P(Y | Z)\) and \(P(X | Z)\) that investigators could theoretically obtain from hypothetical two-sample MR studies. Figure \ref{fig:trivariate_bounds} shows the one-sample IV bounds from the procedure we illustrated above.

\begin{table}[H]
  \center
  \caption{Values of $P(X = 1 | Z = z)$ and $P(Y = 1 | Z = z)$ used to illustrate our approach. For each cell (e.g. row A, column 1), we have $\{P(X = 1 | Z = 0), P(X = 1 | Z = 1), P(X = 1 | Z = 2)\}$ on the first row and $\{P(Y = 1 | Z = 0), P(Y = 1 | Z = 1), P(Y = 1 | Z = 2)\}$ on the second row.}
  \label{tab:subset_plot_summaries}
  `r subset_plot_summaries_for_print_B`
\end{table}

\begin{figure}[H]
  \center
  \includegraphics[width=\linewidth]{`r here("figures", "trivariate_bounds_subset_plot.png")`}
  \caption{One-sample bounds (solid lines) and two-sample bounds (dotted lines). Red color represents one-sample bounds that do not cover zero and gray color represents one-sample bounds that do cover zero.}
  \label{fig:trivariate_bounds}
\end{figure}

Row A of Figure \ref{fig:trivariate_bounds} shows three scenarios where the two-sample bounds are all centered close to zero with similar widths. But, the conclusions from the one-sample bound analysis are rather different. Column 1 shows no one-sample bounds would allow us to determine the presence of a non-zero exposure effect. Column 2 indicates that about `r paste0(filter(subset_plot_summaries, row_i == "A", col_j == 2)[["p_no_zero"]]*100, "%")` of the one-sample IV bounds does not contain \(0\) while for column 3 that number is approximately `r paste0(filter(subset_plot_summaries, row_i == "A", col_j == 3)[["p_no_zero"]]*100, "%")`. 

Row B illustrates three scenarios where the two-sample bounds are centered well above zero and have large widths. We see one case where we have no hope of determining direction of the ATE from the one-sample bounds (column 1), one case where we are most likely to determine the ATE's direction (column 2), and one case where we are unlikely to determine the ATE's direction (column 3).

Row C is similar to row A in that all the two-sample bounds are centered around 0, but the widths of the two-sample bounds are narrow. The three columns indicate similar conclusions as row A, showing that even with rather narrow two-sample bounds centered around 0, the one-sample bounds may still reveal some information about presence as well as the direction of the exposure effect.

Overall, our results above show that some two-sample MR studies could potentially reveal something useful about the ATE had we used a one-sample design. Nevertheless, we mention a word of caution when interpreting the results above, especially concerning the flat prior on the covariances. For example, a scenario like the one resulting in the bounds presented in row B, column 2 only provides honest information about the one-sample bounds if our prior on \(\text{Cov}(X,Y|Z)\) is correctly specified. If the prior is mis-specified whereby most one-sample bounds cover negative values of the ATE, a negative value of the ATE is possible. But in this case, if the ATE is in fact negative, our method does rule out the possibility of one-sample bounds being able to ascertain this because all one-sample bounds covering a negative ATE also covers \(0\).

# Using Bound-Based Analyses in Two, Positive Control Examples \label{data-analysis}

We demonstrate our findings about the behavior of two-sample IV bounds on two real MR studies. Our first study examines the effect of smoking on lung cancer and our second study examines the effect of self-reported high cholesterol on incidence of heart attack. The effect of smoking on lung cancer is known to be strong and positive [@united1964smoking]. Also, while the exact mechanism between high cholesterol and heart disease is still being discussed [@holmes_mendelian_2015; @richardson_evaluating_2020], some meta-analyses of randomized clinical trials of the effect of cholesterol-lowering medication suggest a strong causal relationship [@20051267; @cholesterol_treatment_trialists_ctt_collaborators_effects_2012]. In both cases, we assess what conclusions are attainable based on bound-based analyses in studies where the causal effects are known to be strong and positive.

```{r include = FALSE}
experiments <- read_csv(here("vignettes_data/example_analyses/experiment_info.csv"))
```

The study data were obtained from the UK Biobank data stored in the Integrative Epidemiology Unit (IEU) GWAS database. Specifically, data on smoking was obtained from the data entry ID `r filter(experiments, str_detect(tolower(trait), "smoking"))[['id']]`, data on lung cancer was from data entry ID `r filter(experiments, str_detect(tolower(trait), "lung"))[['id']]`, data on cholesterol was from data entry ID `r filter(experiments, str_detect(tolower(trait), "cholesterol"))[['id']]`, and data on heart attack was from data entry ID `r filter(experiments, str_detect(tolower(trait), "heart"))[['id']]`. We use the \texttt{TwoSampleMR} R package [@mrbase] with the recommended defaults to extract and clean the data. For more details, see Appendix \ref{more-details-data-application-appendix}.

```{r include = FALSE}
smoking_lung_cancer_strength_summaries <- read_csv(file = here("vignettes_data/smoking_lung_cancer_strength_summaries.csv")) %>% 
  mutate(across(where(is.numeric), round, digits = 4))

cholesterol_heart_attack_strength_summaries <- read_csv(file = here("vignettes_data/cholesterol_heart_attack_strength_summaries.csv")) %>% 
  mutate(across(where(is.numeric), ~sprintf(.x, fmt = "%.4f")))
```

For the effect of smoking on lung cancer, we used 84 genetic instruments, and for the effect of cholesterol on heart attack, we used 54 genetic instruments. The average instrument strengths were `r smoking_lung_cancer_strength_summaries[['mean']]` (range: `r smoking_lung_cancer_strength_summaries[['min']]` to `r smoking_lung_cancer_strength_summaries[['max']]`) for smoking and  `r cholesterol_heart_attack_strength_summaries[['mean']]` (range: `r cholesterol_heart_attack_strength_summaries[['min']]` to `r cholesterol_heart_attack_strength_summaries[['max']]`) for cholesterol; these values are much smaller than the ST $= 0.5$ needed to guarantee narrow bounds. As such, the two-sample bounds in Figure \ref{fig:two-sample-bounds} are rather wide; all of them have width greater than 1 and they convey no information about the causal effects of interest. Additionally, using our method from Section \ref{quasi-bayesian}, the direction of the ATE may be difficult to determine had we had one-sample IV bounds; see Figure \ref{fig:one-sample-bounds}. Appendix \ref{more-details-data-application-appendix} contains additional analysis, notably demonstrating that aggregating multiple bounds through intersections are also non-informative.

Overall, while nonparametric bounds allow us to not make parametric assumptions frequent in two-sample MR analyses, they may be too conservative and provide little, if any, information about the exposure effects, even if the exposure effect is known to be positive and strong. Additionally, since many MR studies involve weak instruments, we believe bound-based approaches will likely have limited practical value to uncover causal effects.

\begin{figure*}
  \centering
  \begin{subfigure}{0.5\linewidth}
  \caption{}
  \includegraphics[width=\textwidth]{`r here("figures/example_analyses/smoking_lung_cancer_bivariate_bounds_subset_ukb-d-20116_0_ukb-d-40001_C349.png")`}
  \label{fig:two-sample-bounds-smoking}
  \end{subfigure}%
  ~
  \begin{subfigure}{0.5\linewidth}
  \caption{}
  \includegraphics[width=\textwidth]{`r here("figures/example_analyses/cholesterol_heart_attack_bivariate_bounds_subset_ukb-a-108_ukb-a-434.png")`}
  \label{fig:two-sample-bounds-cholesterol}
  \end{subfigure}
  \caption{Two-sample IV bounds for the two real data examples with 8 SNPs from each data set. A: Two-sample IV bounds for the ATE of smoking on the incidence of lung cancer. B: Two-sample IV bounds for the ATE of high cholesterol on the incidence of heart attack.}
  \label{fig:two-sample-bounds}
\end{figure*}


\begin{figure*}
  \centering
  \begin{subfigure}{0.5\linewidth}
  \centering
  \caption{}
  \includegraphics[width=\textwidth]{`r here("figures/example_analyses/smoking_lung_cancer_individual_SNPs_plot_subset_ukb-d-20116_0_ukb-d-40001_C349.png")`}
  \label{fig:one-sample-bounds-smoking}
  \end{subfigure}%
  ~
  \begin{subfigure}{0.5\linewidth}
  \centering
  \caption{}
  \includegraphics[width=\textwidth]{`r here("figures/example_analyses/cholesterol_heart_attack_individual_SNPs_plot_subset_ukb-a-108_ukb-a-434.png")`}
  \label{fig:one-sample-bounds-cholesterol}
  \end{subfigure}
  \caption{Potential one-sample IV bounds for the two real data examples using the method described in Section \ref{quasi-bayesian}. A: One-sample IV bounds for the ATE of smoking on the incidence of lung cancer from 500 potential one-sample distributions. B: One-sample IV bounds for the ATE of high cholesterol on the incidence of heart attack from 500 potential one-sample distributions.}
  \label{fig:one-sample-bounds}
\end{figure*}


# Discussion \label{conclusion-and-practical-considerations}

Nonparametric bounds are without a doubt an attractive concept. With a minimal set of assumptions, they let investigators obtain bounds on the average treatment effect. However, as we have seen above, in typical MR studies with two-sample summary data, a bound-based analysis may generally uninformative for two reasons. First, while IV bounds in one-sample settings have length always being less than 1, in two-sample settings, this is not always the case. Second, many genetic variants in MR studies are too weakly associated with the exposure in order to produce bounds with length less than \(1\) or bounds that exclude \(0\). Indeed, our two real data examples showed that despite having strong causal effects, a bound-based analysis was unable to detect this effect.

We also outlined an approach to roughly quantify the information loss going from two-sample designs to one-sample designs and to assess the range of conclusions that can be drawn from bound-based approaches if we had one-sample data. We demonstrate our method to a few different settings of two-sample data and showed the range conclusions that can be drawn about the ATE.

What do our results suggest for bound-based analysis in two-sample MR settings in practice? Overall, our general recommendation is that unless investigators have a very strong instrument, ideally exceeding $\text{ST} > 0.5$, bounds will unlikely be useful as a nonparametric analysis of the ATE. Even if $\text{ST} > 0.5$, one would need strong IVs and/or strong effect sizes to make sure that the bounds do not cover $0$. Indeed, in such cases, we believe investigators should use well-informed parametric methods to analyze the ATE. Nevertheless, there may be few limited, but meaningful use cases for bounds in two-sample MR studies. First when one has prior knowledge about the direction of the effect, but wish to get a better sense of the magnitude, nonparametric bounds can provide an upper limit on this magnitude. This is especially useful in cases where the exposure is known to cause harm or benefit, for example in our smoking lung cancer example where the direction of the effect of smoking on lung cancer is well known and an upper bound on this effect would tell investigators about the maximum possible effect that smoking could have on increasing the incidence of lung cancer. Second, two-sample IV bounds can be used to check estimates from parametric methods to see if they lie inside of the bounds; if the estimates lie outside of the bounds, then the parametric models underlying the estimates are likely mis-specified.

\newpage

\printbibliography

\newpage

\setcounter{page}{1}

\setcounter{table}{0}
\setcounter{figure}{0}

\renewcommand{\tablename}{eTable}
\renewcommand{\figurename}{eFigure}

# (APPENDIX) Appendix {-}

# Supplemental Digital Content to "Non-parametric Bounds in Two-Sample Summary-Data Mendelian Randomization: Some Cautionary Tales for Practice"

This document contains the Supplemental Digital Content to our paper "Non-parametric Bounds in Two-Sample Summary-Data Mendelian Randomization: Some Cautionary Tales for Practice". This includes additional details on how we obtain bounds on the Average Treatment Effect, more on the logistic models we used for simulating data, proof of Theorem \ref{thm:UpperBoundWidth}, additional details and results for the "power" analysis presented in Section \ref{bounds-from-bivariate-data}, exploration of the use of multiple IVs in two-sample MR analysis, details on the reconstruction of the one-sample distribution introduced in Section \ref{quasi-bayesian}, and details, summary statistics, and complete results for the two example analyses presented in Section \ref{data-analysis}.

## Bounds on Average Treatment Effect

We briefly review the method presented by @ramsahai_causal_2012 to bound the average treatment effect using two-sample summary data. Let $\vec{\tau}^* = \Big(P(Y = 1 | X = 0, U), P(Y = 1 | X = 1, U), P(X = 1 | Z = 0, U), ..., P(X = 1 | Z = k-1, U)\Big) \in [0,1]^{2+k}$ and $\vec{v}^* = \Big(P(Y = 0 | Z = 0, U), ..., P(Y = 1 | Z = k-1, U), P(X = 0 | Z = 0, U), ..., P(X = 1 | Z = k-1, U), \alpha^*\Big)$ where

$$
\begin{aligned}
\alpha^* &= P(Y = 1 | X = 1, U) - P(Y = 1 | X = 0, U).
\end{aligned}
$$

Since $U \perp Z$, $E_U[P(X = x | Z = z, U)] = P(X = x | Z = z)$ and $E_U[P(Y = y | Z = z, U)] = P(Y = y | Z = z)$. Let $\vec{v} = E_U[\vec{v}^*] = \Big(P(Y = 0 | Z = 0), ..., P(Y = 1 | Z = k-1), P(X = 0 | Z = 0), ..., P(X = 1 | Z = k-1), \alpha \Big)$, where

$$
\begin{aligned}
\alpha &= E_U[P(Y = 1 | X = 1, U) - P(Y = 1 | X = 0, U)] \\
       &= E[Y^1] - E[Y^0] = \text{ATE}.
\end{aligned}
$$

Note that while $\vec{\tau}^*$ and $\vec{v}^*$ are both entirely unobervable, $\vec{v}$ consists of $k$ observable values, and one unobservable value, the ATE.

By the exclusion restriction, we have

$$
P(X = x, Y = y | Z = z, U) = P(Y = 1 | X = x, U) P(X = x | Z = z, U),
$$

which means we can define a mapping $f:[0,1]^{2+k} \mapsto \mathcal{V}$ such that $f(\vec{\tau}^*) = \vec{v^*}$ as

$$
f(y_0, y_1, x_0, x_1, ..., x_{k-1}) =
  \begin{pmatrix}
    (1-y_0)\cdot(1-x_0) + (1 - y_1)\cdot x_0 \\
    y_0\cdot (1-x_0) + y_1\cdot x_0 \\
    \vdots \\
    (1-y_0)\cdot(1-x_{k-1}) + (1 - y_1)\cdot x_{k-1} \\
    y_0\cdot (1-x_{k-1}) + y_1\cdot x_{k-1}
  \end{pmatrix} \label{eq:f}
$$

We define $\mathcal{V} = f([0,1]^{2+k})$.

Since $\vec{v} = E_U[\vec{v}^*]$, $\vec{v}$ must be a convex combination of $\vec{v}^*$. Let $\mathcal{H}$ be the convex hull of $\mathcal{V}$. Then $\vec{v}$ will be in $\mathcal{H}$.

Now, let $\hat{\mathcal{T}}$ be the set of extreme vertices of $[0,1]^{2+k}$, $\hat{\mathcal{V}} = f(\hat{\mathcal{T}})$, and $\hat{\mathcal{H}}$ be the convex hull of $\hat{\mathcal{V}}$. By Theorem 1 in Appendix B of @ramsahai_causal_2012, $\mathcal{H} = \mathcal{\hat{H}}$. This means that $\vec{v} \in \mathcal{\hat{H}}$. Utilizing a program such as Polymake, we can describe $\mathcal{H}$ with a set of inequalities, which give us constraints that $\vec{v}$ must satisfy.

This means that we can obtain inequalities that the components of $\vec{v}$ must satisfy by describing the extreme vertices of $[0,1]^{2+k}$, map them to $\mathcal{V}$ using the relatively simple function $f$, and then use polymake to find inequalities that characterize the convex hull of $f([0,1])^{2+k}$. This gives us a set of inequalities involving the components of $\vec{v}$. Some of these will be verifiable, as they will not include the only unobservable quantity $\alpha$. Others will not be verifiable, but will allow us to obtain bounds on the unobservable quantity $\alpha$ using the observable entries of $\vec{v}$.

<!-- This exact same approach can be used in the trivariate setting, and when imposing different extra assumptions, such as monotonicity of the effect of $Z$ on $X$ \eqref{eq:x_monotone} or $Z$ on $Y$ \eqref{eq:y_monotone}. The latter is done by imposing these assumption on $\hat{\mathcal{V}}$ by removing vectors that violate these extra assumptions. -->

<!-- One important thing to note here is that these expressions do not always result in a valid set of bounds. In our simulations we found that a small fraction of the simulated probability distributions result in bounds where the lower limit is larger than the upper limit. Whether this is a problem in practice remains to be seen, but we are not aware of any real life data sets giving rise to bounds with this behavior. The cause for this is not clear, but among possible explanations is the existence of an assumption that is not captured in the checkable constraints. It is only natural to conclude that one or more assumptions are violated when one encounters a scenario where the bounds are flipped. For more, see Appendix \ref{exploration-of-scenarios-where-bounds-are-flipped}. -->

<!-- We implemented this approach using `R` version 4.0.2 [@R] for the high level calculations, and Polymake [@assarf_computing_2017] to obtain the inequalities. -->

Following the approach from Ramsahai (2012) as outlined above, we obtain bounds on the average treatment effect from the quantities \(P(X = 1 | Z = z)\) and \(P(Y = 1 | Z = z)\), \(z = 0,1,2\). To do so, we first write down the most extreme values of each of \(P(Y = 1 | X = x, U)\) and \(P(X = x | Z = z, U)\) for all \(x=0,1\), \(z=0,1,2\). Since these are probabilities, the extreme values are \(0\) and \(1\). 

```{r echo = FALSE}
expand_grid(PY1X0U = c(0, 1),
            PY1X1U = c(0, 1),
            PY1Z0U = c(0, 1),
            PX1Z1U = c(0, 1),
            PX1Z2U = c(0, 1)) %>%
  pander(split.table = Inf,
                 caption = "Most extreme values of $P(Y = 1 | X = x, U)$ and $P(X = 1 | Z = z, U)$. Here, PY1XxU = $P(Y = 1 | X = x, U)$ and PX1ZzU = $P(X = 1 | Z = z, U)$.")
```

By applying the function \(f\), as presented in \eqref{eq:f}, to each row, we get the most extreme vertices of \(P(X = x | Z = z, U)\) and \(P(Y = y | Z = z, U)\) for all \(x=0,1,\ y=0,1\) and \(z=0,1,2\).

```{r echo = FALSE}
if(!file.exists(here("write_up/matrices/extreme_vertices.csv"))){
  extreme_vertices <- ACEBounds::create_vertices(n_z_levels = 3, data_format = "bivariate")
  
  colnames(extreme_vertices) <- c(paste0("PY", rep(0:1, each = 3), "Z", rep(0:2, times = 2)),
                                  paste0("PX", rep(0:1, each = 3), "Z", rep(0:2, times = 2)),
                                  "$\\alpha$")

  write_csv(extreme_vertices, here("write_up/matrices/extreme_vertices.csv"))
} else {
  extreme_vertices <- read_csv(here("write_up/matrices/extreme_vertices.csv"))
}

pander(extreme_vertices,
       split.table = Inf,
       caption = "Most extreme values of $P(Y = y | Z = z)$ and $P(X = x | Z = z)$. Here, PYyZz = $P(Y = y | Z = z)$, PXxZz = $P(X = x | Z = z)$, and $\\alpha = P(Y = 1 | X = 1,U) - P(Y = 1 | X = 0,U)$.\\label{tab:vertices}")
```

Theorem 1 of Ramsahai (2012) tells us that the values of \(P(X = 1 | Z = z), P(Y = 1 | Z = z),\ z = 0,1,2\) must lie in the convex hull of the vertices given by the rows in Table \ref{tab:vertices}. This means that the vector of these values must be a convex combination of the rows in said table. Using this with the fact that they must sum to 1 is what enables us to use polymake to find inequalities that the values of \(P(X = 1 | Z = z)\), \(P(Y = 1 | Z = z)\), and \(\alpha\) must satisfy. In this particular case, these are as presented below. This table should be read as rows of coefficients for which it holds that \(\sum_{z = 0}^2 c_{X1Zz} \cdot P(X = 1 | Z = z) + \sum_{z = 0}^2 c_{Y0Zz}\cdot P(Y = 0 | Z = z) + c_{Y1Z0}\cdot P(Y = 1 | Z = 0) + c_\alpha \alpha \ge 0\).

```{r}
if(!file.exists(here("write_up/matrices/bivariate_bounds_example.csv"))){
  bivariate_bounds_example <- ACEBounds:::matrices_from_polymake %>% 
    filter(data_format == "bivariate", !x_monotone, !y_monotone, n_z_levels == 3) %>% 
    pull(matrix) %>% .[[1]] %>% 
    select(where(~sum(abs(.x)) > 0))
  
  colnames(bivariate_bounds_example) <- c(
    paste0("c_{Y", c(0,0,0,1), "Z", c(0:2, 0), "}"), 
    paste0("c_{X1Z", c(0:2), "}"), 
    "$c_{\\alpha}$"
  )

  write_csv(bivariate_bounds_example, here("write_up/matrices/bivariate_bounds_example.csv"))
} else {
  bivariate_bounds_example <- read_csv(here("write_up/matrices/bivariate_bounds_example.csv"))
}

pander(bivariate_bounds_example, 
       caption = "Results from polymake. Columns with all zeroes have been removed.")
```

The matrix presented in the table above simplifies to the following set of bounds on the average treatment effect. These are obtained by considering the rows above where \(c_\alpha \neq 0\).

\[
\begin{aligned}
\max &\left \{
\begin{array}{ll}
  \max_{i\neq j} & P(Y = 1 | Z = i) - 2\cdot P(Y = 1 | Z = j) - 2\cdot P(X = 1 | Z = j) \\
  \max_{i\neq j} & P(Y = 1 | Z = i) + P(X = 1 | Z = i) - P(Y = 1 | Z = j) - P(X = 1 | Z = j) - 1 \\
  \max_{i\neq j} & 2\cdot P(Y = 1 | Z = i) + 2\cdot P(X = 1 | Z = i) - P(Y = 1 | Z = j) - 3 \\
  \max_i & -P(Y = 1 | Z = i) - P(X = 1 | Z = i) \\
  \max_i & P(Y = 1 | Z = i) +  P(X = 1 | Z = i) - 2
\end{array}
\right \} \\ \\
& \qquad \qquad \qquad \qquad \le \alpha \le \label{eq:bounds-appendix}\\ \\
& \qquad \quad \min \left \{
\begin{array}{ll}
  \min_{i \neq j} & P(Y = 1 | Z = i) - 2\cdot P(Y = 1 | Z = j) +  2\cdot P(X = 1 | Z = j) + 1 \\
  \min_{i \neq j} & P(Y = 1 | Z = i) + 2\cdot P(Y = 1 | Z = j) -  2\cdot P(X = 1 | Z = j) + 1 \\
  \min_{i \neq j} & P(Y = 1 | Z = i) - P(X = 1 | Z = i) + P(X = 1 | Z = j) - P(Y = 1 | Z = j) + 1 \\
  \min_i & P(X = 1 | Z = i) - P(Y = 1 | Z = i) + 1 \\
  \min_i & P(Y = 1 | Z = i) - P(X = 1 | Z = i) + 1
\end{array}
\right \}
\end{aligned}
\]

Furthermore, we obtain the following checkable constraints from the rows where \(\alpha = 0\):

\begin{equation}
\min \left\{
  \begin{array}{ll}
    \min_{i\neq j} & P(Y = 1 | Z = i) - P(X = 1 | Z = i) - P(Y = 1 | Z = j) - P(X = 1 | Z = j) + 2 \\
    \min_{i\neq j} & P(Y = 1 | Z = i) + P(X = 1 | Z = i) - P(Y = 1 | Z = j) + P(X = 1 | Z = j) \\
    \min_{i} & P(X = 1 | Z = i) \\
    \min_{i} & P(Y = 1 | Z = i) \\
    \min_{i} & 1 - P(X = 1 | Z = i) \\
    \min_{i} & 1 - P(Y = 1 | Z = i)
  \end{array}
\right \} \ge 0 \label{eq:constraints}
\end{equation}

We notice that the constraints from the law of probability are recovered (the last four expressions above) along with 12 non-trivial constraints.

These bounds involve 24 different expressions on both the lower and upper end, making an algebraic exploration of the width very challenging. However, by imposing the two monotonicity assumptions (A5) and (A6), the bounds reduce to just three on the lower end and three on the upper end. This is done by removing rows in the matrix of extreme vertices where the monotonicity assumptions are violated before using Polymake to get the inequalities. The resulting bounds are presented below.

\[
  \begin{aligned}
    &\max
      \begin{Bmatrix}
        P(Y = 1 | Z = 0) - P(X = 1 | Z = 0) - 1 \\
        P(Y = 1 | Z = 0) - P(Y = 1 | Z = 2) - P(X = 1 | Z = 0) + P(X = 1 | Z = 2) - 1 \\
        P(Y = 1 | Z = 0) - P(Y = 1 | Z = 2) + P(X = 1 | Z = 2) - 1
      \end{Bmatrix} 
      \begin{matrix} (L1) \\ (L2) \\ (L3) \end{matrix}  \\
    &\qquad \qquad \qquad \qquad \qquad\le ATE \le \\
    &\min
      \begin{Bmatrix}
        P(Y = 1 | Z = 0) - P(Y = 1 | Z = 2) + P(X = 1 | Z = 0) - P(X = 1 | Z = 2) + 1\\
        P(Y = 1 | Z = 0) - 2\cdot P(Y = 1 | Z = 2) - P(X = 1 | Z = 2) + 2 \\
        2\cdot P(Y = 1 | Z = 0) - P(Y = 1 | Z = 2) + P(X = 1 | Z = 0)
      \end{Bmatrix}
      \begin{matrix} (U1) \\ (U2) \\ (U3) \end{matrix}
  \end{aligned}
\]

```{r include = FALSE}
bivariate_bounds <- read_rds(here("vignettes_data/bivariate_bounds.Rds"))

flipped_bivariate_bounds <- bivariate_bounds %>% 
  filter(upper < lower, !violations) %>% 
  arrange(width) %>% 
  select(Strength = strength_x, lower, upper, width, thetas, gammas) %>% 
  unnest_wider(thetas) %>% 
  rename(`P(X=1|Z=0)` = ...1, 
         `P(X=1|Z=1)` = ...2, 
         `P(X=1|Z=2)` = ...3) %>% 
  unnest_wider(gammas) %>% 
  rename(`P(Y=1|Z=0)` = ...1, 
         `P(Y=1|Z=1)` = ...2, 
         `P(Y=1|Z=2)` = ...3,
         `Lower Bound` = lower,
         `Upper Bound` = upper,
         Width = width) %>% 
  relocate(starts_with("P(X=1"), starts_with("P(Y=1"))

flipped_bivariate_bounds_table <- flipped_bivariate_bounds %>% 
  kable(format = "latex", booktabs = TRUE, longtable = TRUE, caption = "Marginal conditional probabilities resulting in bounds where the upper bound is smaller than the lower bound.", label = "upper-less-than-lower") %>% 
  landscape() %>% 
  kable_styling(
    font_size = 9, 
    latex_options = "repeat_header"
  )
```

We encountered one surprise when studying the behavior of the bounds in \eqref{eq:bounds-appendix}. Of `r format(nrow(bivariate_bounds), scientific = FALSE, big.mark = ",")` randomly generated sets of values for $P(X = 1 | Z = z), P(Y = 1 | Z = z),\ z = 0,1,2$, `r nrow(flipped_bivariate_bounds)` resulted in bounds where the upper limit is smaller than the lower limit without violating any of the verifiable constraints presented in \eqref{eq:constraints}. Table \ref{tab:upper-less-than-lower} gives the values of the marginal conditional distributions with the strength of the IV, the corresponding bounds, and the width. It is notable that the IVs are rather strong in all cases where we see the bounds flip, but the bounds themselves and the widths vary quite a bit. 

We first attributed this to the transition from trivariate to bivariate bounds, but later realized similar scenarios arise when dealing with trivariate bounds from four category IVs. Of `r format(sum(pull(filter(violation_summaries, k == 4), n)), scientific = FALSE, big.mark = ",")` randomly generated sets of values for $P(X = x, Y = y | Z = z),\ x=0,1,\ y=0,1,\ z=0,1,2,3$, `r filter(violation_summaries, k == 4, upper_smallest, !violations)[['n']]` result in bounds where the upper limit is smaller than the lower limit without any violation of the verifiable constraints. It is also worth noting that in a similar number of trivariate distributions randomly generated with a trichotomous instrument, we did not see any cases of flipped bounds without a violation of one or more of the verifiable constraints. Table \ref{tab:flipped-trivariates} show the bounds from these trivariate distributions with the strengths of the IVs, and the width. Again, it is interesting to see the large span of widths and strengths present. 

We have been unable to unearth a reason for why we see this phenomenon. One possible explanation is that the distributions that result in flipped bounds violate some uncheckable assumption. 

```{r include = FALSE}
tri_bounds_flipped <- read_rds(here("data/tri_bounds_flipped.Rds"))
tri_bounds_flipped_table <- tri_bounds_flipped %>% 
  select(Lower = lower, Upper = upper, Strength = strength, Width = width) %>% 
  arrange(Width) %>% 
  kable(format = "latex", 
                    booktabs = TRUE, longtable = TRUE, 
                    caption = "Lower and Upper limits of bounds where the upper limit is less than the lower limit for trivariate distributions with four category instruments.", 
                    label = "flipped-trivariates") %>% 
  kable_styling(latex_options = "repeat_header")
```

<!-- Print tables from R -->
`r flipped_bivariate_bounds_table`

`r tri_bounds_flipped_table`

## Logistic Models \label{appendix-logistic-models}

When a GWAS is run to find associations between genetic markers and a binary trait, the logitisc regression model is often used. For this particular reason, we use the logistic model in our monte carlo integrations to characterize the behavior of the non-parametric bounds from two-sample data.

Specifically, we assume that \(P(Z = 0) = P(Z = 2) = 0.25\) and \(P(Z = 1) = 0.5\), and a value of an unmeasured confounder \(U\) from the standard normal. We assume the exposure $X$ is binary with $\text{logit}(P(X = 1 | Z_1 = z_1, ..., Z_p = z_p, U = u)) = \gamma_0 + \sum_i \gamma_i z_i + \gamma_U u$, where $\text{logit}(a) = \frac{1}{1+\exp(a)}$ and $\gamma_i$ corresponds to the estimand of the regression estimate one would obtain from GWAS studying the relationship between the genetic variant and the exposure. This model has been used in MR studies by @burgess_sample_2014 and @burgess_improving_2012 so that every instrument estimates the same exposure effect. Similarly, we assume that the outcome $Y$ is binary with \(P(Y = 1 | X = x, U = u) = \text{logit}(\beta_0 + \beta_X \cdot x + \beta_U \cdot u)\), which we use to compute the true ATE.

## Proof of Theorem \ref{thm:upperBoundWidth}

First of all, we note that the bounds found using the approach previously described when we impose (A5) and (A6) and the number of categories $k$ of the IV $Z$ is either $2$, $3$, or $4$, are 

\[
  \begin{aligned}
    &\max
      \begin{Bmatrix}
        P(Y = 1 | Z = 0) - P(X = 1 | Z = 0) - 1 \\
        P(Y = 1 | Z = 0) - P(Y = 1 | Z = k) - P(X = 1 | Z = 0) + P(X = 1 | Z = k) - 1 \\
        P(Y = 1 | Z = 0) - P(Y = 1 | Z = k) + P(X = 1 | Z = k) - 1
      \end{Bmatrix} 
      \begin{matrix} (L1) \\ (L2) \\ (L3) \end{matrix}  \\
    &\qquad \qquad \qquad \qquad \qquad\le ATE \le \\
    &\min
      \begin{Bmatrix}
        P(Y = 1 | Z = 0) - P(Y = 1 | Z = k) + P(X = 1 | Z = 0) - P(X = 1 | Z = k) + 1\\
        P(Y = 1 | Z = 0) - 2\cdot P(Y = 1 | Z = k) - P(X = 1 | Z = k) + 2 \\
        2\cdot P(Y = 1 | Z = 0) - P(Y = 1 | Z = k) + P(X = 1 | Z = 0)
      \end{Bmatrix}
      \begin{matrix} (U1) \\ (U2) \\ (U3) \end{matrix}
  \end{aligned}
\]

This gives us a total of nine different expressions for the width of the bounds. We will show that each of these nine expressions are bounded by $2 - 2\cdot ST$. Since we assume monotonicity of the effect of $Z$ on $X$, the strength simplifies to $\text{ST} = P(X = 1 | Z = k) - P(X = 1 | Z = 0)$. 

\textbf{Width = U1 - L1}

Since the lower bound is $L1$, $L1 \ge L2$. Hence, $P(X = 1 | Z = k) \le P(Y = 1 | Z = k)$. Therefore,

$$\begin{aligned}
U1 - L1 &= 2 - P(Y = 1 | Z = k) + 2\cdot P(X = 1 | Z = 0) - 2P(X = 1 | Z = k) \\
        &\le 2 + 2\cdot P(X = 1 | Z = 0) - 2\cdot P(X = 1 | Z = k) \\
        &= 2 - 2\cdot ST.
\end{aligned}$$

\textbf{Width = U2 - L1}

From $U2 \le U1$, $1 - P(Y = 1 | Z = k) \le P(X = 1 | Z = 0)$, and from $L2 \le L1$, $-P(Y = 1 | Z = k) \le -P(X = 1 | Z = k)$. So,

$$\begin{aligned}
U2 - L1 &= -2\cdot P(Y = 1 | Z = k) - P(X = 1 | Z = k) + P(X = 1 | Z = 0) + 3 \\
        &= 3 - ST - 2\cdot P(Y = 1 | Z = k) \\
        &\le 2 - 2\cdot ST
\end{aligned}$$


\textbf{Width = U3 - L1}

Again, $L2 \le L1$ and so $-P(Y = 1 | Z = k) \le -P(X = 1 | Z = k)$. Therefore,

$$\begin{aligned}
U3 - L1 &= P(Y = 1 | Z = 0) - P(Y = 1 | Z = k) + 2\cdot P(X = 1 | Z = 0) + 1 \\
        &= 1 - P(Y = 1 | Z = k) + 2 P(X = 1 | Z = 0) \\
        &\le 1 - P(X = 1 | Z = k) + 2 P(X = 1 | Z = 0) \\
        &= 1 - ST + P(X = 1 | Z = 0) \\
        &= 2 - 2\cdot ST + P(X = 1 | Z = k) - 1 \\
        &\le 2 - 2 \cdot ST
\end{aligned}$$

\textbf{Width = U1 - L2}

$$\begin{aligned}
U1 - L2 &= 2 + 2\cdot P(X = 1 | Z = 0) - 2\cdot P(X = 1 | Z = k) \\
        &= 2 - 2\cdot ST.
\end{aligned}$$

\textbf{Width = U2 - L2}

Since the upper bound is $U2$, $U2 \le U1$ which leads us to $1 - P(Y = 1 | Z = k) \le P(X = 1 | Z = 0)$. So,

$$\begin{aligned}
U2 - L2 &= 3 - P(Y = 1 | Z = k) + P(X = 1 | Z = 0) - 2\cdot P(X = 1 | Z = k) \\
        &= 2 - ST + 1 - P(Y = 1 | Z = k) - P(X = 1 | Z = k) \\
        &\le 2 - 2\cdot ST
\end{aligned}$$

\textbf{Width = U3 - L2}

From $U3 \le U2$, we see that $P(Y = 1 | Z = 0) \le 1 - P(X = 1 | Z = k)$. Therefore,

$$\begin{aligned}
U3 - L2 &= 1 + P(Y = 1 | Z = 0) + 2\cdot P(X = 1 | Z = 0) - P(X = 1 | Z = k) \\
        &\le 2 - 2\cdot ST
\end{aligned}$$

\textbf{Width = U1 - L3}

$$\begin{aligned}
U1 - L3 &= 2 - 2\cdot P(X = 1 | Z = k) + P(X = 1 | Z = 0) \\
        &= 2 - 2\cdot ST - P(X = 1 | Z = 0) \\
        &\le 2 - 2\cdot ST
\end{aligned}$$

\textbf{Width = U2 - L3}

Since the upper bound is $U2$, $1 - P(Y = 1 | Z = k) \le P(X = 1 | Z = 0)$, we see that

$$\begin{aligned}
U2 - L3 &= 3 - P(Y = 1 | Z = k) - 2\cdot P(X = 1 | Z = k) \\
        &\le 2 - 2\cdot P(X = 1 | Z = k) + P(X = 1 | Z = 0) \\
        &\le 2 - 2\cdot ST
\end{aligned}$$

\textbf{Width = U3 - L3}

From $U3 \le  U2$, we see that $P(Y = 1 | Z = 0) \le 1 - P(X = 1 | Z = k)$. Therefore,

$$\begin{aligned}
U3 - L3 &= 1 + P(Y = 1 | Z = 0) + P(X = 1 | Z = 0) - P(X = 1 | Z = k)\\
        &= 1 - ST + P(Y = 1 | Z = 0) \\
        &\le 2 - ST - P(X = 1 | Z = k) \\
        &\le 2 - 2\cdot ST.
\end{aligned}$$

As we see from the derivations above, regardless of which expression is the minimum and which is the maximum in bounds above, the width of the bounds is bounded from above by $2 - 2\cdot ST$. 

\qed

\newpage

## Simulation Setup and Results for Section \ref{properties-of-bounds-from-summary-level-data} \label{appendix-sim-results}

Since GWAS results are most often reported as summary statistics and coefficients from a logistic model, we use monte carlo integration to show the relationship between ST and coefficients in a logistic model. We use the model introduced in Appendix \ref{appendix-logistic-models} with $p=1$. Throughout, we set $\gamma_0 = -\gamma_1$ and $\beta_0 = -\beta_1/2$. This is done to maximize the differences between probabilities $P(X = 1 | Z = z)$, $z=0,1,2$, and $P(Y = 1 | Z = z)$, $z=0,1,2$. For simplicity, we also keep $\beta_U = \gamma_U$. 

For each combination of values of the coefficients $\gamma_1, \gamma_U, \beta_1$ listed below, $10,000,000$ realizations of the unmeasured confounder $U$ are drawn from a standard normal distribution. For each realization, a value of $Z$ is drawn such that $P(Z = 0) = P(Z = 2) = 0.25$, and $P(Z = 1) = 0.5$. Next, values of $X$ and $Y$ are generated using these values such that $\text{logit}(P(X = 1 | Z = z, U = u)) = \gamma_0 + \gamma_1 z + \gamma_U u$ and $\text{logit}(P(Y = 1 | X = x, U = u)) = \beta_0 + \beta_1 x + \beta_U u$. This results in $10,000,000$ realizations of $(X,Y,Z)$. From these, we find the marginal probabilities $P(X = 1 | Z = z)$ and $P(Y = 1 | Z = z)$, $z = 0,1,2$, the values of ST $=\max_{z_1 \neq z_2} |P(X = 1 | Z = z_1) - P(X = 1 | Z = z_2)|$ and the ATE $= P(Y = 1 | X = 1) - P(Y = 1 | X = 0)$. 

\begin{table}[H]
  \centering
  \caption{The monte carlo integration was performed for all combinations of values of the coefficients $\gamma_1, \gamma_U$, and $\beta_1$ presented below.}
  \label{tab:sim_coefficients}
  \input{`r here("tables/sim_coefficients_table.tex")`}
\end{table}

Each set of marginal probabilities leads us to a set of non-parametric bounds from two-sample data. These are shown on Figure \ref{fig:power} together with the ATE. The left plot in Figure \ref{fig:biv_width_vs_strength} shows the width of these bounds plotted against ST, while the right plot shows the values of $\gamma_1$ plotted against ST. 

\clearpage
\begin{sidewaysfigure}
  \centering
  \includegraphics[width=\textheight]{`r here("figures/power.png")`}
  \caption{Bounds based on simulations as described. Upper and lower bounds are connected by a curve (dotted lines) based on a loess extrapolation. This curve is used to find the smallest coefficients needed to detect direction as plotted on Figure \ref{fig:power_curves}.}
  \label{fig:power}
\end{sidewaysfigure}
\clearpage

To find the smallest value of $\gamma_1$ that results in bounds excluding $0$, we fit a loess curve to the lower bounds in Figure \ref{fig:power}, and find the value where this curve crosses $0$. This results in the values depicted on Figure \ref{fig:power_curves}.

## Bounds From Two Sample Data With Multiple IVs \label{appendix-sim-results-multiple-IVs}

Here, we will describe how to expand the monte carlo integration to include multiple IVs. Consider the exposure and outcome models introduced in Appendix \ref{appendix-logistic-models}. $z_i \in \{0,1,2\}$ represents the $i$th instrument, and $\gamma_i$ represents the $i$th instrument's effect on the exposure. Also, for each instrument $i$, we set \(P(Z_i = 0) = P(Z_i = 2) = 0.25\) and \(P(Z_i = 1) = 0.5\). We set $p = 10$ or $p = 50$, and draw \(U\) from a standard normal distribution. Again, for simplicity, we set $\beta_U = \gamma_U$, and $\gamma_0 = -\sum_i \gamma_i$ and $\beta_0 = -\beta_1/2$ to spread out the probabilities $P(X = 1 | Z = z)$ and $P(Y = 1 | X = x)$ as much as possible. $\beta_1$ is set to be either $0.25$, $0.5$, $1$, $1.5$, or $2$. We then consider four scenarios for setting the $\gamma_i$'s:

\begin{enumerate}
\item \emph{Many weak instruments}: \(\gamma_i\) are spread out evenly on the interval \(0\) to \(0.2\).
\item \emph{Many strong instruments}: \(\gamma_i\) are spread out evenly on the interval \(1\) to \(4\). This is the magnitude of $\gamma$s that detected the direction of the ATE in the previous section
\item \emph{Many very weak instruments, one medium strength instrument}: $\gamma_i$, $i=1,2,...,p-1$, are evenly spread out on the interval $0$ to $0.01$, and $\gamma_p = 0.2$. 
\item \emph{Many medium strong instruments, one strong instrument}: $\gamma_i$, $i=1,2,...,p-1$, are evenly spread out on the interval $1$ to $1.2$, and $\gamma_p = 4$.
\end{enumerate}

The first scenario mimics typical magnitudes of coefficients seen in MR studies, and is an example where many genetic traits weakly contribute to the expression of complex traits [@loh_contrasting_2015; @shi_contrasting_2016; @nj_genetic_2017]. The third scenario represents a genetic architecture where only few genetic variants have strong effects on the exposure while others have weak effects [@yang_common_2010]. Scenarios 2 and 4 are as scenarios 1 and 3, but with coefficients of larger magnitude. We don't expect to observe this in practice, but these are the magnitudes that our results in Section \ref{bounds-from-bivariate-data} suggests would result in informative bounds when $p = 1$.

For each scenario, we use monte carlo integration with $1,000,000$ re-samples to obtain $P(X = 1 | Z_j = z_j)$ and $P(Y = 1 | Z_j = z_j)$ -- this procedure is as described in Appendix \ref{appendix-sim-results}. We then use these quantities to obtain two-sample IV bounds for each of the $p$ instruments. Figures \ref{fig:bounds_from_multiple_IV_sims_MR}, \ref{fig:bounds_from_multiple_IV_sims_MR_many_weak}, \ref{fig:bounds_from_multiple_IV_sims_power}, and \ref{fig:bounds_from_multiple_IV_sims_power_many_weak} summarize the results. We see that in scenarios 1 and 2, every bound is non-informative, with widths close to or exceeding $1$. Also, the bounds are nested within each other. Thus, if we were to aggregate the bounds by taking intersections, the width of the intersection bounds will still be close to or exceed $1$. In addition, the increase in magnitude of the $\gamma_i$ coefficient did not improve the bounds. Scenarios 3 and 4 show similar results in that the bounds cover the null effect, but the strongest instrument in each scenario produces a much smaller bound than in scenarios 1 and 2. From Figure \ref{fig:multiple_IVs_scenario_3} it is clear that on the scale that is often observed in MR studies, two-sample nonparametric bounds are generally non-informative. Also, the bounds in scenarios 3 and 4 are again nested leaving us with the conclusion that the intersection of bounds from multiple instruments will give no more information than the strongest of the instruments itself. 

We take a moment to explain the differences between our result in Section \ref{bounds-from-bivariate-data} with a single instrument with $\gamma_i = 4$ and our results in this section where one of the instruments has $\gamma_i = 4$, but others have much smaller $\gamma$s. We see that if the variation in the exposure model is determined by multiple independent instruments, the effect of one single instrument on producing an informative bound greatly diminishes. Specifically, Figure \ref{fig:multiple_IVs_scenario_4} shows that in a setting where we would be able to detect the direction of the ATE from an instrument with $\gamma_i = 4$ if only $p = 10$ instruments are contributing to the exposure, that same coefficient would not be large enough if $p = 50$ instruments were included. This suggests that for exposures that are determined by many instruments, the strongest among these instruments must be even stronger for a bound-based analysis to be useful. In other words, multiple instruments may not be helpful in a bound-based analysis when the exposure is polygenic in nature.

```{r}
dilution_effect <- read_csv(here("tables/dilution_effect.csv")) %>% 
  mutate(across(starts_with("p = "), round, digits = 3),
         Scenario = str_extract(Scenario, "[0-9]")) %>% 
  rename(`$\\beta_U$` = U_on_XY, 
         `$\\gamma_i$` = indIVs_on_X) %>% 
  relocate(Scenario)
```

<!-- \begin{table}[H] -->
<!--   \center -->
<!--   \caption{The largest coefficient allowed in each of the four scenarios results in instruments with very different values of ST when more instruments are included. The effect is more prominent when the support of the coefficients is on the larger end.} -->
<!--   \label{tab:dilution_effect} -->
<!--   `r kable(dilution_effect, format = "latex", booktabs = TRUE, escape = FALSE) %>% add_header_above(header = c(" " = 3, "Strength" = 2))` -->
<!-- \end{table} -->

Our results also have dire implications when some instruments turn out to be invalid. If, as suggested by @swanson_commentary_2017, we take the union of IV bounds so that the union bound is guaranteed to cover the true ATE so long as there is at least one valid instrument, the union bound will likely be non-informative because there was at least one IV bound in our scenario that was non-informative. Without making some assumptions about the nature of the invalid IVs, it would generally be infeasible to obtain useful information from a bound-based analysis.

Overall, combining our results in Section \ref{bounds-from-bivariate-data}, our conclusion about using nonparametric IV bounds in two-sample MR studies is grim. Such a nonparametric analysis would require very strong instruments and/or effect sizes, which are rare in MR studies, and even stronger than those in one-sample settings. Also, multiple instruments are no better than having a single, strong instrument. As discussed in Section \ref{bounds-from-bivariate-data}, a primary reason for the non-informative nature of the IV bounds in two-sample settings is that we don't have information about the joint distribution of $X,Y$ given $Z$. While obtaining this joint distribution is generally difficult in many MR studies, in the next section, we discuss how to obtain a plausible range of the joint distribution \(P(Y, X | Z)\) given two sample MR data \(P(Y|Z)\) and \(P(X | Z)\) in order to create more informative bounds from two-sample MR studies.

\begin{figure*}
  \centering
  \begin{subfigure}{0.5\linewidth}
  \caption{}
  \includegraphics[width=\textwidth]{`r here("figures/strength_vs_coef_multiple_IVs_MR.png")`}
  \label{fig:strength_vs_coef_multiple_IVs_MR}
  \end{subfigure}%
  ~
  \begin{subfigure}{0.5\linewidth}
  \caption{}
  \includegraphics[width=\textwidth]{`r here("figures/strength_vs_coef_multiple_IVs_power.png")`}
  \label{fig:strength_vs_coef_multiple_IVs_power}
  \end{subfigure}
  \caption{Figure showing the dilution effect described in Section \ref{bounds-from-two-sample-data-with-multiple-ivs} in each of the four scenarios. When $p$ is larger, similar sized coefficients lead to lower strength. The effect is smaller when we are in a scenario where one coefficient is relatively much larger than the rest, rather than when the coefficients are evenly spread out. A: Scenarios 1 and 3. B: Scenarios 2 and 4.}
  \label{fig:strength_vs_coef_multiple_IVs}
\end{figure*}

\clearpage

\begin{sidewaysfigure}
  \centering
  \includegraphics[width=\textheight]{`r here("figures/bounds_from_multiple_IV_sims_MR.png")`}
  \caption{Bounds based on monte carlo integration with 1,000,000 resamples in scenario 1.}
  \label{fig:bounds_from_multiple_IV_sims_MR}
\end{sidewaysfigure}

\clearpage

\begin{sidewaysfigure}
  \centering
  \includegraphics[width=\textheight]{`r here("figures/bounds_from_multiple_IV_sims_MR_many_weak.png")`}
  \caption{Bounds based on monte carlo integration with 1,000,000 resamples in scenario 3.}
  \label{fig:bounds_from_multiple_IV_sims_MR_many_weak}
\end{sidewaysfigure}

\clearpage


\begin{sidewaysfigure}
  \centering
  \includegraphics[width=\textheight]{`r here("figures/bounds_from_multiple_IV_sims_power.png")`}
  \caption{Bounds based on monte carlo integration with 1,000,000 resamples in scenario 2.}
  \label{fig:bounds_from_multiple_IV_sims_power}
\end{sidewaysfigure}

\clearpage

\begin{sidewaysfigure}
  \centering
  \includegraphics[width=\textheight]{`r here("figures/bounds_from_multiple_IV_sims_power_many_weak.png")`}
  \caption{Bounds based on monte carlo integration with 1,000,000 resamples in scenario 4.}
  \label{fig:bounds_from_multiple_IV_sims_power_many_weak}
\end{sidewaysfigure}

\clearpage


## Reconstructing the Joint Distribution $P(X, Y | Z)$ \label{appendix-quasi-bayesian-details}

To draw a possible set of values for the joint conditional distribution $P(X = x, Y = y | Z = z)$, we start by writing the joint conditional distribution \(P(X = x, Y = y | Z = z)\) as a function of the marginal conditional distributions \(P(X = x | Z = z)\) and \(P(Y = y | Z = z)\) and the conditional covariance of the exposure \(X\) and \(Y\) given \(Z=z\), \(\text{Cov}(X, Y | Z = z)\), for each \(z\)

\begin{equation}
P(X = x, Y = y | Z = z) = P(X = x | Z = z)P(Y = y | Z = z) + (2\cdot I[x = y] - 1)\text{Cov}(X, Y | Z = z). \label{eq:cov-expression}
\end{equation}

Because \(\text{Cov}(X, Y | Z = z)\) is impossible to estimate from two-sample MR studies, we instead propose to put a prior on this quantity. This prior must not only produce a proper probability distribution of \((X,Y|Z)\), but also satisfy the verifiable constraints from the IV assumptions. Specifically, by the definition of a proper probability distribution, \(\text{Cov}(X, Y | Z = z)\) must satisfy

\[
\begin{aligned}
  \max_z\left\{
      \begin{array}{c}
        -P(X = 1 | Z = z)P(Y = 1 | Z = z) \\
        -P(X = 0 | Z = z)P(Y = 0 | Z = z) \\
        P(X = 1 | Z = z)P(Y = 0 | Z = z) - 1\\
        P(X = 0 | Z = z)P(Y = 1 | Z = z) - 1
      \end{array}
    \right\} & \\
    \le \text{Cov}(X, &Y | Z = z) \le \\
    &\min_z\left\{
      \begin{array}{c}
        1 - P(X = 1 | Z = z)P(Y = 1 | Z = z) \\
        1 - P(X = 0 | Z = z)P(Y = 0 | Z = z) \\
        P(X = 1 | Z = z)P(Y = 0 | Z = z) \\
        P(X = 0 | Z = z)P(Y = 1 | Z = z)
      \end{array}
    \right\}
\end{aligned}
\]

Additionally, by the IV inequality constraints $\max_x \sum_y \max_z P(X = x, Y = y | Z = z)$, for any pair of \((z_1, z_2) \in \{0,1,2\} \times \{0,1,2\}\), the values of \(\text{Cov}(X, Y | Z = z_1)\) and \(\text{Cov}(X, Y | Z = z_2)\) must satisfy

\[
\begin{aligned}
  \max\left\{
      \begin{array}{c}
        -P(X = 0 | Z = z_1)P(Y = 0 | Z = z_1) - P(X = 0 | Z = z_2)P(Y = 1 | Z = z_2) \\
        P(X = 1 | Z = z_1)P(Y = 0 | Z = z_1) + P(X = 1 | Z = z_2)P(Y = 1 | Z = z_2) -1 \\
        P(X = 0 | Z = z_2)P(Y = 0 | Z = z_2) + P(X = 0 | Z = z_1)P(Y = 1 | Z = z_1) - 1 \\
        -P(X = 1 | Z = z_2)P(Y = 0 | Z = z_2) - P(X = 1 | Z = z_1)P(Y = 1 | Z = z_1)
      \end{array}
    \right\} \qquad \qquad & \\ \\
    \le \text{Cov}(X,Y | Z = z_1) - \text{Cov}(X,Y | Z = z_2) \le \qquad \qquad \qquad \qquad  \qquad& \\ \\
    \min\left\{
      \begin{array}{c}
        1 -P(X = 0 | Z = z_1)P(Y = 0 | Z = z_1) - P(X = 0 | Z = z_2)P(Y = 1 | Z = z_2) \\
        P(X = 1 | Z = z_1)P(Y = 0 | Z = z_1) + P(X = 1 | Z = z_2)P(Y = 1 | Z = z_2) \\
        P(X = 0 | Z = z_2)P(Y = 0 | Z = z_2) + P(X = 0 | Z = z_1)P(Y = 1 | Z = z_1) \\
        1 - P(X = 1 | Z = z_2)P(Y = 0 | Z = z_2) - P(X = 1 | Z = z_1)P(Y = 1 | Z = z_1)
      \end{array}
    \right\} &
\end{aligned}
\]

We sequentially sample values of \(\text{Cov}(X, Y | Z = 0), \text{Cov}(X, Y | Z = 1), \text{Cov}(X, Y | Z = 2)\), such that the above inequalities plus the existing constraints in \eqref{eq:constraints} are satisfied. Then, among samples of \(\text{Cov}(X, Y | Z = 0), \text{Cov}(X, Y | Z = 1), \text{Cov}(X, Y | Z = 2)\) that satisfy the constraints, we calculate the joint distribution of \(P(X = x, Y = y | Z = z)\) using \eqref{eq:cov-expression}, leading us to a plausible set of values for the joint distribution \(P(X = x, Y = y | Z = z)\).

For each plausible joint distribution \(P(X = x, Y = y | Z = z)\), we use the one-sample IV bounds by @balke_bounds_1997 and @richardson_ace_2014 to obtain a bound for the ATE. If a large number of the one-sample IV bounds do not cover zero, then there is some evidence for a non-zero exposure effect and the only reason we are not able to detect this effect is due to the limitations of the two-sample design. However, if a large number of the one-sample IV bounds do cover zero, there is less evidence for a non-zero causal effect or that utilizing bound-based approaches to obtain some information about the ATE may be a hopeless exercise even if we are under a one-sample design.

### Sampling of Intersection Bounds From Two Instruments \label{sample-intersection-bounds}

To extend our method for sampling plausible joint distributions of $P(X = x, Y = y | Z = z)$ to the scenario where we have multiple instruments available, we simply repeat the one instrument sampling for each instrument. This is equivalent to assuming that the covariances of $X$ and $Y$ given $Z_1$ are independent of the covariances of $X$ and $Y$ given $Z_2$. Once we have obtained bounds for each instrument, we take the intersection to get the intersection bounds. 

Specifically, say we get bounds \((LB_{1i},UB_{1i}),i = 1,2,...,m\) by sampling $m$ trivariate distributions based on the information we have on \((X,Z_1)\) and \((Y,Z_1)\), and bounds \((LB_{2i}, UB_{2i}),i = 1,2,...,m\) by sampling \(m\) trivariate distributions based on the information we have on \((X,Z_2)\) and \((Y,Z_2)\). We then create the intersection bounds as \(\left(\max_{z \in {1,2}} LB_{zi}, \min_{z \in {1,2}} UB_{zi}\right), i = 1, 2, ..., m\). This, under the assumption that \(\text{Cov}(X, Y | Z_1 = z)\) and \(\text{Cov}(X, Y | Z_2 = z)\) are independent of each other, gives us a sample from the posterior distribution of intersection bounds. We can use this to assess the potential usefulness of aggregating information from two sets of trivariate data, \((X, Y, Z_1)\) and \((X, Y, Z_2)\), using intersection bounds.

## Additional Summary Statistics and Figures for Analyses Presented in Section \ref{data-analysis} \label{more-details-data-application-appendix}

We use the `TwoSampleMR` R package [@mrbase] to extract and preprocess the data for our analyses. For preprocessing, we followed the defaults of the R pacakge where linkage disequilibrium based clumping (\(r^2 \ge 0.001\) within a \(10,000\) kb window using \(p < 5 \times 10^{-8}\) as the level of significance) were performed such that only independent instruments with significant associations were used in the analysis. Afterwards, we obtain the estimated coefficients corresponding to the effects of the SNPs on the exposure and the outcome from a logistic model. Since estimates of the intercept are not included in these reported results, but the marginal proportions of the outcome, exposure, and allele frequencies are known, we find the intercepts by solving \(P(X = 1) = \sum_{z = 0}^2\text{logit}(\beta_0 + \hat{\beta_1}\cdot z)\cdot P(Z_j = z)\) and \(P(Y = 1) = \sum_{z = 0}^2\text{logit}(\gamma_0 + \hat{\gamma_1}\cdot z)\cdot P(Z_j = z)\) for \(\beta_0\) and \(\gamma_0\), respectively. Overall, we have estimates of \(P(Y = 1 | Z_j = z)\) and \(P(X = 1 | Z_j = z)\) for every \(j\) and \(z=0,1,2\).

```{r include = FALSE}
summaries_lung_cancer <- read_csv(here("vignettes_data/summary_stats_smoking_on_lung_cancer.csv"))

snp_marginals_lung_cancer <- summaries_lung_cancer %>% 
  select(SNP, starts_with("P(Z =")) %>%
  unique()

snp_coefs_lung_cancer <- summaries_lung_cancer %>% 
  select(SNP, regression, starts_with("beta")) %>% 
  unique() %>% 
  pivot_longer(cols = c(beta, beta0)) %>% 
  mutate(name = case_when(regression == "exposure" & name == "beta"  ~ "$\\beta_1$",
                          regression == "exposure" & name == "beta0" ~ "$\\beta_0$",
                          regression == "outcome"  & name == "beta"  ~ "$\\gamma_1$",
                          regression == "outcome"  & name == "beta0" ~ "$\\gamma_0$",
                          TRUE ~ NA_character_)) %>% 
  select(-regression) %>% 
  pivot_wider(names_from = name, values_from = value)
```


### Effect of Smoking on Lung Cancer \label{appendix:smoking-on-lung-cancer}

\begin{figure}[H]
  \center
  \includegraphics[width = \textwidth]{`r here("figures/example_analyses/smoking_lung_cancer_marginal_Z.png")`}
  \caption{Histograms of the marginal distribution of instruments, $P(Z = z), z=0,1,2$, estimated after preprocessing.}
  \label{fig:marginal-distribution-of-instruments-lung-cancer}
\end{figure}

\begin{figure}[H]
  \center
  \includegraphics[width = \textwidth]{`r here("figures/example_analyses/smoking_lung_cancer_coefficients.png")`}
  \caption{Histograms of the coefficients from GWAS results of logistic regression of the SNPs on smoking status and lung cancer status. Intercepts ($\beta_0$ and $\gamma_0$) are inferred, while slopes ($\beta_1$ and $\gamma_1$) are as reported.}
  \label{fig:marginal-distribution-of-coefficients-lung-cancer}
\end{figure}

<!-- Print table from R -->
`r kable(snp_coefs_lung_cancer, "latex", longtable = T, booktabs = TRUE, caption = "Coefficients from GWAS results of logistic regression of the SNPs on smoking status and lung cancer status. Intercepts ($\\beta_0$ and $\\gamma_0$) are inferred, while slopes ($\\beta_1$ and $\\gamma_1$) are as reported.", label = "coefficients-lung-cancer", escape = FALSE) %>% kable_styling(latex_options = "repeat_header")`

\begin{figure}[H]
 \center
 \includegraphics[width = 0.99\linewidth]{`r here('figures/example_analyses/strength_histogram.png')`}
 \caption{Histogram of strengths of IVs on the exposure. Here, SNPs are IVs, and smoking status (ever/never) is exposure. We see that all IVs are very weak, with the largest value just below 0.01.}
 \label{fig:strength_histogram}
\end{figure}

\begin{figure}[H]
  \center
  \includegraphics[width = 0.99\linewidth]{`r here('figures/example_analyses/smoking_lung_cancer_bivariate_bounds_ukb-d-20116_0_ukb-d-40001_C349.png')`}
  \caption{Nonparametric two-sample IV bounds on the average treatment effect of smoking on the incidence of lung cancer.}
  \label{fig:smoking_on_lung_cancer_ind_bounds}
\end{figure}

\clearpage

\begin{sidewaysfigure}
  \center
  \includegraphics[width = \textheight]{`r here("figures", "example_analyses", "smoking_lung_cancer_individual_SNPs_plot_ukb-d-20116_0_ukb-d-40001_C349.png")`}
  \caption{500 sets of bounds of the average treatment effect of smoking on lung cancer for each of the 84 SNPs. Each bound is based on a set of values for the trivariate distribution randomly sampled. Bounds are color coded to show if they overlap 0 (grey) or do not (red). All bounds overlap 0.}
  \label{fig:smoking_on_lung_cancer_tri_bounds_all}
\end{sidewaysfigure}

\clearpage

\begin{figure}[H]
  \center
  \includegraphics[width = 0.99\linewidth]{`r here('figures/example_analyses/smoking_lung_cancer_intersection_bounds_plot_ukb-d-20116_0_ukb-d-40001_C349.png')`}
  \caption{Intersection bounds of the average treatment effect of smoking on lung cancer based on randomly sampled trivariate distributions from pairs of SNPs. These 8 pairs were randomly chosen from all possible pairs.}
  \label{fig:smoking_on_lung_cancer_intersections}
\end{figure}


### Effect of High Cholesterol on Heart Attack \label{appendix:cholesterol-on-heart-attack}

```{r include = FALSE}
summaries_cholesterol_heart_attack <- read_csv(here("vignettes_data/summary_stats_cholesterol_on_heart_attack.csv"))
snp_marginals_cholesterol <- summaries_cholesterol_heart_attack %>% 
  select(SNP, starts_with("P(Z =")) %>% 
  unique()

snp_coefs_cholesterol <- summaries_cholesterol_heart_attack %>% 
  select(SNP, regression, starts_with("beta")) %>% 
  unique() %>% 
  pivot_longer(cols = c(beta, beta0)) %>% 
  mutate(name = case_when(regression == "exposure" & name == "beta"  ~ "$\\beta_1$",
                          regression == "exposure" & name == "beta0" ~ "$\\beta_0$",
                          regression == "outcome"  & name == "beta"  ~ "$\\gamma_1$",
                          regression == "outcome"  & name == "beta0" ~ "$\\gamma_0$",
                          TRUE ~ NA_character_)) %>% 
  select(-regression) %>% 
  pivot_wider(names_from = name, values_from = value)

```


\begin{figure}[H]
  \center
  \includegraphics[width = \textwidth]{`r here("figures/example_analyses/cholesterol_heart_attack_marginal_Z.png")`}
  \caption{Histograms of the marginal distribution of instruments, $P(Z = z), z=0,1,2$, estimated after preprocessing.}
  \label{fig:marginal-distribution-of-instruments-cholesterol-heart-attack}
\end{figure}

\begin{table}[H]
  \caption{Table of the marginal distribution of instruments, $P(Z = z), z=0,1,2$, estimated after preprocessing for analysis in Section \ref{cholesterol-on-heart-attack}}
  \label{tab:marginal-distribution-of-instruments-lung-cancer}
  \begin{minipage}{0.5\linewidth}
    \center
    `r kable(filter(snp_marginals_cholesterol, row_number() < 28), format = "latex", booktabs = TRUE) # %>% kable_styling(latex_options = "repeat_header")`
  \end{minipage}
  \qquad
  \begin{minipage}{0.5\linewidth}
    \center
    `r kable(filter(snp_marginals_cholesterol, row_number() > 27), format = "latex", booktabs = TRUE) # %>% kable_styling(latex_options = "repeat_header")`
  \end{minipage}
\end{table}

\begin{figure}[H]
  \center
  \includegraphics[width = 0.99\linewidth]{`r here("figures", "example_analyses", "cholesterol_heart_attack_marginal_conditionals.png")`}
  \caption{Histograms of the marginal conditional probabilities $P(X = 1 | Z = z), z = 0,1,2$ and $P(Y = 1 | Z = z), z=0,1,2$.}
  \label{fig:smoking_on_depression_marginals}
\end{figure}

\begin{figure}[H]
  \center
  \includegraphics[width = \textwidth]{`r here("figures/example_analyses/cholesterol_heart_attack_coefficients.png")`}
  \caption{Histograms of the coefficients from GWAS results of logistic regression of the SNPs on high cholesterol and heart attack, respectively. Intercepts ($\beta_0$ and $\gamma_0$) are inferred, while slopes ($\beta_1$ and $\gamma_1$) are as reported.}
  \label{fig:marginal-distribution-of-coefficients-depression}
\end{figure}

<!-- Print table from R -->
`r kable(snp_coefs_cholesterol, "latex", longtable = T, booktabs = TRUE, caption = "Coefficients from GWAS results of logistic regression of the SNPs on high cholesterol and heart attack status. Intercepts ($\\beta_0$ and $\\gamma_0$) are inferred, while slopes ($\\beta_1$ and $\\gamma_1$) are as reported.", label = "coefficients-cholesterol", escape = FALSE) %>% kable_styling(latex_options = "repeat_header")`


\begin{figure}[H]
 \center
 \includegraphics[width = 0.99\linewidth]{`r here('figures/example_analyses/cholesterol_heart_attack_strength_histogram.png')`}
 \caption{Histogram of strengths of IVs on the exposure. Here, SNPs are IVs, and high cholesterol is the exposure. We see that all IVs are very weak, with the largest value below 0.00225.}
 \label{fig:cholesterol_heart_attack_strength_histogram}
\end{figure}

\begin{figure}[H]
  \includegraphics[width = 0.99\linewidth]{`r here("figures/example_analyses/cholesterol_heart_attack_bivariate_bounds_ukb-a-108_ukb-a-434.png")`}
  \caption{Nonparametric two-sample IV bounds on the average treatment effect of high cholesterol on the incidence of heart attack.}
  \label{fig:cholesterol_on_heart_attack_ind_bounds}
\end{figure}

\clearpage

\begin{sidewaysfigure}
  \center
  \includegraphics[width = \textheight]{`r here("figures", "example_analyses", "cholesterol_heart_attack_individual_SNPs_plot_ukb-a-108_ukb-a-434.png")`}
    \caption{500 sets of bounds of the average treatment effect of high cholesterol on heart attack for each of the 54 SNPs. Each bound is based on a set of values for the trivariate distribution randomly sampled. Bounds are color coded to show if they overlap 0 (grey) or do not (red). All bounds overlap 0.}
    \label{fig:cholesterol_heart_attack_tri_bounds_all}
\end{sidewaysfigure}

\clearpage

\begin{figure}[H]
  \center
  \includegraphics[width = 0.99\linewidth]{`r here('figures/example_analyses/cholesterol_heart_attack_intersection_bounds_plot_ukb-a-108_ukb-a-434.png')`}
  \caption{Intersection bounds of the average treatment effect of high cholesterol on heart attack based on randomly sampled trivariate distributions from pairs of SNPs. These 8 pairs were randomly chosen from all possible pairs.}
  \label{fig:cholesterol_on_heart_attack_intersections}
\end{figure}

\newpage